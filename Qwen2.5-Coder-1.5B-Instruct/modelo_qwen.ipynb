{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Instalação e importação de módulos"
      ],
      "metadata": {
        "id": "y3fFUA-JdFOh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PngmsobmWP3j"
      },
      "outputs": [],
      "source": [
        "!pip install -q transformers torch accelerate"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from IPython.display import clear_output"
      ],
      "metadata": {
        "id": "gKi2HeXLbeuS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Antes de prosseguir, verificar se está com a GPU habilitada. A inferência demora demais se estiver usando apenas CPU."
      ],
      "metadata": {
        "id": "2l5kMj5acedb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"GPU disponível?\", torch.cuda.is_available())\n",
        "\n",
        "if not torch.cuda.is_available():\n",
        "    print(\"Ative a GPU em Runtime -> Change runtime type -> T4 GPU.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uHa6LnMLau4I",
        "outputId": "f19726d4-1f41-46b0-98fe-61fb57c7004d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU disponível? True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Carregando o modelo Qwen2.5-Coder-1.5B-Instruct do HuggingFace"
      ],
      "metadata": {
        "id": "xJpw8_ENWo6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_id = \"Qwen/Qwen2.5-Coder-1.5B-Instruct\"\n",
        "\n",
        "print(f\"Carregando {model_id}...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    dtype=torch.float16,\n",
        "    device_map=\"cuda\"\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xVlrxPC-Wk_Z",
        "outputId": "f3171827-1452-4249-f783-e070564c38f6",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Carregando Qwen/Qwen2.5-Coder-1.5B-Instruct...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Definições de funções úteis: **Geração de árvore de diretórios a partir do repositório clonado** e **função de inferência** com prompt."
      ],
      "metadata": {
        "id": "N_4ISyH0W43Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def gerar_arvore_diretorios(caminho_raiz, max_depth=3, ignorar=['.git', 'node_modules', 'dist', 'build', 'coverage', 'venv', '.github', 'assets']):\n",
        "    tree_str = \"\"\n",
        "    root_level = caminho_raiz.count(os.sep)\n",
        "\n",
        "    for root, dirs, files in os.walk(caminho_raiz):\n",
        "        dirs[:] = [d for d in dirs if d not in ignorar]\n",
        "        level = root.count(os.sep) - root_level\n",
        "        if level > max_depth:\n",
        "            continue\n",
        "\n",
        "        indent = ' ' * 4 * level\n",
        "        tree_str += f\"{indent}{os.path.basename(root)}/\\n\"\n",
        "\n",
        "        if level < max_depth:\n",
        "            subindent = ' ' * 4 * (level + 1)\n",
        "            for f in files[:10]:\n",
        "                tree_str += f\"{subindent}{f}\\n\"\n",
        "            if len(files) > 10:\n",
        "                tree_str += f\"{subindent}... (+{len(files)-10} arquivos)\\n\"\n",
        "\n",
        "    return tree_str\n",
        "\n",
        "def ler_readme():\n",
        "  ...\n",
        "\n",
        "def inferir_arquitetura_pela_tree(tree_text, prompt):\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": \"Você é um especialista em análise de código e padrões arquiteturais.\"},\n",
        "        {\"role\": \"user\", \"content\": prompt}\n",
        "    ]\n",
        "\n",
        "    inputs = tokenizer.apply_chat_template(messages, tokenize=True, return_dict=True, return_tensors=\"pt\", add_generation_prompt=True).to(model.device)\n",
        "\n",
        "    generated_ids = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=1024,\n",
        "        temperature=0.2\n",
        "    )\n",
        "\n",
        "    return tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0].split(\"assistant\")[-1].strip()"
      ],
      "metadata": {
        "id": "ljS_0AbYW6av"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Clonando repositório objeto da análise"
      ],
      "metadata": {
        "id": "K9tAVTmwXJBp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/Mintplex-Labs/anything-llm.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L7LLgO6cXDSj",
        "outputId": "68167357-4431-476b-9dac-1efa0085e5df",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'anything-llm' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Definição do prompt"
      ],
      "metadata": {
        "id": "WlKL1n4ieBmh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"\n",
        "Aja como um Arquiteto de Software Sênior.\n",
        "Analise a estrutura de arquivos (Tree) abaixo.\n",
        "\n",
        "TAREFA:\n",
        "Classifique a arquitetura deste projeto escolhendo dentre estes padrões:\n",
        "[Camadas, Pipe-and-Filters, Client-Server, Peer-to-Peer, Microservices, Blockchain, SOA, Publish-Subscribe, Shared-Data, Blackboard].\n",
        "\n",
        "IMPORTANTE:\n",
        "Você DEVE fornecer 5 palpites diferentes, com uma porcentagem de confiança para cada um.\n",
        "\n",
        "Siga ESTRITAMENTE este formato de resposta:\n",
        "\n",
        "1. [Nome do Padrão] - [XX]%\n",
        "   Justificativa: [Breve explicação baseada em pastas específicas]\n",
        "\n",
        "2. [Nome do Padrão] - [XX]%\n",
        "   Justificativa: [...]\n",
        "\n",
        "(Repita até o 5)\n",
        "\n",
        "--- INÍCIO DA TREE ---\n",
        "{}\n",
        "--- FIM DA TREE ---\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "cPDEKDHdeAwQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Por fim, executamos o modelo e esperamos pelo resultado da análise (aprox. 1min)"
      ],
      "metadata": {
        "id": "TrMYQG1oXfvK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "repo_path = \"./anything-llm\"\n",
        "tree_visual = gerar_arvore_diretorios(repo_path)\n",
        "\n",
        "print(f\"Estrutura capturada (primeiras 10 linhas):\\n{'\\n'.join(tree_visual.splitlines()[:10])}...\\n\")\n",
        "\n",
        "print(\"Aguardando análise do modelo...\")\n",
        "\n",
        "resultado = inferir_arquitetura_pela_tree(tree_visual, prompt.format(tree_visual))\n",
        "\n",
        "clear_output(wait=True)\n",
        "\n",
        "print(\"Análise gerada:\")\n",
        "print(resultado)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0wPqmtQeXZGF",
        "outputId": "dfb9a2c8-f2a4-4ec6-fc99-5687e662a74e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Análise gerada:\n",
            "Claro, vou analisar a estrutura de arquivos e classificar a arquitetura deste projeto usando os padrões mencionados. Aqui estão meus palpites com suas respectivas porcentagens de confiança:\n",
            "\n",
            "1. **Pipe-and-Filters** - 20%\n",
            "   Justificativa: O projeto usa um pipeline de processamento que envolve várias fases de transformação e filtragem de dados, conforme mostrado nas pastas `server`, `utils`, e `middleware`.\n",
            "\n",
            "2. **Client-Server** - 30%\n",
            "   Justificativa: A arquitetura é dividida em duas camadas principais: o cliente (front-end) e o servidor (back-end). O cliente envia solicitações ao servidor, que processa as informações e retorna resultados.\n",
            "\n",
            "3. **Peer-to-Peer** - 10%\n",
            "   Justificativa: Não há evidências de comunicação entre peers no projeto. Todos os componentes são independentes e operam em sua própria instância.\n",
            "\n",
            "4. **Microservices** - 10%\n",
            "   Justificativa: O projeto é dividido em vários microserviços, cada um com sua responsabilidade específica. Por exemplo, o `server` contém várias rotas e serviços, enquanto o `frontend` é responsável pela interface do usuário.\n",
            "\n",
            "5. **Blockchain** - 5%\n",
            "   Justificativa: Não há evidências de uso de blockchain no projeto. Todas as transações são realizadas de forma centralizada.\n",
            "\n",
            "---\n",
            "\n",
            "### Explicação dos Palpites\n",
            "\n",
            "1. **Pipe-and-Filters**: O projeto utiliza um pipeline de processamento que envolve várias fases de transformação e filtragem de dados, conforme mostrado nas pastas `server`, `utils`, e `middleware`. Isso sugere que o sistema está organizado em camadas, onde cada camada realiza uma função específica antes da próxima.\n",
            "\n",
            "2. **Client-Server**: A arquitetura é dividida em duas camadas principais: o cliente (front-end) e o servidor (back-end). O cliente envia solicitações ao servidor, que processa as informações e retorna resultados. Isso indica que o sistema tem uma estrutura cliente-servidor, onde o cliente se conecta ao servidor para obter dados ou executar tarefas.\n",
            "\n",
            "3. **Peer-to-Peer**: Não há evidências de comunicação entre peers no projeto. Todos os componentes são independentes e operam em sua própria instância. Isso sugere que o sistema não possui uma estrutura peer-to-peer.\n",
            "\n",
            "4. **Microservices**: O projeto é dividido em vários microserviços, cada um com sua responsabilidade específica. Por exemplo, o `server` contém várias rotas e serviços, enquanto o `frontend` é responsável pela interface do usuário. Isso sugere que o sistema está organizado em microserviços, onde cada microserviço tem uma função específica.\n",
            "\n",
            "5. **Blockchain**: Não há evidências de uso de blockchain no projeto. Todas as transações são realizadas de forma centralizada. Isso sugere que o sistema não possui uma estrutura blockchain.\n",
            "\n",
            "Esses palpites foram baseados na estrutura geral do projeto e nos nomes das pastas. Se você tiver mais perguntas ou precisar de mais detalhes sobre algum aspecto específico, sinta-se à vontade para perguntar!\n"
          ]
        }
      ]
    }
  ]
}