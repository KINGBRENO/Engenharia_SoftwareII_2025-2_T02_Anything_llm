{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Instalação e importação de módulos"
      ],
      "metadata": {
        "id": "y3fFUA-JdFOh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "PngmsobmWP3j"
      },
      "outputs": [],
      "source": [
        "!pip install -q transformers torch accelerate"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import time\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from IPython.display import clear_output\n",
        "from google.colab import drive"
      ],
      "metadata": {
        "id": "gKi2HeXLbeuS"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Antes de prosseguir, verificar se está com a GPU habilitada. A inferência demora demais se estiver usando apenas CPU."
      ],
      "metadata": {
        "id": "2l5kMj5acedb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"GPU disponível?\", torch.cuda.is_available())\n",
        "\n",
        "if not torch.cuda.is_available():\n",
        "    print(\"Ative a GPU em Runtime -> Change runtime type -> T4 GPU.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uHa6LnMLau4I",
        "outputId": "1da24a75-31f2-4776-bbbd-5fbf048e9a5d",
        "collapsed": true
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU disponível? True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Carregando o modelo Qwen2.5-Coder-1.5B-Instruct do HuggingFace"
      ],
      "metadata": {
        "id": "xJpw8_ENWo6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_id = \"Qwen/Qwen2.5-Coder-1.5B-Instruct\"\n",
        "\n",
        "print(f\"Carregando {model_id}...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    dtype=torch.float16,\n",
        "    device_map=\"cuda\"\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xVlrxPC-Wk_Z",
        "outputId": "89f5ceea-4a6b-483d-e587-df39f14b97c3",
        "collapsed": true
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Carregando Qwen/Qwen2.5-Coder-1.5B-Instruct...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Definições de funções úteis: **Geração de árvore de diretórios a partir do repositório clonado** e **função de inferência** com prompt."
      ],
      "metadata": {
        "id": "N_4ISyH0W43Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def gerar_arvore_diretorios(caminho_raiz, max_depth=3, ignorar=['.git', 'node_modules', 'dist', 'build', 'coverage', 'venv', '.github', 'assets']):\n",
        "    tree_str = \"\"\n",
        "    root_level = caminho_raiz.count(os.sep)\n",
        "\n",
        "    for root, dirs, files in os.walk(caminho_raiz):\n",
        "        dirs[:] = [d for d in dirs if d not in ignorar]\n",
        "        level = root.count(os.sep) - root_level\n",
        "        if level > max_depth:\n",
        "            continue\n",
        "\n",
        "        indent = ' ' * 4 * level\n",
        "        tree_str += f\"{indent}{os.path.basename(root)}/\\n\"\n",
        "\n",
        "        if level < max_depth:\n",
        "            subindent = ' ' * 4 * (level + 1)\n",
        "            for f in files[:10]:\n",
        "                tree_str += f\"{subindent}{f}\\n\"\n",
        "            if len(files) > 10:\n",
        "                tree_str += f\"{subindent}... (+{len(files)-10} arquivos)\\n\"\n",
        "\n",
        "    return tree_str\n",
        "\n",
        "def extrair_resumo_readmes(repo_path, max_chars=2000):\n",
        "    readmes_encontrados = \"\"\n",
        "\n",
        "    locais_chave = [\n",
        "        repo_path,\n",
        "        os.path.join(repo_path, \"server\"),\n",
        "        os.path.join(repo_path, \"frontend\"),\n",
        "        os.path.join(repo_path, \"collector\")\n",
        "    ]\n",
        "\n",
        "    for pasta in locais_chave:\n",
        "        caminho_arquivo = os.path.join(pasta, \"README.md\")\n",
        "\n",
        "        if os.path.exists(caminho_arquivo):\n",
        "            try:\n",
        "                with open(caminho_arquivo, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
        "                    conteudo = f.read()\n",
        "\n",
        "                    conteudo = \"\\n\".join([line for line in conteudo.splitlines() if line.strip()])\n",
        "\n",
        "                    resumo = conteudo[:max_chars]\n",
        "\n",
        "                    nome_pasta = os.path.basename(pasta)\n",
        "                    if nome_pasta == os.path.basename(repo_path):\n",
        "                        nome_pasta = \"RAIZ DO PROJETO\"\n",
        "\n",
        "                    readmes_encontrados += f\"\\n--- CONTEÚDO DO README ({nome_pasta}) ---\\n\"\n",
        "                    readmes_encontrados += resumo\n",
        "                    readmes_encontrados += \"\\n... (conteúdo truncado para economizar memória)...\\n\"\n",
        "            except Exception as e:\n",
        "                print(f\"Erro ao ler {caminho_arquivo}: {e}\")\n",
        "\n",
        "    return readmes_encontrados\n",
        "\n",
        "def inferir_arquitetura_pela_tree(tree_text, prompt):\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": \"Você é um especialista em análise de código e padrões arquiteturais.\"},\n",
        "        {\"role\": \"user\", \"content\": prompt}\n",
        "    ]\n",
        "\n",
        "    inputs = tokenizer.apply_chat_template(messages, tokenize=True, return_dict=True, return_tensors=\"pt\", add_generation_prompt=True).to(model.device)\n",
        "\n",
        "    generated_ids = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=1024,\n",
        "        temperature=0.2\n",
        "    )\n",
        "\n",
        "    return tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0].split(\"assistant\")[-1].strip()"
      ],
      "metadata": {
        "id": "ljS_0AbYW6av"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Clonando repositório objeto da análise"
      ],
      "metadata": {
        "id": "K9tAVTmwXJBp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/Mintplex-Labs/anything-llm.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L7LLgO6cXDSj",
        "outputId": "f76862f9-c065-4f36-bb0e-a268975574d4",
        "collapsed": true
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'anything-llm' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Definição do prompt (en/pt)"
      ],
      "metadata": {
        "id": "WlKL1n4ieBmh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"\n",
        "Analise os artefatos abaixo de um repositório de software:\n",
        "1. A estrutura de pastas (Tree).\n",
        "2. Trechos dos arquivos de documentação (READMEs).\n",
        "\n",
        "TAREFA:\n",
        "Classifique a arquitetura deste projeto escolhendo padrões arquiteturais que se encaixem a ele\n",
        "(ex.: Camadas, Pipe-and-Filters, Client-Server, Peer-to-Peer, Microservices, Blockchain, SOA, Publish-Subscribe, Shared-Data, Blackboard).\n",
        "\n",
        "IMPORTANTE:\n",
        "Você DEVE fornecer 5 palpites diferentes, com uma porcentagem de confiança para cada um.\n",
        "\n",
        "Siga ESTRITAMENTE este formato de resposta:\n",
        "\n",
        "1. [Nome do Padrão] - [XX]%\n",
        "   Justificativa: [Explicação baseada em pastas específicas]\n",
        "\n",
        "2. [Nome do Padrão] - [XX]%\n",
        "   Justificativa: [...]\n",
        "\n",
        "(Repita até o 5)\n",
        "\n",
        "--- INÍCIO DA TREE ---\n",
        "{}\n",
        "--- FIM DA TREE ---\n",
        "\n",
        "--- INÍCIO DOS READMES ---\n",
        "{}\n",
        "--- FIM DOS READMES ---\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "NfCs-_38DXh6"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Por fim, executamos o modelo e esperamos pelo resultado da análise (aprox. 40s)"
      ],
      "metadata": {
        "id": "TrMYQG1oXfvK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "repo_path = \"./anything-llm\"\n",
        "tree_visual = gerar_arvore_diretorios(repo_path)\n",
        "readmes_content = extrair_resumo_readmes(repo_path)\n",
        "\n",
        "print(f\"Estrutura capturada (primeiras 10 linhas):\\n{'\\n'.join(tree_visual.splitlines()[:10])}...\\n\")\n",
        "\n",
        "print(\"Aguardando análise do modelo...\")\n",
        "\n",
        "start = time.perf_counter()\n",
        "resultado = inferir_arquitetura_pela_tree(tree_visual, prompt.format(tree_visual, readmes_content))\n",
        "end = time.perf_counter()\n",
        "\n",
        "tempo_execucao = end-start\n",
        "\n",
        "clear_output(wait=True)\n",
        "\n",
        "print(\"Análise gerada:\\n\")\n",
        "print(f\"Tempo de execução = {tempo_execucao}s\\n\")\n",
        "print(resultado)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0wPqmtQeXZGF",
        "outputId": "e0d00750-ee44-45f9-c2d8-5b06a4c6465a",
        "collapsed": true
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Análise gerada:\n",
            "\n",
            "Tempo de execução = 24.766483065000102s\n",
            "\n",
            "1. **Camadas** - O projeto está organizado em camadas, onde as funcionalidades são divididas entre diferentes diretórios. Por exemplo, `server` contém a lógica back-end, enquanto `collector` contém a lógica front-end. Isso facilita a manutenção e escalabilidade do sistema.\n",
            "\n",
            "2. **Pipe-and-Filters** - O projeto usa pipes e filtros para organizar e manipular dados. Por exemplo, o arquivo `utils/collectorApi/index.js` define um pipe para processar dados da API, enquanto o arquivo `utils/collector/processRawText/index.js` define um filtro para processar texto raw.\n",
            "\n",
            "3. **Client-Server** - O projeto é cliente-servidor, onde o cliente (front-end) envia solicitações ao servidor (back-end). Isso permite a comunicação entre diferentes partes do sistema e a escalabilidade.\n",
            "\n",
            "4. **Peer-to-Peer** - O projeto não utiliza peer-to-peer para compartilhar dados ou recursos. No entanto, o uso de serviços como Docker e Kubernetes pode ser considerado um tipo de peer-to-peer na forma de hospedagem de aplicativos.\n",
            "\n",
            "5. **Microservices** - O projeto é composto por vários microserviços, cada um com sua própria responsabilidade. Isso permite a modularização e escalabilidade do sistema.\n",
            "\n",
            "---\n",
            "\n",
            "Palpite 1: Camadas - 50%\n",
            "\n",
            "Justificação: O projeto está organizado em camadas, onde as funcionalidades são divididas entre diferentes diretórios. Isso facilita a manutenção e escalabilidade do sistema.\n",
            "\n",
            "Palpite 2: Pipe-and-Filters - 50%\n",
            "\n",
            "Justificação: O projeto usa pipes e filtros para organizar e manipular dados. Por exemplo, o arquivo `utils/collectorApi/index.js` define um pipe para processar dados da API, enquanto o arquivo `utils/collector/processRawText/index.js` define um filtro para processar texto raw.\n",
            "\n",
            "Palpite 3: Client-Server - 50%\n",
            "\n",
            "Justificação: O projeto é cliente-servidor, onde o cliente (front-end) envia solicitações ao servidor (back-end). Isso permite a comunicação entre diferentes partes do sistema e a escalabilidade.\n",
            "\n",
            "Palpite 4: Peer-to-Peer - 25%\n",
            "\n",
            "Justificação: O projeto não utiliza peer-to-peer para compartilhar dados ou recursos. No entanto, o uso de serviços como Docker e Kubernetes pode ser considerado um tipo de peer-to-peer na forma de hospedagem de aplicativos.\n",
            "\n",
            "Palpite 5: Microservices - 25%\n",
            "\n",
            "Justificação: O projeto é composto por vários microserviços, cada um com sua própria responsabilidade. Isso permite a modularização e escalabilidade do sistema.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Salvar entradas e saídas do Drive"
      ],
      "metadata": {
        "id": "e0kHemKh_a_4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')\n",
        "\n",
        "input_path = '/content/drive/MyDrive/Qwen-analysis/input-Qwen.txt'\n",
        "output_path = '/content/drive/MyDrive/Qwen-analysis/output-Qwen.txt'\n",
        "\n",
        "with open(input_path, 'w') as f:\n",
        "    f.write(tree_visual)\n",
        "    f.write(readmes_content)\n",
        "\n",
        "with open(output_path, 'w') as f:\n",
        "    f.write(f\"TEMPO DE EXECUCAO={tempo_execucao}\\n\")\n",
        "    f.write(resultado)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "ZYSwOI-O_ftl",
        "outputId": "0084cf9c-6acb-4297-a0d5-6b08d97121bc"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    }
  ]
}