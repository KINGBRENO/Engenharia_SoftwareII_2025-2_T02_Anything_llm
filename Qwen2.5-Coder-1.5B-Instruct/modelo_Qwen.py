# -*- coding: utf-8 -*-
"""modelo-Qwen.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zWuxAdiRz9AQqnKo4Arehrjgsu3Hz3-P

Instalação e importação de módulos
"""

!pip install -q transformers torch accelerate

import os
import torch
import time
from transformers import AutoTokenizer, AutoModelForCausalLM
from IPython.display import clear_output
from google.colab import drive

"""Antes de prosseguir, verificar se está com a GPU habilitada. A inferência demora demais se estiver usando apenas CPU."""

print("GPU disponível?", torch.cuda.is_available())

if not torch.cuda.is_available():
    print("Ative a GPU em Runtime -> Change runtime type -> T4 GPU.")

"""Carregando o modelo Qwen2.5-Coder-1.5B-Instruct do HuggingFace"""

model_id = "Qwen/Qwen2.5-Coder-1.5B-Instruct"

print(f"Carregando {model_id}...")
tokenizer = AutoTokenizer.from_pretrained(model_id)
model = AutoModelForCausalLM.from_pretrained(
    model_id,
    dtype=torch.float16,
    device_map="cuda"
)

"""Definição de função de inferência do modelo a partir do prompt."""

def inferir_arquitetura_pela_tree(prompt):
    messages = [
        {"role": "system", "content": "Você é um especialista em análise de código e padrões arquiteturais."},
        {"role": "user", "content": prompt}
    ]

    inputs = tokenizer.apply_chat_template(messages, tokenize=True, return_dict=True, return_tensors="pt", add_generation_prompt=True).to(model.device)

    generated_ids = model.generate(
        **inputs,
        max_new_tokens=1024,
        temperature=0.2
    )

    return tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0].split("assistant")[-1].strip()

"""Definição das entradas

"""

input = []

for idx in range(1,5):
  with open(f"./inputs/entrada{idx}.txt", 'r', encoding="utf-8") as f:
    input.append(f.read())

"""Montagem do prompt"""

prompt = """
Act as a Senior Software Architect.

Analyze the following software repository artifacts:
1. The folder structure (File Tree);
2. The package.json file;
3. The commit history (last 100 commits).

TASK:
Identify which architectural patterns are present in the analyzed repository.
(Examples of architectural patterns: Layered Architecture, Client-Server, Microservices, Monolithic, Pipe-and-Filters, Event-Driven Architecture, Service-Oriented Architecture (SOA), Peer-to-Peer (P2P), Blackboard, Hexagonal Architecture (Ports and Adapters), Clean Architecture, Serverless, MVC (Model-View-Controller), CQRS, Microkernel (Plug-ins), Broker Pattern, Master-Slave).

Provide 5 predictions with associated probability percentages.

STRICTLY follow this format:
1. [Pattern Name] - [XX]%
   Justification: [Explanation for the prediction based on the artifacts]

--- START OF TREE ---
{}
--- END OF TREE ---

--- START OF PACKAGE.JSON ---
{}
--- END OF PACKAGE.JSON ---

--- START OF COMMIT HISTORY ---
{}
--- END OF COMMIT HISTORY ---
"""

"""Por fim, executamos o modelo e esperamos pelo resultado da análise (aprox. 40s)"""

repo_path = "./anything-llm"

print("Aguardando análise do modelo...")

start = time.perf_counter()
resultado = inferir_arquitetura_pela_tree(prompt.format(input[3], input[1], input[2]))
end = time.perf_counter()

tempo_execucao = end-start

clear_output(wait=True)

print("Análise gerada:\n")
print(f"Tempo de execução = {tempo_execucao}s\n")

with open(f"output.txt", 'w') as f:
  f.write(resultado)

"""Fazer download das entradas e saídas"""

from google.colab import files

files.download("output.txt")
