# -*- coding: utf-8 -*-
"""modelo-1-5b.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zWuxAdiRz9AQqnKo4Arehrjgsu3Hz3-P

Instalação e importação de módulos
"""

!pip install -q transformers torch accelerate

import os
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from IPython.display import clear_output

"""Antes de prosseguir, verificar se está com a GPU habilitada. A inferência demora demais se estiver usando apenas CPU."""

print("GPU disponível?", torch.cuda.is_available())

if not torch.cuda.is_available():
    print("Ative a GPU em Runtime -> Change runtime type -> T4 GPU.")

"""Carregando o modelo Qwen2.5-Coder-1.5B-Instruct do HuggingFace"""

model_id = "Qwen/Qwen2.5-Coder-1.5B-Instruct"

print(f"Carregando {model_id}...")
tokenizer = AutoTokenizer.from_pretrained(model_id)
model = AutoModelForCausalLM.from_pretrained(
    model_id,
    dtype=torch.float16,
    device_map="cuda"
)

"""Definições de funções úteis: **Geração de árvore de diretórios a partir do repositório clonado** e **função de inferência** com prompt."""

def gerar_arvore_diretorios(caminho_raiz, max_depth=3, ignorar=['.git', 'node_modules', 'dist', 'build', 'coverage', 'venv', '.github', 'assets']):
    tree_str = ""
    root_level = caminho_raiz.count(os.sep)

    for root, dirs, files in os.walk(caminho_raiz):
        dirs[:] = [d for d in dirs if d not in ignorar]
        level = root.count(os.sep) - root_level
        if level > max_depth:
            continue

        indent = ' ' * 4 * level
        tree_str += f"{indent}{os.path.basename(root)}/\n"

        if level < max_depth:
            subindent = ' ' * 4 * (level + 1)
            for f in files[:10]:
                tree_str += f"{subindent}{f}\n"
            if len(files) > 10:
                tree_str += f"{subindent}... (+{len(files)-10} arquivos)\n"

    return tree_str

def ler_readme():
  ...

def inferir_arquitetura_pela_tree(tree_text, prompt):
    messages = [
        {"role": "system", "content": "Você é um especialista em análise de código e padrões arquiteturais."},
        {"role": "user", "content": prompt}
    ]

    inputs = tokenizer.apply_chat_template(messages, tokenize=True, return_dict=True, return_tensors="pt", add_generation_prompt=True).to(model.device)

    generated_ids = model.generate(
        **inputs,
        max_new_tokens=1024,
        temperature=0.2
    )

    return tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0].split("assistant")[-1].strip()

"""Clonando repositório objeto da análise"""

!git clone https://github.com/Mintplex-Labs/anything-llm.git

"""Definição do prompt"""

prompt = """
Aja como um Arquiteto de Software Sênior.
Analise a estrutura de arquivos (Tree) abaixo.

TAREFA:
Classifique a arquitetura deste projeto escolhendo dentre estes padrões:
[Camadas, Pipe-and-Filters, Client-Server, Peer-to-Peer, Microservices, Blockchain, SOA, Publish-Subscribe, Shared-Data, Blackboard].

IMPORTANTE:
Você DEVE fornecer 5 palpites diferentes, com uma porcentagem de confiança para cada um.

Siga ESTRITAMENTE este formato de resposta:

1. [Nome do Padrão] - [XX]%
   Justificativa: [Breve explicação baseada em pastas específicas]

2. [Nome do Padrão] - [XX]%
   Justificativa: [...]

(Repita até o 5)

--- INÍCIO DA TREE ---
{}
--- FIM DA TREE ---
"""

"""Por fim, executamos o modelo e esperamos pelo resultado da análise (aprox. 1min)"""

repo_path = "./anything-llm"
tree_visual = gerar_arvore_diretorios(repo_path)

print(f"Estrutura capturada (primeiras 10 linhas):\n{'\n'.join(tree_visual.splitlines()[:10])}...\n")

print("Aguardando análise do modelo...")

resultado = inferir_arquitetura_pela_tree(tree_visual, prompt.format(tree_visual))

clear_output(wait=True)

print("Análise gerada:")
print(resultado)