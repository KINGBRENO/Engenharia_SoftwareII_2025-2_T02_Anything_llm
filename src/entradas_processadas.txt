

===== C:\Users\Breno\AppData\Local\Temp\repo_1u3zfsju\repo\BARE_METAL.md =====
Run AnythingLLM in production without Docker
[!WARNING]
This method of deployment is **not supported** by the core-team and is to be used as a reference for your deployment.
You are fully responsible for securing your deployment and data in this mode.
**Any issues** experienced from bare-metal or non-containerized deployments will be **not** answered or supported.
Here you can find the scripts and known working process to run AnythingLLM outside of a Docker container.
Minimum Requirements
[!TIP]
You should aim for at least 2GB of RAM. Disk storage is proportional to however much data
you will be storing (documents, vectors, models, etc). Minimum 10GB recommended.
NodeJS v18
Yarn
Getting started
1. Clone the repo into your server as the user who the application will run as.
`git clone git@github.com:Mintplex-Labs/anything-llm.git`
2. `cd anything-llm` and run `yarn setup`. This will install all dependencies to run in production as well as debug the application.
3. `cp server/.env.example server/.env` to create the basic ENV file for where instance settings will be read from on service start.
4. Ensure that the `server/.env` file has _at least_ these keys to start. These values will persist and this file will be automatically written and managed after your first successful boot.
5. Edit the `frontend/.env` file for the `VITE_BASE_API` to now be set to `/api`. This is documented in the .env for which one you should use.
To start the application
AnythingLLM is comprised of three main sections. The `frontend`, `server`, and `collector`. When running in production you will be running `server` and `collector` on two different processes, with a build step for compilation of the frontend.
1. Build the frontend application.
`cd frontend && yarn build` - this will produce a `frontend/dist` folder that will be used later.
2. Copy `frontend/dist` to `server/public` - `cp -R frontend/dist server/public`.
This should create a folder in `server` named `public` which contains a top level `index.html` file and various other files/folders.
3. Migrate and prepare your database file.
4. Boot the server in production
`cd server && NODE_ENV=production node index.js &`
5. Boot the collection in another process
`cd collector && NODE_ENV=production node index.js &`
AnythingLLM should now be running on `
Updating AnythingLLM
To update AnythingLLM with future updates you can `git pull origin master` to pull in the latest code and then repeat steps 2 - 5 to deploy with all changes fully.
_note_ You should ensure that each folder runs `yarn` again to ensure packages are up to date in case any dependencies were added, changed, or removed.
_note_ You should `pkill node` before running an update so that you are not running multiple AnythingLLM processes on the same instance as this can cause conflicts.
Example update script
Using Nginx?
If you are using Nginx, you can use the following example configuration to proxy the requests to the server. Chats for streaming require **websocket** connections, so you need to ensure that the Nginx configuration is set up to support websockets. You can do this with a simple reverse proxy configuration.


===== C:\Users\Breno\AppData\Local\Temp\repo_1u3zfsju\repo\CONTRIBUTING.md =====
Contributing to AnythingLLM
AnythingLLM is an open-source project and we welcome contributions from the community.
Reporting Issues
If you encounter a bug or have a feature request, please open an issue on the
GitHub issue tracker.
Picking an issue
We track issues on the GitHub issue tracker. If you are looking for something to
work on, check the good first issue label. These issues are typically the best described and have the smallest scope. There may be issues that are not labeled as good first issue, but are still a good starting point.
If there's an issue you are interested in working on, please leave a comment on the issue. This will help us avoid duplicate work. Additionally, if you have questions about the issue, please ask them in the issue comments. We are happy to provide guidance on how to approach the issue.
Before you start
Keep in mind that we are a small team and have limited resources. We will do our best to review and merge your PRs, but please be patient. Ultimately, **we become the maintainer** of your changes. It is our responsibility to make sure that the changes are working as expected and are of high quality as well as being compatible with the rest of the project both for existing users and for future users & features.
Before you start working on an issue, please read the following so that you don't waste time on something that is not a good fit for the project or is more suitable for a personal fork. We would rather answer a comment on an issue than close a PR after you've spent time on it. Your time is valuable and we appreciate your time and effort to make AnythingLLM better.
0. (most important) If you are making a PR that does not have a corresponding issue, **it will not be merged.** _The only exception to this is language translations._
1. If you are modifying the permission system for a new role or something custom, you are likely better off forking the project and building your own version since this is a core part of the project and is only to be maintained by the AnythingLLM team.
2. Integrations (LLM, Vector DB, etc.) are reviewed at our discretion. We will eventually get to them. Do not expect us to merge your integration PR instantly since there are often many moving parts and we want to make sure we get it right. We will get to it!
3. It is our discretion to merge or not merge a PR. We value every contribution, but we also value the quality of the code and the user experience we envision for the project. It is a fine line to walk when running a project like this and please understand that merging or not merging a PR is not a reflection of the quality of the contribution and is not personal. We will do our best to provide feedback on the PR and help you make the changes necessary to get it merged.
4. **Security** is always important. If you have a security concern, please do not open an issue. Instead, please open a CVE on our designated reporting platform Huntr or contact us at team@mintplexlabs.com.
Configuring Git
First, fork the repository on GitHub, then clone your fork:
Then add the main repository as a remote:
Setting up your development environment
In the root of the repository, run:
This will install the dependencies, set up the proper and expected ENV files for the project, and run the prisma setup script.
Next, run:
This will start the server, frontend, and collector in development mode. Changes to the code will be hot reloaded.
Best practices for pull requests
For the best chance of having your pull request accepted, please follow these guidelines:
1. Unit test all bug fixes and new features. Your code will not be merged if it
 doesn't have tests.
1. If you change the public API, update the documentation in the `anythingllm-docs` repository.
1. Aim to minimize the number of changes in each pull request. Keep to solving
 one problem at a time, when possible.
1. Before marking a pull request ready-for-review, do a self review of your code.
 Is it clear why you are making the changes? Are the changes easy to understand?
1. Use conventional commit messages as pull request titles. Examples:
 * New feature: `feat: adding foo API`
 * Bug fix: `fix: issue with foo API`
 * Documentation change: `docs: adding foo API documentation`
1. If your pull request is a work in progress, leave the pull request as a draft.
 We will assume the pull request is ready for review when it is opened.
1. When writing tests, test the error cases. Make sure they have understandable
 error messages.
Project structure
The core library is written in Node.js. There are additional sub-repositories for the embed widget and browser extension. These are not part of the core AnythingLLM project, but are maintained by the AnythingLLM team.
`server`: Node.js server source code
`frontend`: React frontend source code
`collector`: Python collector source code
Release process
Changes to the core AnythingLLM project are released through the `master` branch. When a PR is merged into `master`, a new version of the package is published to Docker and GitHub Container Registry under the `latest` tag.
When a new version is released, the following steps are taken a new image is built and pushed to Docker Hub and GitHub Container Registry under the assoicated version tag. Version tags are of the format `v<major>.<minor>.<patch>` and are pinned code, while `latest` is the latest version of the code at any point in time.
Desktop propogation
Changes to the desktop app are downstream of the core AnythingLLM project. Releases of the desktop app are published at the same time as the core AnythingLLM project. Code from the core AnythingLLM project is copied into the desktop app into an Electron wrapper. The Electron wrapper that wraps around the core AnythingLLM project is **not** part of the core AnythingLLM project, but is maintained by the AnythingLLM team.
License
By contributing to AnythingLLM (this repository), you agree to license your contributions under the MIT license.


===== C:\Users\Breno\AppData\Local\Temp\repo_1u3zfsju\repo\pull_request_template.md =====
### Pull Request Type
<!-- For change type, change [ ] to [x]. -->
[ ] ‚ú® feat
[ ] üêõ fix
[ ] ‚ôªÔ∏è refactor
[ ] üíÑ style
[ ] üî® chore
[ ] üìù docs
Relevant Issues
<!-- Use "resolves #xxx" to auto resolve on merge. Otherwise, please use "connect #xxx" -->
resolves #xxx
What is in this change?
<!-- Describe the changes in this PR that are impactful to the repo. -->
Additional Information
<!-- Add any other context about the Pull Request here that was not captured above. -->
Developer Validations
<!-- All of the applicable items should be checked. -->
[ ] I ran `yarn lint` from the root of the repo & committed changes
[ ] Relevant documentation has been updated
[ ] I have tested my code functionality
[ ] Docker build succeeds locally


===== C:\Users\Breno\AppData\Local\Temp\repo_1u3zfsju\repo\README.md =====
<a name="readme-top"></a>
<p align="center">
 <a href=" src=" alt="AnythingLLM logo"></a>
</p>
<div align='center'>
<a href=" target="_blank"><img src=" alt="Mintplex-Labs%2Fanything-llm | Trendshift" style="width: 250px; height: 55px;" width="250" height="55"/></a>
</div>
<p align="center">
 <b>AnythingLLM:</b> The all-in-one AI app you were looking for.<br />
 Chat with your docs, use AI Agents, hyper-configurable, multi-user, & no frustrating setup required.
</p>
<p align="center">
 <a href=" target="_blank">
 <img src=" alt="Discord">
 </a> |
 <a href=" target="_blank">
 <img src=" alt="License">
 </a> |
 <a href=" target="_blank">
 Docs
 </a> |
 <a href=" target="_blank">
 Hosted Instance
 </a>
</p>
<p align="center">
 <b>English</b> ¬∑ <a href='./locales/README.zh-CN.md'>ÁÆÄ‰Ωì‰∏≠Êñá</a> ¬∑ <a href='./locales/README.ja-JP.md'>Êó•Êú¨Ë™û</a>
</p>
<p align="center">
üëâ AnythingLLM for desktop (Mac, Windows, & Linux)! <a href=" target="_blank"> Download Now</a>
</p>
A full-stack application that enables you to turn any document, resource, or piece of content into context that any LLM can use as a reference during chatting. This application allows you to pick and choose which LLM or Vector Database you want to use as well as supporting multi-user management and permissions.
<details>
<summary><kbd>Watch the demo!</kbd></summary>
[](
</details>
Product Overview
AnythingLLM is a full-stack application where you can use commercial off-the-shelf LLMs or popular open source LLMs and vectorDB solutions to build a private ChatGPT with no compromises that you can run locally as well as host remotely and be able to chat intelligently with any documents you provide it.
AnythingLLM divides your documents into objects called `workspaces`. A Workspace functions a lot like a thread, but with the addition of containerization of your documents. Workspaces can share documents, but they do not talk to each other so you can keep your context for each workspace clean.
Cool features of AnythingLLM
üÜï **Full MCP-compatibility**
üÜï **No-code AI Agent builder**
üñºÔ∏è **Multi-modal support (both closed and open-source LLMs!)**
**Custom AI Agents**
üë§ Multi-user instance support and permissioning _Docker version only_
ü¶æ Agents inside your workspace (browse the web, etc)
üí¨ Custom Embeddable Chat widget for your website _Docker version only_
üìñ Multiple document type support (PDF, TXT, DOCX, etc)
Simple chat UI with Drag-n-Drop functionality and clear citations.
100% Cloud deployment ready.
Works with all popular closed and open-source LLM providers.
Built-in cost & time-saving measures for managing very large documents compared to any other chat UI.
Full Developer API for custom integrations!
Much more...install and find out!
Supported LLMs, Embedder Models, Speech models, and Vector Databases
Large Language Models (LLMs):**
Any open-source llama.cpp compatible model
OpenAI
OpenAI (Generic)
Azure OpenAI
AWS Bedrock
Anthropic
NVIDIA NIM (chat models)
Google Gemini Pro
Hugging Face (chat models)
Ollama (chat models)
LM Studio (all models)
LocalAI (all models)
Together AI (chat models)
Fireworks AI (chat models)
Perplexity (chat models)
OpenRouter (chat models)
DeepSeek (chat models)
Mistral
Groq
Cohere
KoboldCPP
LiteLLM
Text Generation Web UI
Apipie
xAI
Novita AI (chat models)
PPIO
Moonshot AI
Microsoft Foundry Local
CometAPI (chat models)
Embedder models:**
AnythingLLM Native Embedder (default)
OpenAI
Azure OpenAI
LocalAI (all)
Ollama (all)
LM Studio (all)
Cohere
Audio Transcription models:**
AnythingLLM Built-in (default)
OpenAI
TTS (text-to-speech) support:**
Native Browser Built-in (default)
PiperTTSLocal - runs in browser
OpenAI TTS
ElevenLabs
Any OpenAI Compatible TTS service.
STT (speech-to-text) support:**
Native Browser Built-in (default)
Vector Databases:**
LanceDB (default)
PGVector
Astra DB
Pinecone
Chroma & ChromaCloud
Weaviate
Qdrant
Milvus
Zilliz
Technical Overview
This monorepo consists of six main sections:
`frontend`: A viteJS + React frontend that you can run to easily create and manage all your content the LLM can use.
`server`: A NodeJS express server to handle all the interactions and do all the vectorDB management and LLM interactions.
`collector`: NodeJS express server that processes and parses documents from the UI.
`docker`: Docker instructions and build process + information for building from source.
`embed`: Submodule for generation & creation of the web embed widget.
`browser-extension`: Submodule for the chrome browser extension.
üõ≥ Self-Hosting
Mintplex Labs & the community maintain a number of deployment methods, scripts, and templates that you can use to run AnythingLLM locally. Refer to the table below to read how to deploy on your preferred environment or to automatically deploy.
| Docker | AWS | GCP | Digital Ocean | Render.com |
|----------------------------------------|----|-----|---------------|------------|
| [![Deploy on Docker][docker-btn]][docker-deploy] | [![Deploy on AWS][aws-btn]][aws-deploy] | [![Deploy on GCP][gcp-btn]][gcp-deploy] | [![Deploy on DigitalOcean][do-btn]][do-deploy] | [![Deploy on Render.com][render-btn]][render-deploy] |
| Railway | RepoCloud | Elestio | Northflank |
| --- | --- | --- | --- |
| [![Deploy on Railway][railway-btn]][railway-deploy] | [![Deploy on RepoCloud][repocloud-btn]][repocloud-deploy] | [![Deploy on Elestio][elestio-btn]][elestio-deploy] | [![Deploy on Northflank][northflank-btn]][northflank-deploy] |
or set up a production AnythingLLM instance without Docker ‚Üí
How to setup for development
`yarn setup` To fill in the required `.env` files you'll need in each of the application sections (from root of repo).
 - Go fill those out before proceeding. Ensure `server/.env.development` is filled or else things won't work right.
`yarn dev:server` To boot the server locally (from root of repo).
`yarn dev:frontend` To boot the frontend locally (from root of repo).
`yarn dev:collector` To then run the document collector (from root of repo).
Learn about documents
Learn about vector caching
External Apps & Integrations
_These are apps that are not maintained by Mintplex Labs, but are compatible with AnythingLLM. A listing here is not an endorsement._
Midori AI Subsystem Manager - A streamlined and efficient way to deploy AI systems using Docker container technology.
Coolify - Deploy AnythingLLM with a single click.
GPTLocalhost for Microsoft Word - A local Word Add-in for you to use AnythingLLM in Microsoft Word.
Telemetry & Privacy
AnythingLLM by Mintplex Labs Inc contains a telemetry feature that collects anonymous usage information.
<details>
<summary><kbd>More about Telemetry & Privacy for AnythingLLM</kbd></summary>
Why?
We use this information to help us understand how AnythingLLM is used, to help us prioritize work on new features and bug fixes, and to help us improve AnythingLLM's performance and stability.
Opting out
Set `DISABLE_TELEMETRY` in your server or docker .env settings to "true" to opt out of telemetry. You can also do this in-app by going to the sidebar > `Privacy` and disabling telemetry.
What do you explicitly track?
We will only track usage details that help us make product and roadmap decisions, specifically:
Type of your installation (Docker or Desktop)
When a document is added or removed. No information _about_ the document. Just that the event occurred. This gives us an idea of use.
Type of vector database in use. This helps us prioritize changes when updates arrive for that provider.
Type of LLM provider & model tag in use. This helps us prioritize changes when updates arrive for that provider or model, or combination thereof. eg: reasoning vs regular, multi-modal models, etc.
When a chat is sent. This is the most regular "event" and gives us an idea of the daily-activity of this project across all installations. Again, only the **event** is sent - we have no information on the nature or content of the chat itself.
You can verify these claims by finding all locations `Telemetry.sendTelemetry` is called. Additionally these events are written to the output log so you can also see the specific data which was sent - if enabled. **No IP or other identifying information is collected**. The Telemetry provider is PostHog - an open-source telemetry collection service.
We take privacy very seriously, and we hope you understand that we want to learn how our tool is used, without using annoying popup surveys, so we can build something worth using. The anonymous data is _never_ shared with third parties, ever.
View all telemetry events in source code
</details>
üëã Contributing
Contributing to AnythingLLM - How to contribute to AnythingLLM.
üíñ Sponsors
Premium Sponsors
<!-- premium-sponsors (reserved for $100/mth sponsors who request to be called out here and/or are non-private sponsors) -->
<a href=" target="_blank">
 <img src=" height="100px" alt="User avatar: DCS DIGITAL" />
</a>
<!-- premium-sponsors -->
All Sponsors
<!-- all-sponsors --><a href=" src=" width="60px" alt="User avatar: Jascha" /></a><a href=" src=" width="60px" alt="User avatar: KickAss" /></a><a href=" src=" width="60px" alt="User avatar: ShadowArcanist" /></a><a href=" src=" width="60px" alt="User avatar: Atlas" /></a><a href=" src=" width="60px" alt="User avatar: Predrag StojadinovicÃÅ" /></a><a href=" src=" width="60px" alt="User avatar: Diego Spinola" /></a><a href=" src=" width="60px" alt="User avatar: Kyle" /></a><a href=" src=" width="60px" alt="User avatar: Giulio De Pasquale" /></a><a href=" src=" width="60px" alt="User avatar: " /></a><a href=" src=" width="60px" alt="User avatar: MacStadium" /></a><a href=" src=" width="60px" alt="User avatar: " /></a><a href=" src=" width="60px" alt="User avatar: " /></a><a href=" src=" width="60px" alt="User avatar: " /></a><a href=" src=" width="60px" alt="User avatar: " /></a><a href=" src=" width="60px" alt="User avatar: Dennis" /></a><a href=" src=" width="60px" alt="User avatar: Michael Hamilton, Ph.D." /></a><a href=" src=" width="60px" alt="User avatar: " /></a><a href=" src=" width="60px" alt="User avatar: TernaryLabs" /></a><a href=" src=" width="60px" alt="User avatar: Daniel Cela" /></a><a href=" src=" width="60px" alt="User avatar: Alesso" /></a><a href=" src=" width="60px" alt="User avatar: Rune Mathisen" /></a><a href=" src=" width="60px" alt="User avatar: " /></a><a href=" src=" width="60px" alt="User avatar: " /></a><a href=" src=" width="60px" alt="User avatar: Alan" /></a><a href=" src=" width="60px" alt="User avatar: Damien Peters" /></a><a href=" src=" width="60px" alt="User avatar: DCS Digital" /></a><a href=" src=" width="60px" alt="User avatar: Paul Mcilreavy" /></a><a href=" src=" width="60px" alt="User avatar: Til Wolf" /></a><a href=" src=" width="60px" alt="User avatar: Leopoldo Crhistian Riverin Gomez" /></a><a href=" src=" width="60px" alt="User avatar: AJEsau" /></a><a href=" src=" width="60px" alt="User avatar: Steven VanOmmeren" /></a><a href=" src=" width="60px" alt="User avatar: Casey Boettcher" /></a><a href=" src=" width="60px" alt="User avatar: " /></a><a href=" src=" width="60px" alt="User avatar: Avineet" /></a><a href=" src=" width="60px" alt="User avatar: Chris" /></a><a href=" src=" width="60px" alt="User avatar: mirko" /></a><a href=" src=" width="60px" alt="User avatar: Tim Champ" /></a><a href=" src=" width="60px" alt="User avatar: Peter Mathisen" /></a><a href=" src=" width="60px" alt="User avatar: Ed di Girolamo" /></a><a href=" src=" width="60px" alt="User avatar: Wojciech Mi≈Çkowski" /></a><!-- all-sponsors -->
üåü Contributors
[](
[](
üîó More Products
**[VectorAdmin][vector-admin]:** An all-in-one GUI & tool-suite for managing vector databases.
**[OpenAI Assistant Swarm][assistant-swarm]:** Turn your entire library of OpenAI assistants into one single army commanded from a single agent.
<div align="right">
[
</div>
Copyright ¬© 2025 [Mintplex Labs][profile-link]. <br />
This project is MIT licensed.
<!-- LINK GROUP -->
[back-to-top]: 
[profile-link]: 
[vector-admin]: 
[assistant-swarm]: 
[docker-btn]: ./images/deployBtns/docker.png
[docker-deploy]: ./docker/HOW_TO_USE_DOCKER.md
[aws-btn]: ./images/deployBtns/aws.png
[aws-deploy]: ./cloud-deployments/aws/cloudformation/DEPLOY.md
[gcp-btn]: 
[gcp-deploy]: ./cloud-deployments/gcp/deployment/DEPLOY.md
[do-btn]: 
[do-deploy]: ./cloud-deployments/digitalocean/terraform/DEPLOY.md
[render-btn]: 
[render-deploy]: 
[render-btn]: 
[render-deploy]: 
[railway-btn]: 
[railway-deploy]: 
[repocloud-btn]: 
[repocloud-deploy]: 
[elestio-btn]: 
[elestio-deploy]: 
[northflank-btn]: 
[northflank-deploy]:


===== C:\Users\Breno\AppData\Local\Temp\repo_1u3zfsju\repo\SECURITY.md =====
Security Policy
Supported Versions
Use this section to tell people about which versions of your project are
currently being supported with security updates.
| Version | Supported |
| ------- | ------------------ |
| 0.1.x | :white_check_mark: |
Reporting a Vulnerability
If a security concern is found that you would like to disclose you can create a PR for it or if you would like to clear this issue before posting you can email Core Mintplex Labs Team.


===== C:\Users\Breno\AppData\Local\Temp\repo_1u3zfsju\repo\.devcontainer\README.md =====
AnythingLLM Development Container Setup
Welcome to the AnythingLLM development container configuration, designed to create a seamless and feature-rich development environment for this project.
<center><h1><b>PLEASE READ THIS</b></h1></center>
Prerequisites
Docker
Visual Studio Code
Remote - Containers VS Code extension
Features
**Base Image**: Built on `mcr.microsoft.com/devcontainers/javascript-node:1-18-bookworm`, thus Node.JS LTS v18.
**Additional Tools**: Includes `hadolint`, and essential apt-packages such as `curl`, `gnupg`, and more.
**Ports**: Configured to auto-forward ports `3000` (Frontend) and `3001` (Backend).
**Environment Variables**: Sets `NODE_ENV` to `development` and `ESLINT_USE_FLAT_CONFIG` to `true`.
**VS Code Extensions**: A suite of extensions such as `Prettier`, `Docker`, `ESLint`, and more are automatically installed. Please revise if you do not agree with any of these extensions. AI-powered extensions and time trackers are (for now) not included to avoid any privacy concerns, but you can install them later in your own environment.
Getting Started
1. Using GitHub Codespaces. Just select to create a new workspace, and the devcontainer will be created for you.
2. Using your Local VSCode (Release or Insiders). We suggest you first make a fork of the repo and then clone it to your local machine using VSCode tools. Then open the project folder in VSCode, which will prompt you to open the project in a devcontainer. Select yes, and the devcontainer will be created for you. If this does not happen, you can open the command palette and select "Remote-Containers: Reopen in Container".
On Creation:
When the container is built for the first time, it will automatically run `yarn setup` to ensure everything is in place for the Collector, Server and Frontend. This command is expected to be automatically re-run if there is a content change on next reboot.
Work in the Container:
Once the container is up, be patient. Some extensions may complain because dependencies are still being installed, and in the Extensions tab, some may ask you to "Reload" the project. Don't do that yet. First, wait until all settle down for the first time. We suggest you create a new VSCode profile for this devcontainer, so any configuration and extensions you change, won't affect your default profile.
Checklist:
[ ] The usual message asking you to start the Server and Frontend in different windows are now "hidden" in the building process of the devcontainer. Don't forget to do as suggested.
[ ] Open a JavaScript file, for example "server/index.js" and check if `eslint` is working. It will complain that `'err' is defined but never used.`. This means it is working.
[ ] Open a React File, for example, "frontend/src/main.jsx," and check if `eslint` complains about `Fast refresh only works when a file has exports. Move your component(s) to a separate file.`. Again, it means `eslint` is working. Now check at the status bar if the `Prettier` has a double checkmark :heavy_check_mark: (double). It means Prettier is working. You will see a nice extension `Formatting:`:heavy_check_mark: that can be used to disable the `Format on Save` feature temporarily.
[ ] Check if, on the left pane, you have the NPM Scripts (this may be disabled; look at the "Explorer" tree-dots up-right). There will be scripts inside the `package.json` files. You will basically need to run the `dev:collector`, `dev:server` and the `dev:frontend` in this order. When the frontend finishes starting, a window browser will open **inside** the VSCode. Still, you can open it outside.
:warning: **Important for all developers** :warning:
[ ] When you are using the `NODE_ENV=development` the server will not store the configurations you set for security reasons. Please set the proper config on file `.env.development`. The side-effect if you don't, everytime you restart the server, you will be sent to the "Onboarding" page again.
Note when using GitHub Codespaces**
[ ] When running the "Server" for the first time, it will automatically configure its port to be publicly accessible by default, as this is required for the front end to reach the server backend. To know more, read the content of the `.env` file on the frontend folder about this, and if any issues occur, make sure to manually set the port "Visibility" of the "Server" is set to "Public" if needed. Again, this is only needed for developing on GitHub Codespaces.
For the Collector:**
[x] In the past, the Collector dwelled within the Python domain, but now it has journeyed to the splendid realm of Node.JS. Consequently, the configuration complexities of bygone versions are no longer a concern.
Now it is ready to start
In the status bar you will see three shortcuts names `Collector`, `Server` and `Frontend`. Just click-and-wait on that order (don't forget to set the Server port 3001 to Public if you are using GH Codespaces **_before_** starting the Frontend).
Now you can enjoy your time developing instead of reconfiguring everything.
Debugging with the devcontainers
For debugging the collector, server and frontend
First, make sure the built-in extension (ms-vscode.js-debug) is active (I don't know why it would not be, but just in case). If you want, you can install the nightly version (ms-vscode.js-debug-nightly)
Then, in the "Run and Debug" tab (Ctrl+shift+D), you can select on the menu:
Collector debug. This will start the collector in debug mode and attach the debugger. Works very well.
Server debug. This will start the server in debug mode and attach the debugger. Works very well.
Frontend debug. This will start the frontend in debug mode and attach the debugger. I am still struggling with this one. I don't know if VSCode can handle the .jsx files seamlessly as the pure .js on the server. Maybe there is a need for a particular configuration for Vite or React. Anyway, it starts. Another two configurations launch Chrome and Edge, and I think we could add breakpoints on .jsx files somehow. The best scenario would be always to use the embedded browser. WIP.
Please leave comments on the Issues tab or the []("


===== C:\Users\Breno\AppData\Local\Temp\repo_1u3zfsju\repo\cloud-deployments\aws\cloudformation\aws_https_instructions.md =====
How to Configure HTTPS for Anything LLM AWS private deployment
Instructions for manual configuration after generating and running the aws cloudformation template (aws_build_from_source_no_credentials.json). Tested on following browsers: Firefox version 119, Chrome version 118, Edge 118.
Requirements**
Successful deployment of Amazon Linux 2023 EC2 instance with Docker container running Anything LLM
Admin priv to configure Elastic IP for EC2 instance via AWS Management Console UI
Admin priv to configure DNS services (i.e. AWS Route 53) via AWS Management Console UI
Admin priv to configure EC2 Security Group rules via AWS Management Console UI
Step 1: Allocate and assign Elastic IP Address to your deployed EC2 instance
1. Follow AWS instructions on allocating EIP here: 
2. Follow AWS instructions on assigning EIP to EC2 instance here: 
Step 2: Configure DNS A record to resolve to the previously assigned EC2 instance via EIP 
These instructions assume that you already have a top-level domain configured and are using a subdomain 
to access AnythingLLM.
1. Follow AWS instructions on routing traffic to EC2 instance here: 
Step 3: Install and enable nginx
These instructions are for CLI configuration and assume you are logged in to EC2 instance as the ec2-user.
1. $sudo yum install nginx -y
2. $sudo systemctl enable nginx && sudo systemctl start nginx
Step 4: Install certbot
These instructions are for CLI configuration and assume you are logged in to EC2 instance as the ec2-user.
1. $sudo yum install -y augeas-libs
2. $sudo python3 -m venv /opt/certbot/
3. $sudo /opt/certbot/bin/pip install --upgrade pip
4. $sudo /opt/certbot/bin/pip install certbot certbot-nginx
5. $sudo ln -s /opt/certbot/bin/certbot /usr/bin/certbot
Step 5: Configure temporary Inbound Traffic Rule for Security Group to certbot DNS verification
1. Follow AWS instructions on creating inbound rule (http port 80 0.0.0.0/0) for EC2 security group here: 
Step 6: Comment out default http NGINX proxy configuration
These instructions are for CLI configuration and assume you are logged in to EC2 instance as the ec2-user.
1. $sudo vi /etc/nginx/nginx.conf
2. In the nginx.conf file, comment out the default server block configuration for 80. It should look something like the following:
3. Enter ':wq' to save the changes to the nginx default config
Step 7: Create simple http proxy configuration for AnythingLLM 
These instructions are for CLI configuration and assume you are logged in to EC2 instance as the ec2-user.
1. $sudo vi /etc/nginx/conf.d/anything.conf
2. Add the following configuration ensuring that you add your FQDN:.
3. Enter ':wq' to save the changes to the anything config file
Step 8: Test nginx http proxy config and restart nginx service
These instructions are for CLI configuration and assume you are logged in to EC2 instance as the ec2-user.
1. $sudo nginx -t
2. $sudo systemctl restart nginx
3. Navigate to in a browser and you should be proxied to the AnythingLLM web UI.
Step 9: Generate/install cert
These instructions are for CLI configuration and assume you are logged in to EC2 instance as the ec2-user.
1. $sudo certbot --nginx -d [Insert FQDN here] 
 Example command: $sudo certbot --nginx -d anythingllm.exampleorganization.org
 This command will generate the appropriate certificate files, write the files to /etc/letsencrypt/live/yourFQDN, and make updates to the nginx
 configuration file for anythingllm located at /etc/nginx/conf.d/anything.llm
3. Enter the email address you would like to use for updates.
4. Accept the terms of service.
5. Accept or decline to receive communication from LetsEncrypt.
Step 10: Test Cert installation
1. $sudo cat /etc/nginx/conf.d/anything.conf
Your should see a completely updated configuration that includes and a redirect configuration for 
2. Navigate to in a browser and you should be proxied to the AnythingLLM web UI.
Step 11: (Optional) Remove temporary Inbound Traffic Rule for Security Group to certbot DNS verification
1. Follow AWS instructions on deleting inbound rule (http port 80 0.0.0.0/0) for EC2 security group here:


===== C:\Users\Breno\AppData\Local\Temp\repo_1u3zfsju\repo\cloud-deployments\aws\cloudformation\DEPLOY.md =====
How to deploy a private AnythingLLM instance on AWS
With an AWS account you can easily deploy a private AnythingLLM instance on AWS. This will create a url that you can access from any browser over HTTP (HTTPS not supported). This single instance will run on your own keys and they will not be exposed - however if you want your instance to be protected it is highly recommend that you set a password once setup is complete.
Quick Launch (EASY)**
1. Log in to your AWS account
2. Open CloudFormation
3. Ensure you are deploying in a geographic zone that is nearest to your physical location to reduce latency.
4. Click `Create Stack`
5. Use the file `cloudformation_create_anythingllm.json` as your JSON template.
6. Click Deploy. 
7. Wait for stack events to finish and be marked as `Completed`
8. View `Outputs` tab.
9. Wait for all resources to be built. Now wait until instance is available on `[InstanceIP]:3001`.
This process may take up to 10 minutes. See **Note** below on how to visualize this process.
The output of this cloudformation stack will be:
1 EC2 Instance
1 Security Group with 0.0.0.0/0 access on port 3001
1 EC2 Instance Volume `gb2` of 10Gib minimum - customizable pre-deploy.
Requirements**
An AWS account with billing information.
Please read this notice before submitting issues about your deployment
Note:** 
Your instance will not be available instantly. Depending on the instance size you launched with it can take 5-10 minutes to fully boot up.
If you want to check the instance's progress, navigate to your deployed EC2 instances and connect to your instance via SSH in browser.
Once connected run `sudo tail -f /var/log/cloud-init-output.log` and wait for the file to conclude deployment of the docker image.
You should see an output like this
Additionally, your use of this deployment process means you are responsible for any costs of these AWS resources fully.


===== C:\Users\Breno\AppData\Local\Temp\repo_1u3zfsju\repo\cloud-deployments\digitalocean\terraform\DEPLOY.md =====
How to deploy a private AnythingLLM instance on DigitalOcean using Terraform
With a DigitalOcean account, you can easily deploy a private AnythingLLM instance using Terraform. This will create a URL that you can access from any browser over HTTP (HTTPS not supported). This single instance will run on your own keys, and they will not be exposed. However, if you want your instance to be protected, it is highly recommended that you set a password once setup is complete.
The output of this Terraform configuration will be:
1 DigitalOcean Droplet
An IP address to access your application
Requirements**
An DigitalOcean account with billing information
Terraform installed on your local machine
 - Follow the instructions in the official Terraform documentation for your operating system.
How to deploy on DigitalOcean
Open your terminal and navigate to the `docker` folder
1. Create a `.env` file by cloning the `.env.example`. 
2. Navigate to `digitalocean/terraform` folder.
3. Replace the token value in the provider "digitalocean" block in main.tf with your DigitalOcean API token.
4. Run the following commands to initialize Terraform, review the infrastructure changes, and apply them:
Confirm the changes by typing yes when prompted.
5. Once the deployment is complete, Terraform will output the public IP address of your droplet. You can access your application using this IP address.
How to deploy on DigitalOcean
To delete the resources created by Terraform, run the following command in the terminal:
`
terraform destroy 
`
Please read this notice before submitting issues about your deployment
Note:** 
Your instance will not be available instantly. Depending on the instance size you launched with it can take anywhere from 5-10 minutes to fully boot up.
If you want to check the instances progress, navigate to your deployed instances and connect to your instance via SSH in browser.
Once connected run `sudo tail -f /var/log/cloud-init-output.log` and wait for the file to conclude deployment of the docker image.
Additionally, your use of this deployment process means you are responsible for any costs of these Digital Ocean resources fully.


===== C:\Users\Breno\AppData\Local\Temp\repo_1u3zfsju\repo\cloud-deployments\gcp\deployment\DEPLOY.md =====
How to deploy a private AnythingLLM instance on GCP
With a GCP account you can easily deploy a private AnythingLLM instance on GCP. This will create a url that you can access from any browser over HTTP (HTTPS not supported). This single instance will run on your own keys and they will not be exposed - however if you want your instance to be protected it is highly recommend that you set a password once setup is complete.
The output of this cloudformation stack will be:
1 GCP VM
1 Security Group with 0.0.0.0/0 access on Ports 22 & 3001
1 GCP VM Volume `gb2` of 10Gib minimum
Requirements**
An GCP account with billing information.
How to deploy on GCP
Open your terminal
1. Log in to your GCP account using the following command:
2. After successful login, Run the following command to create a deployment using the Deployment Manager CLI:
Once you execute these steps, the CLI will initiate the deployment process on GCP based on your configuration file. You can monitor the deployment status and view the outputs using the Google Cloud Console or the Deployment Manager CLI commands.
ssh into the instance
Delete the deployment
Please read this notice before submitting issues about your deployment
Note:** 
Your instance will not be available instantly. Depending on the instance size you launched with it can take anywhere from 5-10 minutes to fully boot up.
If you want to check the instances progress, navigate to your deployed instances and connect to your instance via SSH in browser.
Once connected run `sudo tail -f /var/log/cloud-init-output.log` and wait for the file to conclude deployment of the docker image.
Additionally, your use of this deployment process means you are responsible for any costs of these GCP resources fully.


===== C:\Users\Breno\AppData\Local\Temp\repo_1u3zfsju\repo\cloud-deployments\helm\charts\anythingllm\README.md =====
anythingllm
AnythingLLM
The all-in-one Desktop & Docker AI application with built-in RAG, AI agents, No-code agent builder, MCP compatibility, and more.
Configuration & Usage**
**Config vs Secrets:** This chart exposes application configuration via two mechanisms:
 - `config` (in `values.yaml`) ‚Äî rendered into a `ConfigMap` and injected using `envFrom` in the pod. Do NOT place sensitive values (API keys, secrets) in `config` because `ConfigMap`s are not encrypted.
 - `env` / `envFrom` ‚Äî the preferred way to inject secrets. Use Kubernetes `Secret` objects and reference them from `env` (with `valueFrom.secretKeyRef`) or `envFrom.secretRef`.
**Storage & STORAGE_DIR mapping:** The chart creates (or mounts) a `PersistentVolumeClaim` using the `persistentVolume.*` settings. The container mount path is set from `persistentVolume.mountPath`. Ensure the container `STORAGE_DIR` config key matches that path (defaults are set in `values.yaml`).
Providing API keys & secrets (recommended)**
Use Kubernetes Secrets. Below are example workflows and `values.yaml` snippets.
1) Create a Kubernetes Secret with API keys:
2) Reference the Secret from `values.yaml` using `envFrom` (recommended when your secret contains multiple env keys):
This will inject all key/value pairs from the `openai-secret` Secret as environment variables in the container.
3) Or reference a single secret key via `env` (explicit mapping):
Notes:
Avoid placing secret values into `config:` (the chart's `ConfigMap`) ‚Äî `ConfigMap`s are visible to anyone who can read the namespace. Use `Secret` objects for any credentials/tokens.
If you use a GitOps workflow, consider integrating an external secret operator (ExternalSecrets, SealedSecrets, etc.) so you don't store raw secrets in Git.
Example `values-secret.yaml` to pass during `helm install`**
Install with:
Best practices & tips**
Use `envFrom` for convenience when many environment variables are stored in a single `Secret` and use `env`/`valueFrom` for explicit single-key mappings.
Use `kubectl create secret generic` or your secrets management solution. If you need to reference multiple different provider keys (OpenAI, Anthropic, etc.), create a single `Secret` with multiple keys or multiple Secrets and add multiple `envFrom` entries.
Keep probe paths and `service.port` aligned. If your probes fail after deployment, check that the probe `port` matches the container port (or named port ` and that the `path` is valid.
For storage, if you have a pre-existing PVC set `persistentVolume.existingClaim` to the PVC name; the chart will mount that claim (and will not attempt to create a new PVC).
For production, provide resource `requests` and `limits` in `values.yaml` to prevent scheduler starvation and to control cost.
Values
| Key | Type | Default | Description |
|-----|------|---------|-------------|
| affinity | object | `{}` | |
| config.DISABLE_TELEMETRY | string | `"true"` | |
| config.GID | string | `"1000"` | |
| config.NODE_ENV | string | `"production"` | |
| config.STORAGE_DIR | string | `"/storage"` | |
| config.UID | string | `"1000"` | |
| env | object | `{}` | |
| envFrom | object | `{}` | |
| fullnameOverride | string | `""` | |
| image.pullPolicy | string | `"IfNotPresent"` | |
| image.repository | string | `"mintplexlabs/anythingllm"` | |
| image.tag | string | `"1.9.0"` | |
| imagePullSecrets | list | `[]` | |
| ingress.annotations | object | `{}` | |
| ingress.className | string | `""` | |
| ingress.enabled | bool | `false` | |
| ingress.hosts[0].host | string | `"chart-example.local"` | |
| ingress.hosts[0].paths[0].path | string | `"/"` | |
| ingress.hosts[0].paths[0].pathType | string | `"ImplementationSpecific"` | |
| ingress.tls | list | `[]` | |
| initContainers | list | `[]` | |
| livenessProbe.failureThreshold | int | `3` | |
| livenessProbe. | string | `"/v1/api/health"` | |
| livenessProbe. | int | `8888` | |
| livenessProbe.initialDelaySeconds | int | `15` | |
| livenessProbe.periodSeconds | int | `5` | |
| nameOverride | string | `""` | |
| nodeSelector | object | `{}` | |
| persistentVolume.accessModes[0] | string | `"ReadWriteOnce"` | |
| persistentVolume.annotations | object | `{}` | |
| persistentVolume.existingClaim | string | `""` | |
| persistentVolume.labels | object | `{}` | |
| persistentVolume.mountPath | string | `"/storage"` | |
| persistentVolume.size | string | `"8Gi"` | |
| podAnnotations | object | `{}` | |
| podLabels | object | `{}` | |
| podSecurityContext.fsGroup | int | `1000` | |
| readinessProbe. | string | `"/v1/api/health"` | |
| readinessProbe. | int | `8888` | |
| readinessProbe.initialDelaySeconds | int | `15` | |
| readinessProbe.periodSeconds | int | `5` | |
| readinessProbe.successThreshold | int | `2` | |
| replicaCount | int | `1` | |
| resources | object | `{}` | |
| securityContext | object | `{}` | |
| service.port | int | `3001` | |
| service.type | string | `"ClusterIP"` | |
| serviceAccount.annotations | object | `{}` | |
| serviceAccount.automount | bool | `true` | |
| serviceAccount.create | bool | `true` | |
| serviceAccount.name | string | `""` | |
| tolerations | list | `[]` | |
| volumeMounts | list | `[]` | |
| volumes | list | `[]` | |


===== C:\Users\Breno\AppData\Local\Temp\repo_1u3zfsju\repo\collector\hotdir\__HOTDIR__.md =====
What is the "Hot directory"
This is a pre-set file location that documents will be written to when uploaded by AnythingLLM. There is really no need to touch it.


===== C:\Users\Breno\AppData\Local\Temp\repo_1u3zfsju\repo\docker\HOW_TO_USE_DOCKER.md =====
How to use Dockerized Anything LLM
Use the Dockerized version of AnythingLLM for a much faster and complete startup of AnythingLLM.
Minimum Requirements
[!TIP]
Running AnythingLLM on AWS/GCP/Azure?
You should aim for at least 2GB of RAM. Disk storage is proportional to however much data
you will be storing (documents, vectors, models, etc). Minimum 10GB recommended.
`docker` installed on your machine
`yarn` and `node` on your machine
access to an LLM running locally or remotely
\*AnythingLLM by default uses a built-in vector database powered by LanceDB
\*AnythingLLM by default embeds text on instance privately Learn More
Recommend way to run dockerized AnythingLLM!
[!IMPORTANT]
If you are running another service on localhost like Chroma, LocalAi, or LMStudio
you will need to use to access the service from within
the docker container using AnythingLLM as `localhost:xxxx` will not resolve for the host system.
> **Requires** Docker v18.03+ on Win/Mac and 20.10+ on Linux/Ubuntu for host.docker.internal to resolve!
> _Linux_: add `--add-host=host.docker.internal:host-gateway` to docker run command for this to resolve.
> eg: Chroma host URL running on localhost:8000 on host machine needs to be 
when used in AnythingLLM.
[!TIP]
It is best to mount the containers storage volume to a folder on your host machine
so that you can pull in future updates without deleting your existing data!
Pull in the latest image from docker. Supports both `amd64` and `arm64` CPU architectures.
<table>
<tr>
<th colspan="2">Mount the storage locally and run AnythingLLM in Docker</th>
</tr>
<tr>
<td>
 Linux/MacOs
</td>
<td>
</td>
</tr>
<tr>
<td>
 Windows
</td>
<td>
</td>
</tr>
<tr>
<td> Docker Compose</td>
<td>
 </td>
</tr>
</table>
Go to ` and you are now using AnythingLLM! All your data and progress will persist between
container rebuilds or pulls from Docker Hub.
How to use the user interface
To access the full application, visit ` in your browser.
About UID and GID in the ENV
The UID and GID are set to 1000 by default. This is the default user in the Docker container and on most host operating systems. If there is a mismatch between your host user UID and GID and what is set in the `.env` file, you may experience permission issues.
Build locally from source _not recommended for casual use_
`git clone` this repo and `cd anything-llm` to get to the root directory.
`touch server/storage/anythingllm.db` to create empty SQLite DB file.
`cd docker/`
`cp .env.example .env` **you must do this before building**
`docker-compose up -d --build` to build the image - this will take a few moments.
Your docker host will show the image as online once the build process is completed. This will build the app to `
Integrations and one-click setups
The integrations below are templates or tooling built by the community to make running the docker experience of AnythingLLM easier.
Use the Midori AI Subsystem to Manage AnythingLLM
Follow the setup found on Midori AI Subsystem Site for your host OS
After setting that up install the AnythingLLM docker backend to the Midori AI Subsystem.
Once that is done, you are all set!
Common questions and fixes
Cannot connect to service running on localhost!
If you are in docker and cannot connect to a service running on your host machine running on a local interface or loopback:
`localhost`
`127.0.0.1`
`0.0.0.0`
[!IMPORTANT]
On linux ` does not work.
Use ` instead to emulate this functionality.
Then in docker you need to replace that localhost part with `host.docker.internal`. For example, if running Ollama on the host machine, bound to you should put ` into the connection URL in AnythingLLM.
API is not working, cannot login, LLM is "offline"?
You are likely running the docker container on a remote machine like EC2 or some other instance where the reachable URL
is not ` and instead is something like ` - in this case all you need to do is add the following to your `frontend/.env.production` before running `docker-compose up -d --build`
For example, if the docker instance is available on `192.186.1.222` your `VITE_API_BASE` would look like `VITE_API_BASE=" in `frontend/.env.production`.
Having issues with Ollama?
If you are getting errors like `llama:streaming - could not stream chat. Error: connect ECONNREFUSED 172.17.0.1:11434` then visit the README below.
Fix common issues with Ollama
Still not working?
Ask for help on Discord


===== C:\Users\Breno\AppData\Local\Temp\repo_1u3zfsju\repo\locales\README.fa-IR.md =====
<a name="readme-top"></a>
<p align="center">
 <a href=" src=" alt="AnythingLLM logo"></a>
</p>
<div align='center'>
<a href=" target="_blank"><img src=" alt="Mintplex-Labs%2Fanything-llm | Trendshift" style="width: 250px; height: 55px;" width="250" height="55"/></a>
</div>
<p align="center" dir="rtl">
 <b>AnythingLLM:</b> ÿßŸæŸÑ€å⁄©€åÿ¥ŸÜ ŸáŸÖŸá‚Äå⁄©ÿßÿ±Ÿá ŸáŸàÿ¥ ŸÖÿµŸÜŸàÿπ€å ⁄©Ÿá ÿØŸÜÿ®ÿßŸÑÿ¥ ÿ®ŸàÿØ€åÿØ.<br />
 ÿ®ÿß ÿßÿ≥ŸÜÿßÿØ ÿÆŸàÿØ ⁄Üÿ™ ⁄©ŸÜ€åÿØÿå ÿßÿ≤ ÿπÿßŸÖŸÑ‚ÄåŸáÿß€å ŸáŸàÿ¥ ŸÖÿµŸÜŸàÿπ€å ÿßÿ≥ÿ™ŸÅÿßÿØŸá ⁄©ŸÜ€åÿØÿå ÿ®ÿß ŸÇÿßÿ®ŸÑ€åÿ™ Ÿæ€å⁄©ÿ±ÿ®ŸÜÿØ€å ÿ®ÿßŸÑÿßÿå ⁄ÜŸÜÿØ ⁄©ÿßÿ±ÿ®ÿ±Ÿáÿå Ÿà ÿ®ÿØŸàŸÜ ŸÜ€åÿßÿ≤ ÿ®Ÿá ÿ™ŸÜÿ∏€åŸÖÿßÿ™ Ÿæ€å⁄Ü€åÿØŸá.
</p>
<p align="center">
 <a href=" target="_blank">
 <img src=" alt="Discord">
 </a> |
 <a href=" target="_blank">
 <img src=" alt="License">
 </a> |
 <a href=" target="_blank">
 Docs
 </a> |
 <a href=" target="_blank">
 Hosted Instance
 </a>
</p>
<p align="center" dir="rtl">
 <b>English</b> ¬∑ <a href='./locales/README.zh-CN.md'>ÁÆÄ‰Ωì‰∏≠Êñá</a> ¬∑ <a href='./locales/README.ja-JP.md'>Êó•Êú¨Ë™û</a> ¬∑ <b>ŸÅÿßÿ±ÿ≥€å</b>
</p>
<p align="center" dir="rtl">
üëà AnythingLLM ÿ®ÿ±ÿß€å ÿØÿ≥⁄©ÿ™ÿßŸæ (ŸÖ⁄©ÿå Ÿà€åŸÜÿØŸàÿ≤ Ÿà ŸÑ€åŸÜŸà⁄©ÿ≥)! <a href=" target="_blank">ÿØÿßŸÜŸÑŸàÿØ ⁄©ŸÜ€åÿØ</a>
</p>
<div dir="rtl">
€å⁄© ÿßŸæŸÑ€å⁄©€åÿ¥ŸÜ ⁄©ÿßŸÖŸÑ ⁄©Ÿá ÿ®Ÿá ÿ¥ŸÖÿß ÿßŸÖ⁄©ÿßŸÜ ŸÖ€å‚ÄåÿØŸáÿØ Ÿáÿ± ÿ≥ŸÜÿØÿå ŸÖŸÜÿ®ÿπ €åÿß ŸÖÿ≠ÿ™Ÿàÿß€å€å ÿ±ÿß ÿ®Ÿá ÿ≤ŸÖ€åŸÜŸá‚Äåÿß€å ÿ™ÿ®ÿØ€åŸÑ ⁄©ŸÜ€åÿØ ⁄©Ÿá Ÿáÿ± LLM ŸÖ€å‚Äåÿ™ŸàÿßŸÜÿØ ÿØÿ± ÿ≠€åŸÜ ⁄ØŸÅÿ™⁄ØŸà ÿ®Ÿá ÿπŸÜŸàÿßŸÜ ŸÖÿ±ÿ¨ÿπ ÿßÿ≤ ÿßŸìŸÜ ÿßÿ≥ÿ™ŸÅÿßÿØŸá ⁄©ŸÜÿØ. ÿß€åŸÜ ÿ®ÿ±ŸÜÿßŸÖŸá ÿ®Ÿá ÿ¥ŸÖÿß ÿßÿ¨ÿßÿ≤Ÿá ŸÖ€å‚ÄåÿØŸáÿØ LLM €åÿß Ÿæÿß€å⁄ØÿßŸá ÿØÿßÿØŸá ÿ®ÿ±ÿØÿßÿ±€å ŸÖŸàÿ±ÿØ ŸÜÿ∏ÿ± ÿÆŸàÿØ ÿ±ÿß ÿßŸÜÿ™ÿÆÿßÿ® ⁄©ŸÜ€åÿØ Ÿà ŸáŸÖ⁄ÜŸÜ€åŸÜ ÿßÿ≤ ŸÖÿØ€åÿ±€åÿ™ ⁄ÜŸÜÿØ ⁄©ÿßÿ±ÿ®ÿ±Ÿá Ÿà ŸÖÿ¨Ÿàÿ≤Ÿáÿß Ÿæÿ¥ÿ™€åÿ®ÿßŸÜ€å ŸÖ€å‚Äå⁄©ŸÜÿØ.
</div>
<details>
<summary><kbd>ÿØŸÖŸà€å Ÿà€åÿØ€åŸà€å€å ÿ±ÿß ÿ™ŸÖÿßÿ¥ÿß ⁄©ŸÜ€åÿØ!</kbd></summary>
[](
</details>
<div dir="rtl">
ŸÜŸÖÿß€å ⁄©ŸÑ€å ŸÖÿ≠ÿµŸàŸÑ
AnythingLLM €å⁄© ÿßŸæŸÑ€å⁄©€åÿ¥ŸÜ ⁄©ÿßŸÖŸÑ ÿßÿ≥ÿ™ ⁄©Ÿá ÿØÿ± ÿßŸìŸÜ ŸÖ€å‚Äåÿ™ŸàÿßŸÜ€åÿØ ÿßÿ≤ LLM‚ÄåŸáÿß€å ÿ™ÿ¨ÿßÿ±€å ÿßŸìŸÖÿßÿØŸá €åÿß LLM‚ÄåŸáÿß€å ŸÖÿ™ŸÜ‚Äåÿ®ÿßÿ≤ ŸÖÿ≠ÿ®Ÿàÿ® Ÿà ÿ±ÿßŸá‚Äåÿ≠ŸÑ‚ÄåŸáÿß€å vectorDB ÿ®ÿ±ÿß€å ÿ≥ÿßÿÆÿ™ €å⁄© ChatGPT ÿÆÿµŸàÿµ€å ÿ®ÿØŸàŸÜ ŸÖÿ≠ÿØŸàÿØ€åÿ™ ÿßÿ≥ÿ™ŸÅÿßÿØŸá ⁄©ŸÜ€åÿØ ⁄©Ÿá ŸÖ€å‚Äåÿ™ŸàÿßŸÜ€åÿØ ÿßŸìŸÜ ÿ±ÿß ÿ®Ÿá ÿµŸàÿ±ÿ™ ŸÖÿ≠ŸÑ€å ÿßÿ¨ÿ±ÿß ⁄©ŸÜ€åÿØ €åÿß ÿßÿ≤ ÿ±ÿßŸá ÿØŸàÿ± ŸÖ€åÿ≤ÿ®ÿßŸÜ€å ⁄©ŸÜ€åÿØ Ÿà ÿ®ÿß Ÿáÿ± ÿ≥ŸÜÿØ€å ⁄©Ÿá ÿ®Ÿá ÿßŸìŸÜ ÿßÿ±ÿßŸäŸîŸá ŸÖ€å‚ÄåÿØŸá€åÿØÿå ŸáŸàÿ¥ŸÖŸÜÿØÿßŸÜŸá ⁄ØŸÅÿ™⁄ØŸà ⁄©ŸÜ€åÿØ.
AnythingLLM ÿßÿ≥ŸÜÿßÿØ ÿ¥ŸÖÿß ÿ±ÿß ÿ®Ÿá ÿßÿ¥€åÿß€å€å ÿ®Ÿá ŸÜÿßŸÖ `workspaces` ÿ™ŸÇÿ≥€åŸÖ ŸÖ€å‚Äå⁄©ŸÜÿØ. €å⁄© Workspace ŸÖÿßŸÜŸÜÿØ €å⁄© ÿ±ÿ¥ÿ™Ÿá ÿπŸÖŸÑ ŸÖ€å‚Äå⁄©ŸÜÿØÿå ÿßŸÖÿß ÿ®ÿß ÿßÿ∂ÿßŸÅŸá ÿ¥ÿØŸÜ ⁄©ÿßŸÜÿ™€åŸÜÿ±ÿ≥ÿßÿ≤€å ÿßÿ≥ŸÜÿßÿØ ÿ¥ŸÖÿß. WorkspaceŸáÿß ŸÖ€å‚Äåÿ™ŸàÿßŸÜŸÜÿØ ÿßÿ≥ŸÜÿßÿØ ÿ±ÿß ÿ®Ÿá ÿßÿ¥ÿ™ÿ±ÿß⁄© ÿ®⁄Øÿ∞ÿßÿ±ŸÜÿØÿå ÿßŸÖÿß ÿ®ÿß €å⁄©ÿØ€å⁄Øÿ± ÿßÿ±ÿ™ÿ®ÿßÿ∑ ÿ®ÿ±ŸÇÿ±ÿßÿ± ŸÜŸÖ€å‚Äå⁄©ŸÜŸÜÿØ ÿ™ÿß ÿ®ÿ™ŸàÿßŸÜ€åÿØ ÿ≤ŸÖ€åŸÜŸá Ÿáÿ± workspace ÿ±ÿß ÿ™ŸÖ€åÿ≤ ŸÜ⁄ØŸá ÿØÿßÿ±€åÿØ.
</div>
<div dir="rtl">
Ÿà€å⁄ò⁄Ø€å‚ÄåŸáÿß€å ÿ¨ÿ∞ÿßÿ® AnythingLLM
üÜï **ÿπÿßŸÖŸÑ‚ÄåŸáÿß€å ŸáŸàÿ¥ ŸÖÿµŸÜŸàÿπ€å ÿ≥ŸÅÿßÿ±ÿ¥€å**
üñºÔ∏è **Ÿæÿ¥ÿ™€åÿ®ÿßŸÜ€å ÿßÿ≤ ⁄ÜŸÜÿØ ŸÖÿØŸÑ (ŸáŸÖ LLMŸáÿß€å ŸÖÿ™ŸÜ‚Äåÿ®ÿßÿ≤ Ÿà ŸáŸÖ ÿ™ÿ¨ÿßÿ±€å!)**
üë§ Ÿæÿ¥ÿ™€åÿ®ÿßŸÜ€å ÿßÿ≤ ⁄ÜŸÜÿØ ⁄©ÿßÿ±ÿ®ÿ± Ÿà ÿ≥€åÿ≥ÿ™ŸÖ ŸÖÿ¨Ÿàÿ≤Ÿáÿß _ŸÅŸÇÿ∑ ÿØÿ± ŸÜÿ≥ÿÆŸá Docker_
ü¶æ ÿπÿßŸÖŸÑ‚ÄåŸáÿß ÿØÿ± ŸÅÿ∂ÿß€å ⁄©ÿßÿ±€å ÿ¥ŸÖÿß (ŸÖÿ±Ÿàÿ± Ÿàÿ®ÿå ÿßÿ¨ÿ±ÿß€å ⁄©ÿØ Ÿà ÿ∫€åÿ±Ÿá)
üí¨ Ÿà€åÿ¨ÿ™ ⁄Üÿ™ ŸÇÿßÿ®ŸÑ ÿ¨ÿßÿ≥ÿßÿ≤€å ÿ≥ŸÅÿßÿ±ÿ¥€å ÿ®ÿ±ÿß€å Ÿàÿ®‚Äåÿ≥ÿß€åÿ™ ÿ¥ŸÖÿß _ŸÅŸÇÿ∑ ÿØÿ± ŸÜÿ≥ÿÆŸá Docker_
üìñ Ÿæÿ¥ÿ™€åÿ®ÿßŸÜ€å ÿßÿ≤ ÿßŸÜŸàÿßÿπ ŸÖÿÆÿ™ŸÑŸÅ ÿ≥ŸÜÿØ (PDFÿå TXTÿå DOCX Ÿà ÿ∫€åÿ±Ÿá)
ÿ±ÿßÿ®ÿ∑ ⁄©ÿßÿ±ÿ®ÿ±€å ÿ≥ÿßÿØŸá ⁄Üÿ™ ÿ®ÿß ŸÇÿßÿ®ŸÑ€åÿ™ ⁄©ÿ¥€åÿØŸÜ Ÿà ÿ±Ÿáÿß ⁄©ÿ±ÿØŸÜ Ÿà ÿßÿ≥ÿ™ŸÜÿßÿØŸáÿß€å Ÿàÿßÿ∂ÿ≠
€±€∞€∞Ÿ™ ÿßŸìŸÖÿßÿØŸá ÿßÿ≥ÿ™ŸÇÿ±ÿßÿ± ÿØÿ± ŸÅÿ∂ÿß€å ÿßÿ®ÿ±€å
ÿ≥ÿßÿ≤⁄Øÿßÿ± ÿ®ÿß ÿ™ŸÖÿßŸÖ ÿßÿ±ÿßŸäŸîŸá‚ÄåÿØŸáŸÜÿØ⁄ØÿßŸÜ ŸÖÿ≠ÿ®Ÿàÿ® LLM ŸÖÿ™ŸÜ‚Äåÿ®ÿßÿ≤ Ÿà ÿ™ÿ¨ÿßÿ±€å
ÿØÿßÿ±ÿß€å ÿßŸÇÿØÿßŸÖÿßÿ™ ÿØÿßÿÆŸÑ€å ÿµÿ±ŸÅŸá‚Äåÿ¨Ÿà€å€å ÿØÿ± Ÿáÿ≤€åŸÜŸá Ÿà ÿ≤ŸÖÿßŸÜ ÿ®ÿ±ÿß€å ŸÖÿØ€åÿ±€åÿ™ ÿßÿ≥ŸÜÿßÿØ ÿ®ÿ≥€åÿßÿ± ÿ®ÿ≤ÿ±⁄Ø ÿØÿ± ŸÖŸÇÿß€åÿ≥Ÿá ÿ®ÿß ÿ≥ÿß€åÿ± ÿ±ÿßÿ®ÿ∑‚ÄåŸáÿß€å ⁄©ÿßÿ±ÿ®ÿ±€å ⁄Üÿ™
API ⁄©ÿßŸÖŸÑ ÿ™Ÿàÿ≥ÿπŸá‚ÄåÿØŸáŸÜÿØŸá ÿ®ÿ±ÿß€å €å⁄©Ÿæÿßÿ±⁄ÜŸá‚Äåÿ≥ÿßÿ≤€å‚ÄåŸáÿß€å ÿ≥ŸÅÿßÿ±ÿ¥€å!
Ÿà ŸÖŸàÿßÿ±ÿØ ÿ®€åÿ¥ÿ™ÿ±... ŸÜÿµÿ® ⁄©ŸÜ€åÿØ Ÿà ⁄©ÿ¥ŸÅ ⁄©ŸÜ€åÿØ!
LLMŸáÿßÿå ŸÖÿØŸÑ‚ÄåŸáÿß€å Embedderÿå ŸÖÿØŸÑ‚ÄåŸáÿß€å ⁄ØŸÅÿ™ÿßÿ±€å Ÿà Ÿæÿß€å⁄ØÿßŸá‚ÄåŸáÿß€å ÿØÿßÿØŸá ÿ®ÿ±ÿØÿßÿ±€å Ÿæÿ¥ÿ™€åÿ®ÿßŸÜ€å ÿ¥ÿØŸá
ŸÖÿØŸÑ‚ÄåŸáÿß€å ÿ≤ÿ®ÿßŸÜ€å ÿ®ÿ≤ÿ±⁄Ø (LLMs):**
Any open-source llama.cpp compatible model
OpenAI
OpenAI (Generic)
Azure OpenAI
AWS Bedrock
Anthropic
NVIDIA NIM (chat models)
Google Gemini Pro
Hugging Face (chat models)
Ollama (chat models)
LM Studio (all models)
LocalAi (all models)
Together AI (chat models)
Fireworks AI (chat models)
Perplexity (chat models)
OpenRouter (chat models)
DeepSeek (chat models)
Mistral
Groq
Cohere
KoboldCPP
LiteLLM
Text Generation Web UI
Apipie
xAI
Novita AI (chat models)
PPIO
<div dir="rtl">
ŸÖÿØŸÑ‚ÄåŸáÿß€å Embedder:**
AnythingLLM Native Embedder (Ÿæ€åÿ¥‚ÄåŸÅÿ±ÿ∂)
OpenAI
Azure OpenAI
LocalAi (ŸáŸÖŸá)
Ollama (ŸáŸÖŸá)
LM Studio (ŸáŸÖŸá)
Cohere
ŸÖÿØŸÑ‚ÄåŸáÿß€å ÿ±ŸàŸÜŸà€åÿ≥€å ÿµŸàÿ™€å:**
AnythingLLM Built-in (Ÿæ€åÿ¥‚ÄåŸÅÿ±ÿ∂)
OpenAI
Ÿæÿ¥ÿ™€åÿ®ÿßŸÜ€å TTS (ÿ™ÿ®ÿØ€åŸÑ ŸÖÿ™ŸÜ ÿ®Ÿá ⁄ØŸÅÿ™ÿßÿ±):**
ÿßŸÖ⁄©ÿßŸÜÿßÿ™ ÿØÿßÿÆŸÑ€å ŸÖÿ±Ÿàÿ±⁄Øÿ± (Ÿæ€åÿ¥‚ÄåŸÅÿ±ÿ∂)
PiperTTSLocal - ÿßÿ¨ÿ±ÿß ÿØÿ± ŸÖÿ±Ÿàÿ±⁄Øÿ±
OpenAI TTS
ElevenLabs
Ÿáÿ± ÿ≥ÿ±Ÿà€åÿ≥ TTS ÿ≥ÿßÿ≤⁄Øÿßÿ± ÿ®ÿß OpenAI
Ÿæÿ¥ÿ™€åÿ®ÿßŸÜ€å STT (ÿ™ÿ®ÿØ€åŸÑ ⁄ØŸÅÿ™ÿßÿ± ÿ®Ÿá ŸÖÿ™ŸÜ):**
ÿßŸÖ⁄©ÿßŸÜÿßÿ™ ÿØÿßÿÆŸÑ€å ŸÖÿ±Ÿàÿ±⁄Øÿ± (Ÿæ€åÿ¥‚ÄåŸÅÿ±ÿ∂)
Ÿæÿß€å⁄ØÿßŸá‚ÄåŸáÿß€å ÿØÿßÿØŸá ÿ®ÿ±ÿØÿßÿ±€å:**
LanceDB (Ÿæ€åÿ¥‚ÄåŸÅÿ±ÿ∂)
PGVector
Astra DB
Pinecone
Chroma
Weaviate
Qdrant
Milvus
Zilliz
ŸÜŸÖÿß€å ⁄©ŸÑ€å ŸÅŸÜ€å
ÿß€åŸÜ ŸÖÿÆÿ≤ŸÜ ÿ¥ÿßŸÖŸÑ ÿ≥Ÿá ÿ®ÿÆÿ¥ ÿßÿµŸÑ€å ÿßÿ≥ÿ™:
`frontend`: €å⁄© ÿ±ÿßÿ®ÿ∑ ⁄©ÿßÿ±ÿ®ÿ±€å viteJS + React ⁄©Ÿá ŸÖ€å‚Äåÿ™ŸàÿßŸÜ€åÿØ ÿ®ÿ±ÿß€å ÿß€åÿ¨ÿßÿØ Ÿà ŸÖÿØ€åÿ±€åÿ™ ÿßŸìÿ≥ÿßŸÜ ÿ™ŸÖÿßŸÖ ŸÖÿ≠ÿ™Ÿàÿß€å ŸÇÿßÿ®ŸÑ ÿßÿ≥ÿ™ŸÅÿßÿØŸá ÿ™Ÿàÿ≥ÿ∑ LLM ÿßÿ¨ÿ±ÿß ⁄©ŸÜ€åÿØ.
`server`: €å⁄© ÿ≥ÿ±Ÿàÿ± NodeJS express ÿ®ÿ±ÿß€å ŸÖÿØ€åÿ±€åÿ™ ÿ™ŸÖÿßŸÖ ÿ™ÿπÿßŸÖŸÑÿßÿ™ Ÿà ÿßŸÜÿ¨ÿßŸÖ ŸÖÿØ€åÿ±€åÿ™ vectorDB Ÿà ÿ™ÿπÿßŸÖŸÑÿßÿ™ LLM.
`collector`: ÿ≥ÿ±Ÿàÿ± NodeJS express ⁄©Ÿá ÿßÿ≥ŸÜÿßÿØ ÿ±ÿß ÿßÿ≤ ÿ±ÿßÿ®ÿ∑ ⁄©ÿßÿ±ÿ®ÿ±€å Ÿæÿ±ÿØÿßÿ≤ÿ¥ Ÿà ÿ™ÿ¨ÿ≤€åŸá ŸÖ€å‚Äå⁄©ŸÜÿØ.
`docker`: ÿØÿ≥ÿ™Ÿàÿ±ÿßŸÑÿπŸÖŸÑ‚ÄåŸáÿß€å Docker Ÿà ŸÅÿ±ÿßŸì€åŸÜÿØ ÿ≥ÿßÿÆÿ™ + ÿßÿ∑ŸÑÿßÿπÿßÿ™ ÿ®ÿ±ÿß€å ÿ≥ÿßÿÆÿ™ ÿßÿ≤ ŸÖŸÜÿ®ÿπ.
`embed`: ÿ≤€åÿ±ŸÖÿß⁄òŸàŸÑ ÿ®ÿ±ÿß€å ÿ™ŸàŸÑ€åÿØ Ÿà ÿß€åÿ¨ÿßÿØ Ÿà€åÿ¨ÿ™ ŸÇÿßÿ®ŸÑ ÿ¨ÿßÿ≥ÿßÿ≤€å Ÿàÿ®.
`browser-extension`: ÿ≤€åÿ±ŸÖÿß⁄òŸàŸÑ ÿ®ÿ±ÿß€å ÿßŸÅÿ≤ŸàŸÜŸá ŸÖÿ±Ÿàÿ±⁄Øÿ± ⁄©ÿ±ŸàŸÖ.
</div>
üõ≥ ŸÖ€åÿ≤ÿ®ÿßŸÜ€å ÿ¥ÿÆÿµ€å
<div dir="rtl">
Mintplex Labs Ÿà ÿ¨ÿßŸÖÿπŸá ⁄©ÿßÿ±ÿ®ÿ±ÿßŸÜÿå ÿ±Ÿàÿ¥‚ÄåŸáÿßÿå ÿßÿ≥⁄©ÿ±€åŸæÿ™‚ÄåŸáÿß Ÿà ŸÇÿßŸÑÿ®‚ÄåŸáÿß€å ŸÖÿ™ÿπÿØÿØ€å ÿ±ÿß ÿ®ÿ±ÿß€å ÿßÿ¨ÿ±ÿß€å AnythingLLM ÿ®Ÿá ÿµŸàÿ±ÿ™ ŸÖÿ≠ŸÑ€å ŸÜ⁄ØŸáÿØÿßÿ±€å ŸÖ€å‚Äå⁄©ŸÜŸÜÿØ. ÿ®ÿ±ÿß€å ŸÖÿ∑ÿßŸÑÿπŸá ŸÜÿ≠ŸàŸá ÿßÿ≥ÿ™ŸÇÿ±ÿßÿ± ÿØÿ± ŸÖÿ≠€åÿ∑ ŸÖŸàÿ±ÿØ ŸÜÿ∏ÿ± ÿÆŸàÿØ €åÿß ÿßÿ≥ÿ™ŸÇÿ±ÿßÿ± ÿÆŸàÿØ⁄©ÿßÿ±ÿå ÿ®Ÿá ÿ¨ÿØŸàŸÑ ÿ≤€åÿ± ŸÖÿ±ÿßÿ¨ÿπŸá ⁄©ŸÜ€åÿØ.
</div>
| Docker | AWS | GCP | Digital Ocean | Render.com |
|----------------------------------------|----|-----|---------------|------------|
| [![Deploy on Docker][docker-btn]][docker-deploy] | [![Deploy on AWS][aws-btn]][aws-deploy] | [![Deploy on GCP][gcp-btn]][gcp-deploy] | [![Deploy on DigitalOcean][do-btn]][do-deploy] | [![Deploy on Render.com][render-btn]][render-deploy] |
| Railway | RepoCloud | Elestio |
| --- | --- | --- |
| [![Deploy on Railway][railway-btn]][railway-deploy] | [![Deploy on RepoCloud][repocloud-btn]][repocloud-deploy] | [![Deploy on Elestio][elestio-btn]][elestio-deploy] |
<div dir="rtl">
€åÿß ÿ±ÿßŸá‚ÄåÿßŸÜÿØÿßÿ≤€å ŸÜŸÖŸàŸÜŸá ÿ™ŸàŸÑ€åÿØ€å AnythingLLM ÿ®ÿØŸàŸÜ Docker ‚Üí
ÿ±ÿßŸá‚ÄåÿßŸÜÿØÿßÿ≤€å ÿ®ÿ±ÿß€å ÿ™Ÿàÿ≥ÿπŸá
`yarn setup` ÿ®ÿ±ÿß€å Ÿæÿ± ⁄©ÿ±ÿØŸÜ ŸÅÿß€åŸÑ‚ÄåŸáÿß€å `.env` ŸÖŸàÿ±ÿØ ŸÜ€åÿßÿ≤ ÿØÿ± Ÿáÿ± ÿ®ÿÆÿ¥ ÿßÿ≤ ÿ®ÿ±ŸÜÿßŸÖŸá (ÿßÿ≤ ÿ±€åÿ¥Ÿá ŸÖÿÆÿ≤ŸÜ).
 - ŸÇÿ®ŸÑ ÿßÿ≤ ÿßÿØÿßŸÖŸáÿå ÿßŸìŸÜ‚ÄåŸáÿß ÿ±ÿß Ÿæÿ± ⁄©ŸÜ€åÿØ. ÿßÿ∑ŸÖ€åŸÜÿßŸÜ ÿ≠ÿßÿµŸÑ ⁄©ŸÜ€åÿØ ⁄©Ÿá `server/.env.development` Ÿæÿ± ÿ¥ÿØŸá ÿßÿ≥ÿ™ÿå ÿØÿ± ÿ∫€åÿ± ÿß€åŸÜ ÿµŸàÿ±ÿ™ ŸáŸÖŸá ⁄Ü€åÿ≤ ÿØÿ±ÿ≥ÿ™ ⁄©ÿßÿ± ŸÜÿÆŸàÿßŸáÿØ ⁄©ÿ±ÿØ.
`yarn dev:server` ÿ®ÿ±ÿß€å ÿ±ÿßŸá‚ÄåÿßŸÜÿØÿßÿ≤€å ÿ≥ÿ±Ÿàÿ± ÿ®Ÿá ÿµŸàÿ±ÿ™ ŸÖÿ≠ŸÑ€å (ÿßÿ≤ ÿ±€åÿ¥Ÿá ŸÖÿÆÿ≤ŸÜ).
`yarn dev:frontend` ÿ®ÿ±ÿß€å ÿ±ÿßŸá‚ÄåÿßŸÜÿØÿßÿ≤€å ŸÅÿ±ÿßŸÜÿ™‚ÄåÿßŸÜÿØ ÿ®Ÿá ÿµŸàÿ±ÿ™ ŸÖÿ≠ŸÑ€å (ÿßÿ≤ ÿ±€åÿ¥Ÿá ŸÖÿÆÿ≤ŸÜ).
`yarn dev:collector` ÿ®ÿ±ÿß€å ÿßÿ¨ÿ±ÿß€å ÿ¨ŸÖÿπ‚Äå⁄©ŸÜŸÜÿØŸá ÿßÿ≥ŸÜÿßÿØ (ÿßÿ≤ ÿ±€åÿ¥Ÿá ŸÖÿÆÿ≤ŸÜ).
ÿØÿ±ÿ®ÿßÿ±Ÿá ÿßÿ≥ŸÜÿßÿØ ÿ®€åÿ¥ÿ™ÿ± ÿ®ÿØÿßŸÜ€åÿØ
ÿØÿ±ÿ®ÿßÿ±Ÿá ⁄©ÿ¥‚Äå⁄©ÿ±ÿØŸÜ ÿ®ÿ±ÿØÿßÿ± ÿ®€åÿ¥ÿ™ÿ± ÿ®ÿØÿßŸÜ€åÿØ
ÿ™ŸÑŸá‚ÄåŸÖÿ™ÿ±€å Ÿà ÿ≠ÿ±€åŸÖ ÿÆÿµŸàÿµ€å
AnythingLLM ÿ™Ÿàÿ≥ÿ∑ Mintplex Labs Inc ÿØÿßÿ±ÿß€å Ÿà€å⁄ò⁄Ø€å ÿ™ŸÑŸá‚ÄåŸÖÿ™ÿ±€å ÿßÿ≥ÿ™ ⁄©Ÿá ÿßÿ∑ŸÑÿßÿπÿßÿ™ ÿßÿ≥ÿ™ŸÅÿßÿØŸá ŸÜÿßÿ¥ŸÜÿßÿ≥ ÿ±ÿß ÿ¨ŸÖÿπ‚ÄåÿßŸìŸàÿ±€å ŸÖ€å‚Äå⁄©ŸÜÿØ.
<details>
<summary><kbd>ÿßÿ∑ŸÑÿßÿπÿßÿ™ ÿ®€åÿ¥ÿ™ÿ± ÿØÿ±ÿ®ÿßÿ±Ÿá ÿ™ŸÑŸá‚ÄåŸÖÿ™ÿ±€å Ÿà ÿ≠ÿ±€åŸÖ ÿÆÿµŸàÿµ€å AnythingLLM</kbd></summary>
⁄Üÿ±ÿßÿü
<div dir="rtl">
ŸÖÿß ÿßÿ≤ ÿß€åŸÜ ÿßÿ∑ŸÑÿßÿπÿßÿ™ ÿ®ÿ±ÿß€å ÿØÿ±⁄© ŸÜÿ≠ŸàŸá ÿßÿ≥ÿ™ŸÅÿßÿØŸá ÿßÿ≤ AnythingLLMÿå ÿßŸàŸÑŸà€åÿ™‚Äåÿ®ŸÜÿØ€å ⁄©ÿßÿ± ÿ±Ÿà€å Ÿà€å⁄ò⁄Ø€å‚ÄåŸáÿß€å ÿ¨ÿØ€åÿØ Ÿà ÿ±ŸÅÿπ ÿßÿ¥⁄©ÿßŸÑÿßÿ™ÿå Ÿà ÿ®Ÿáÿ®ŸàÿØ ÿπŸÖŸÑ⁄©ÿ±ÿØ Ÿà Ÿæÿß€åÿØÿßÿ±€å AnythingLLM ÿßÿ≥ÿ™ŸÅÿßÿØŸá ŸÖ€å‚Äå⁄©ŸÜ€åŸÖ.
</div>
ÿ∫€åÿ±ŸÅÿπÿßŸÑ ⁄©ÿ±ÿØŸÜ
<div dir="rtl">
ÿ®ÿ±ÿß€å ÿ∫€åÿ±ŸÅÿπÿßŸÑ ⁄©ÿ±ÿØŸÜ ÿ™ŸÑŸá‚ÄåŸÖÿ™ÿ±€åÿå `DISABLE_TELEMETRY` ÿ±ÿß ÿØÿ± ÿ™ŸÜÿ∏€åŸÖÿßÿ™ .env ÿ≥ÿ±Ÿàÿ± €åÿß ÿØÿß⁄©ÿ± ÿÆŸàÿØ ÿ±Ÿà€å "true" ÿ™ŸÜÿ∏€åŸÖ ⁄©ŸÜ€åÿØ. ŸáŸÖ⁄ÜŸÜ€åŸÜ ŸÖ€å‚Äåÿ™ŸàÿßŸÜ€åÿØ ÿß€åŸÜ ⁄©ÿßÿ± ÿ±ÿß ÿØÿ± ÿ®ÿ±ŸÜÿßŸÖŸá ÿ®ÿß ÿ±ŸÅÿ™ŸÜ ÿ®Ÿá ŸÜŸàÿßÿ± ⁄©ŸÜÿßÿ±€å > `ÿ≠ÿ±€åŸÖ ÿÆÿµŸàÿµ€å` Ÿà ÿ∫€åÿ±ŸÅÿπÿßŸÑ ⁄©ÿ±ÿØŸÜ ÿ™ŸÑŸá‚ÄåŸÖÿ™ÿ±€å ÿßŸÜÿ¨ÿßŸÖ ÿØŸá€åÿØ.
</div>
ÿØŸÇ€åŸÇÿßŸã ⁄ÜŸá ⁄Ü€åÿ≤€å ÿ±ÿß ÿ±ÿØ€åÿßÿ®€å ŸÖ€å‚Äå⁄©ŸÜ€åÿØÿü
<div dir="rtl">
ŸÖÿß ŸÅŸÇÿ∑ ÿ¨ÿ≤ŸäŸî€åÿßÿ™ ÿßÿ≥ÿ™ŸÅÿßÿØŸá‚Äåÿß€å ÿ±ÿß ⁄©Ÿá ÿ®Ÿá ŸÖÿß ÿØÿ± ÿ™ÿµŸÖ€åŸÖ‚Äå⁄Ø€åÿ±€å‚ÄåŸáÿß€å ŸÖÿ≠ÿµŸàŸÑ Ÿà ŸÜŸÇÿ¥Ÿá ÿ±ÿßŸá ⁄©ŸÖ⁄© ŸÖ€å‚Äå⁄©ŸÜÿØÿå ÿ±ÿØ€åÿßÿ®€å ŸÖ€å‚Äå⁄©ŸÜ€åŸÖÿå ÿ®Ÿá ÿ∑Ÿàÿ± ÿÆÿßÿµ:
ŸÜŸàÿπ ŸÜÿµÿ® ÿ¥ŸÖÿß (Docker €åÿß Desktop)
ÿ≤ŸÖÿßŸÜ€å ⁄©Ÿá ÿ≥ŸÜÿØ€å ÿßÿ∂ÿßŸÅŸá €åÿß ÿ≠ÿ∞ŸÅ ŸÖ€å‚Äåÿ¥ŸàÿØ. Ÿá€å⁄Ü ÿßÿ∑ŸÑÿßÿπÿßÿ™€å _ÿØÿ±ÿ®ÿßÿ±Ÿá_ ÿ≥ŸÜÿØ ŸÜÿØÿßÿ±€åŸÖ. ŸÅŸÇÿ∑ ÿ±Ÿà€åÿØÿßÿØ ÿ´ÿ®ÿ™ ŸÖ€å‚Äåÿ¥ŸàÿØ.
ŸÜŸàÿπ Ÿæÿß€å⁄ØÿßŸá ÿØÿßÿØŸá ÿ®ÿ±ÿØÿßÿ±€å ÿØÿ± ÿ≠ÿßŸÑ ÿßÿ≥ÿ™ŸÅÿßÿØŸá. ÿ®Ÿá ŸÖÿß ⁄©ŸÖ⁄© ŸÖ€å‚Äå⁄©ŸÜÿØ ÿ®ÿØÿßŸÜ€åŸÖ ⁄©ÿØÿßŸÖ ÿßÿ±ÿßŸäŸîŸá‚ÄåÿØŸáŸÜÿØŸá ÿ®€åÿ¥ÿ™ÿ± ÿßÿ≥ÿ™ŸÅÿßÿØŸá ŸÖ€å‚Äåÿ¥ŸàÿØ.
ŸÜŸàÿπ LLM ÿØÿ± ÿ≠ÿßŸÑ ÿßÿ≥ÿ™ŸÅÿßÿØŸá. ÿ®Ÿá ŸÖÿß ⁄©ŸÖ⁄© ŸÖ€å‚Äå⁄©ŸÜÿØ ŸÖÿ≠ÿ®Ÿàÿ®‚Äåÿ™ÿ±€åŸÜ ÿßŸÜÿ™ÿÆÿßÿ® ÿ±ÿß ÿ®ÿ¥ŸÜÿßÿ≥€åŸÖ.
ÿßÿ±ÿ≥ÿßŸÑ ⁄Üÿ™. ÿß€åŸÜ ŸÖÿπŸÖŸàŸÑ‚Äåÿ™ÿ±€åŸÜ "ÿ±Ÿà€åÿØÿßÿØ" ÿßÿ≥ÿ™ Ÿà ÿ®Ÿá ŸÖÿß ÿß€åÿØŸá‚Äåÿß€å ÿßÿ≤ ŸÅÿπÿßŸÑ€åÿ™ ÿ±Ÿàÿ≤ÿßŸÜŸá ŸÖ€å‚ÄåÿØŸáÿØ.
ŸÖ€å‚Äåÿ™ŸàÿßŸÜ€åÿØ ÿß€åŸÜ ÿßÿØÿπÿßŸáÿß ÿ±ÿß ÿ®ÿß Ÿæ€åÿØÿß ⁄©ÿ±ÿØŸÜ ÿ™ŸÖÿßŸÖ ŸÖ⁄©ÿßŸÜ‚ÄåŸáÿß€å€å ⁄©Ÿá `Telemetry.sendTelemetry` ŸÅÿ±ÿßÿÆŸàÿßŸÜ€å ŸÖ€å‚Äåÿ¥ŸàÿØÿå ÿ™ÿßŸî€å€åÿØ ⁄©ŸÜ€åÿØ. ÿßÿ±ÿßŸäŸîŸá‚ÄåÿØŸáŸÜÿØŸá ÿ™ŸÑŸá‚ÄåŸÖÿ™ÿ±€å PostHog ÿßÿ≥ÿ™.
ŸÖÿ¥ÿßŸáÿØŸá ŸáŸÖŸá ÿ±Ÿà€åÿØÿßÿØŸáÿß€å ÿ™ŸÑŸá‚ÄåŸÖÿ™ÿ±€å ÿØÿ± ⁄©ÿØ ŸÖŸÜÿ®ÿπ
</div>
</details>
üëã ŸÖÿ¥ÿßÿ±⁄©ÿ™
<div dir="rtl">
ÿß€åÿ¨ÿßÿØ issue
ÿß€åÿ¨ÿßÿØ PR ÿ®ÿß ŸÅÿ±ŸÖÿ™ ŸÜÿßŸÖ ÿ¥ÿßÿÆŸá `<ÿ¥ŸÖÿßÿ±Ÿá issue>-<ŸÜÿßŸÖ ⁄©Ÿàÿ™ÿßŸá>`
ÿ™ÿßŸî€å€åÿØ ÿßÿ≤ ÿ™€åŸÖ ÿßÿµŸÑ€å
</div>
üåü ŸÖÿ¥ÿßÿ±⁄©ÿ™‚Äå⁄©ŸÜŸÜÿØ⁄ØÿßŸÜ
[](
[](
üîó ŸÖÿ≠ÿµŸàŸÑÿßÿ™ ÿ®€åÿ¥ÿ™ÿ±
<div dir="rtl">
**[VectorAdmin][vector-admin]:** €å⁄© ÿ±ÿßÿ®ÿ∑ ⁄©ÿßÿ±ÿ®ÿ±€å Ÿà ŸÖÿ¨ŸÖŸàÿπŸá ÿßÿ®ÿ≤ÿßÿ± ŸáŸÖŸá‚Äå⁄©ÿßÿ±Ÿá ÿ®ÿ±ÿß€å ŸÖÿØ€åÿ±€åÿ™ Ÿæÿß€å⁄ØÿßŸá‚ÄåŸáÿß€å ÿØÿßÿØŸá ÿ®ÿ±ÿØÿßÿ±€å.
**[OpenAI Assistant Swarm][assistant-swarm]:** ÿ™ÿ®ÿØ€åŸÑ ⁄©ŸÑ ⁄©ÿ™ÿßÿ®ÿÆÿßŸÜŸá ÿØÿ≥ÿ™€åÿßÿ±ÿßŸÜ OpenAI ÿ®Ÿá €å⁄© ÿßÿ±ÿ™ÿ¥ Ÿàÿßÿ≠ÿØ ÿ™ÿ≠ÿ™ ŸÅÿ±ŸÖÿßŸÜ €å⁄© ÿπÿßŸÖŸÑ.
</div>
<div align="right">
[
</div>
<div dir="ltr" align="left">
Copyright ¬© 2025 [Mintplex Labs][profile-link]. <br />
This project is MIT licensed.
</div>
<!-- LINK GROUP -->
[back-to-top]: 
[profile-link]: 
[vector-admin]: 
[assistant-swarm]: 
[docker-btn]: ./images/deployBtns/docker.png
[docker-deploy]: ./docker/HOW_TO_USE_DOCKER.md
[aws-btn]: ./images/deployBtns/aws.png
[aws-deploy]: ./cloud-deployments/aws/cloudformation/DEPLOY.md
[gcp-btn]: 
[gcp-deploy]: ./cloud-deployments/gcp/deployment/DEPLOY.md
[do-btn]: 
[do-deploy]: ./cloud-deployments/digitalocean/terraform/DEPLOY.md
[render-btn]: 
[render-deploy]: 
[render-btn]: 
[render-deploy]: 
[railway-btn]: 
[railway-deploy]: 
[repocloud-btn]: 
[repocloud-deploy]: 
[elestio-btn]: 
[elestio-deploy]:


===== C:\Users\Breno\AppData\Local\Temp\repo_1u3zfsju\repo\locales\README.ja-JP.md =====
<a name="readme-top"></a>
<p align="center">
 <a href=" src=" alt="AnythingLLM logo"></a>
</p>
<div align='center'>
<a href=" target="_blank"><img src=" alt="Mintplex-Labs%2Fanything-llm | Trendshift" style="width: 250px; height: 55px;" width="250" height="55"/></a>
</div>
<p align="center">
 <b>AnythingLLM:</b> „ÅÇ„Å™„Åü„Åã„ÇôÊé¢„Åó„Å¶„ÅÑ„Åü„Ç™„Éº„É´„Ç§„É≥„ÉØ„É≥AI„Ç¢„Éï„Çö„É™„ÄÇ<br />
 „Éà„Çô„Ç≠„É•„É°„É≥„Éà„Å®„ÉÅ„É£„ÉÉ„Éà„Åó„ÄÅAI„Ç®„Éº„Ç∑„Çô„Çß„É≥„Éà„Çí‰ΩøÁî®„Åó„ÄÅÈ´òÂ∫¶„Å´„Ç´„Çπ„Çø„Éû„Ç§„Çπ„ÇôÂèØËÉΩ„Å¶„Çô„ÄÅË§áÊï∞„É¶„Éº„Çµ„Çô„ÉºÂØæÂøú„ÄÅÈù¢ÂÄí„Å™Ë®≠ÂÆö„ÅØ‰∏çË¶Å„Å¶„Çô„Åô„ÄÇ
</p>
<p align="center">
 <a href=" target="_blank">
 <img src=" alt="Discord">
 </a> |
 <a href=" target="_blank">
 <img src=" alt="„É©„Ç§„Çª„É≥„Çπ">
 </a> |
 <a href=" target="_blank">
 „Éà„Çô„Ç≠„É•„É°„É≥„Éà
 </a> |
 <a href=" target="_blank">
 „Éõ„Çπ„Éà„Åï„Çå„Åü„Ç§„É≥„Çπ„Çø„É≥„Çπ
 </a>
</p>
<p align="center">
 <a href='../README.md'>English</a> ¬∑ <a href='./README.zh-CN.md'>ÁÆÄ‰Ωì‰∏≠Êñá</a> ¬∑ <b>Êó•Êú¨Ë™û</b>
</p>
<p align="center">
üëâ „ÉÜ„Çô„Çπ„ÇØ„Éà„ÉÉ„Éï„ÇöÁî®AnythingLLM(Mac„ÄÅWindows„ÄÅLinuxÂØæÂøú)!<a href=" target="_blank">‰ªä„Åô„Åè„Çô„Çø„Çô„Ç¶„É≥„É≠„Éº„Éà„Çô</a>
</p>
„Åì„Çå„ÅØ„ÄÅ‰ªªÊÑè„ÅÆ„Éà„Çô„Ç≠„É•„É°„É≥„Éà„ÄÅ„É™„ÇΩ„Éº„Çπ„ÄÅ„Åæ„Åü„ÅØ„Ç≥„É≥„ÉÜ„É≥„ÉÑ„ÅÆÊñ≠Áâá„Çí„ÄÅ„ÉÅ„É£„ÉÉ„Éà‰∏≠„Å´LLM„Åã„ÇôÂèÇÁÖß„Å®„Åó„Å¶‰ΩøÁî®„Å¶„Çô„Åç„Çã„Ç≥„É≥„ÉÜ„Ç≠„Çπ„Éà„Å´Â§âÊèõ„Å¶„Çô„Åç„Çã„Éï„É´„Çπ„Çø„ÉÉ„ÇØ„Ç¢„Éï„Çö„É™„Ç±„Éº„Ç∑„Éß„É≥„Å¶„Çô„Åô„ÄÇ„Åì„ÅÆ„Ç¢„Éï„Çö„É™„Ç±„Éº„Ç∑„Éß„É≥„Çí‰ΩøÁî®„Åô„Çã„Å®„ÄÅ‰ΩøÁî®„Åô„ÇãLLM„Åæ„Åü„ÅØ„Éò„Çô„ÇØ„Éà„É´„ÉÜ„Çô„Éº„Çø„Éò„Çô„Éº„Çπ„ÇíÈÅ∏Êäû„Åó„ÄÅ„Éû„É´„ÉÅ„É¶„Éº„Çµ„Çô„ÉºÁÆ°ÁêÜ„Å®Ê®©Èôê„Çí„Çµ„Éõ„Çö„Éº„Éà„Å¶„Çô„Åç„Åæ„Åô„ÄÇ
<details>
<summary><kbd>„ÉÜ„Çô„É¢„ÇíË¶ã„Çã!</kbd></summary>
[](
</details>
Ë£ΩÂìÅÊ¶ÇË¶Å
AnythingLLM„ÅØ„ÄÅÂ∏ÇË≤©„ÅÆLLM„ÇÑ‰∫∫Ê∞ó„ÅÆ„ÅÇ„Çã„Ç™„Éº„Éï„Çö„É≥„ÇΩ„Éº„ÇπLLM„ÄÅ„Åä„Çà„Å≤„Çô„Éò„Çô„ÇØ„Éà„É´DB„ÇΩ„É™„É•„Éº„Ç∑„Éß„É≥„Çí‰ΩøÁî®„Åó„Å¶„ÄÅÂ¶•Âçî„ÅÆ„Å™„ÅÑ„Éï„Çö„É©„Ç§„Éò„Çô„Éº„ÉàChatGPT„ÇíÊßãÁØâ„Å¶„Çô„Åç„Çã„Éï„É´„Çπ„Çø„ÉÉ„ÇØ„Ç¢„Éï„Çö„É™„Ç±„Éº„Ç∑„Éß„É≥„Å¶„Çô„Åô„ÄÇ„É≠„Éº„Ç´„É´„Å¶„ÇôÂÆüË°å„Åô„Çã„Åì„Å®„ÇÇ„ÄÅ„É™„É¢„Éº„Éà„Å¶„Çô„Éõ„Çπ„Éà„Åô„Çã„Åì„Å®„ÇÇ„Å¶„Çô„Åç„ÄÅÊèê‰æõ„Åï„Çå„Åü„Éà„Çô„Ç≠„É•„É°„É≥„Éà„Å®Áü•ÁöÑ„Å´„ÉÅ„É£„ÉÉ„Éà„Å¶„Çô„Åç„Åæ„Åô„ÄÇ
AnythingLLM„ÅØ„ÄÅ„Éà„Çô„Ç≠„É•„É°„É≥„Éà„Çí`„ÉØ„Éº„ÇØ„Çπ„Éò„Çö„Éº„Çπ`„Å®Âëº„ÅØ„Çô„Çå„Çã„Ç™„Éï„Çô„Ç∑„Çô„Çß„ÇØ„Éà„Å´ÂàÜÂâ≤„Åó„Åæ„Åô„ÄÇ„ÉØ„Éº„ÇØ„Çπ„Éò„Çö„Éº„Çπ„ÅØ„Çπ„É¨„ÉÉ„Éà„Çô„ÅÆ„Çà„ÅÜ„Å´Ê©üËÉΩ„Åó„Åæ„Åô„Åã„Çô„ÄÅ„Éà„Çô„Ç≠„É•„É°„É≥„Éà„ÅÆ„Ç≥„É≥„ÉÜ„ÉäÂåñ„Åã„ÇôËøΩÂä†„Åï„Çå„Å¶„ÅÑ„Åæ„Åô„ÄÇ„ÉØ„Éº„ÇØ„Çπ„Éò„Çö„Éº„Çπ„ÅØ„Éà„Çô„Ç≠„É•„É°„É≥„Éà„ÇíÂÖ±Êúâ„Å¶„Çô„Åç„Åæ„Åô„Åã„Çô„ÄÅ‰∫í„ÅÑ„Å´ÈÄö‰ø°„Åô„Çã„Åì„Å®„ÅØ„Å™„ÅÑ„Åü„ÇÅ„ÄÅÂêÑ„ÉØ„Éº„ÇØ„Çπ„Éò„Çö„Éº„Çπ„ÅÆ„Ç≥„É≥„ÉÜ„Ç≠„Çπ„Éà„Çí„ÇØ„É™„Éº„É≥„Å´‰øù„Å§„Åì„Å®„Åã„Çô„Å¶„Çô„Åç„Åæ„Åô„ÄÇ
AnythingLLM„ÅÆ„ÅÑ„Åè„Å§„Åã„ÅÆ„ÇØ„Éº„É´„Å™Ê©üËÉΩ
**„Éû„É´„ÉÅ„É¶„Éº„Çµ„Çô„Éº„Ç§„É≥„Çπ„Çø„É≥„Çπ„ÅÆ„Çµ„Éõ„Çö„Éº„Éà„Å®Ê®©Èôê‰ªò‰∏é**
„ÉØ„Éº„ÇØ„Çπ„Éò„Çö„Éº„ÇπÂÜÖ„ÅÆ„Ç®„Éº„Ç∑„Çô„Çß„É≥„Éà(„Ç¶„Çß„Éï„Çô„ÇíÈñ≤Ë¶ß„ÄÅ„Ç≥„Éº„Éà„Çô„ÇíÂÆüË°å„Å™„Å®„Çô)
„Ç¶„Çß„Éï„Çô„Çµ„Ç§„ÉàÁî®„ÅÆ„Ç´„Çπ„Çø„É†Âüã„ÇÅËæº„ÅøÂèØËÉΩ„Å™„ÉÅ„É£„ÉÉ„Éà„Ç¶„Ç£„Ç∑„Çô„Çß„ÉÉ„Éà
Ë§áÊï∞„ÅÆ„Éà„Çô„Ç≠„É•„É°„É≥„Éà„Çø„Ç§„Éï„Çö„ÅÆ„Çµ„Éõ„Çö„Éº„Éà(PDF„ÄÅTXT„ÄÅDOCX„Å™„Å®„Çô)
„Ç∑„É≥„Éï„Çö„É´„Å™UI„Åã„Çâ„Éò„Çô„ÇØ„Éà„É´„ÉÜ„Çô„Éº„Çø„Éò„Çô„Éº„ÇπÂÜÖ„ÅÆ„Éà„Çô„Ç≠„É•„É°„É≥„Éà„ÇíÁÆ°ÁêÜ
2„Å§„ÅÆ„ÉÅ„É£„ÉÉ„Éà„É¢„Éº„Éà„Çô`‰ºöË©±`„Å®`„ÇØ„Ç®„É™`„ÄÇ‰ºöË©±„ÅØ‰ª•Ââç„ÅÆË≥™Âïè„Å®‰øÆÊ≠£„Çí‰øùÊåÅ„Åó„Åæ„Åô„ÄÇ„ÇØ„Ç®„É™„ÅØ„Éà„Çô„Ç≠„É•„É°„É≥„Éà„Å´ÂØæ„Åô„Çã„Ç∑„É≥„Éï„Çö„É´„Å™QA„Å¶„Çô„Åô
„ÉÅ„É£„ÉÉ„Éà‰∏≠„ÅÆÂºïÁî®
100%„ÇØ„É©„Ç¶„Éà„Çô„ÉÜ„Çô„Éï„Çö„É≠„Ç§„É°„É≥„ÉàÂØæÂøú„ÄÇ
„ÄåÁã¨Ëá™„ÅÆLLM„ÇíÊåÅÂèÇ„Äç„É¢„ÉÜ„Çô„É´„ÄÇ
Â§ßË¶èÊ®°„Å™„Éà„Çô„Ç≠„É•„É°„É≥„Éà„ÇíÁÆ°ÁêÜ„Åô„Çã„Åü„ÇÅ„ÅÆÈùûÂ∏∏„Å´ÂäπÁéáÁöÑ„Å™„Ç≥„Çπ„ÉàÂâäÊ∏õÁ≠ñ„ÄÇÂ∑®Â§ß„Å™„Éà„Çô„Ç≠„É•„É°„É≥„Éà„ÇÑ„Éà„É©„É≥„Çπ„ÇØ„É™„Éï„Çö„Éà„ÇíÂüã„ÇÅËæº„ÇÄ„Åü„ÇÅ„Å´‰∏ÄÂ∫¶‰ª•‰∏äÊîØÊâï„ÅÜ„Åì„Å®„ÅØ„ÅÇ„Çä„Åæ„Åõ„Çì„ÄÇ‰ªñ„ÅÆ„Éà„Çô„Ç≠„É•„É°„É≥„Éà„ÉÅ„É£„ÉÉ„Éà„Éõ„Çô„ÉÉ„Éà„ÇΩ„É™„É•„Éº„Ç∑„Éß„É≥„Çà„Çä„ÇÇ90%„Ç≥„Çπ„ÉàÂäπÁéá„Åã„ÇôËâØ„ÅÑ„Å¶„Çô„Åô„ÄÇ
„Ç´„Çπ„Çø„É†Áµ±Âêà„ÅÆ„Åü„ÇÅ„ÅÆÂÆåÂÖ®„Å™ÈñãÁô∫ËÄÖAPI!
„Çµ„Éõ„Çö„Éº„Éà„Åï„Çå„Å¶„ÅÑ„ÇãLLM„ÄÅÂüã„ÇÅËæº„Åø„É¢„ÉÜ„Çô„É´„ÄÅÈü≥Â£∞„É¢„ÉÜ„Çô„É´„ÄÅ„Åä„Çà„Å≤„Çô„Éò„Çô„ÇØ„Éà„É´„ÉÜ„Çô„Éº„Çø„Éò„Çô„Éº„Çπ
Ë®ÄË™ûÂ≠¶Áøí„É¢„ÉÜ„Çô„É´:**
llama.cpp‰∫íÊèõ„ÅÆ‰ªªÊÑè„ÅÆ„Ç™„Éº„Éï„Çö„É≥„ÇΩ„Éº„Çπ„É¢„ÉÜ„Çô„É´
OpenAI
OpenAI (Ê±éÁî®)
Azure OpenAI
Anthropic
Google Gemini Pro
Hugging Face („ÉÅ„É£„ÉÉ„Éà„É¢„ÉÜ„Çô„É´)
Ollama („ÉÅ„É£„ÉÉ„Éà„É¢„ÉÜ„Çô„É´)
LM Studio („Åô„Å∏„Çô„Å¶„ÅÆ„É¢„ÉÜ„Çô„É´)
LocalAi („Åô„Å∏„Çô„Å¶„ÅÆ„É¢„ÉÜ„Çô„É´)
Together AI („ÉÅ„É£„ÉÉ„Éà„É¢„ÉÜ„Çô„É´)
Fireworks AI („ÉÅ„É£„ÉÉ„Éà„É¢„ÉÜ„Çô„É´)
Perplexity („ÉÅ„É£„ÉÉ„Éà„É¢„ÉÜ„Çô„É´)
OpenRouter („ÉÅ„É£„ÉÉ„Éà„É¢„ÉÜ„Çô„É´)
Novita AI („ÉÅ„É£„ÉÉ„Éà„É¢„ÉÜ„Çô„É´)
Mistral
Groq
Cohere
KoboldCPP
PPIO
CometAPI („ÉÅ„É£„ÉÉ„Éà„É¢„ÉÜ„Çô„É´)
Âüã„ÇÅËæº„Åø„É¢„ÉÜ„Çô„É´:**
AnythingLLM„Éç„Ç§„ÉÜ„Ç£„Éï„ÇôÂüã„ÇÅËæº„Åø(„ÉÜ„Çô„Éï„Ç©„É´„Éà)
OpenAI
Azure OpenAI
LocalAi („Åô„Å∏„Çô„Å¶)
Ollama („Åô„Å∏„Çô„Å¶)
LM Studio („Åô„Å∏„Çô„Å¶)
Cohere
Èü≥Â£∞Â§âÊèõ„É¢„ÉÜ„Çô„É´:**
AnythingLLMÂÜÖËîµ(„ÉÜ„Çô„Éï„Ç©„É´„Éà)
OpenAI
TTS(„ÉÜ„Ç≠„Çπ„Éà„Åã„ÇâÈü≥Â£∞„Å∏)„Çµ„Éõ„Çö„Éº„Éà:**
„Éç„Ç§„ÉÜ„Ç£„Éï„Çô„Éï„Çô„É©„Ç¶„Çµ„ÇôÂÜÖËîµ(„ÉÜ„Çô„Éï„Ç©„É´„Éà)
OpenAI TTS
ElevenLabs
STT(Èü≥Â£∞„Åã„Çâ„ÉÜ„Ç≠„Çπ„Éà„Å∏)„Çµ„Éõ„Çö„Éº„Éà:**
„Éç„Ç§„ÉÜ„Ç£„Éï„Çô„Éï„Çô„É©„Ç¶„Çµ„ÇôÂÜÖËîµ(„ÉÜ„Çô„Éï„Ç©„É´„Éà)
„Éò„Çô„ÇØ„Éà„É´„ÉÜ„Çô„Éº„Çø„Éò„Çô„Éº„Çπ:**
LanceDB(„ÉÜ„Çô„Éï„Ç©„É´„Éà)
PGVector
Astra DB
Pinecone
Chroma
Weaviate
QDrant
Milvus
Zilliz
ÊäÄË°ìÊ¶ÇË¶Å
„Åì„ÅÆ„É¢„Éé„É¨„Éõ„Çö„ÅØ„ÄÅ‰∏ª„Å´3„Å§„ÅÆ„Çª„ÇØ„Ç∑„Éß„É≥„Å¶„ÇôÊßãÊàê„Åï„Çå„Å¶„ÅÑ„Åæ„Åô:
`frontend`: LLM„Åã„Çô‰ΩøÁî®„Å¶„Çô„Åç„Çã„Åô„Å∏„Çô„Å¶„ÅÆ„Ç≥„É≥„ÉÜ„É≥„ÉÑ„ÇíÁ∞°Âçò„Å´‰ΩúÊàê„Åä„Çà„Å≤„ÇôÁÆ°ÁêÜ„Å¶„Çô„Åç„ÇãviteJS + React„Éï„É≠„É≥„Éà„Ç®„É≥„Éà„Çô„ÄÇ
`server`: „Åô„Å∏„Çô„Å¶„ÅÆ„Ç§„É≥„Çø„É©„ÇØ„Ç∑„Éß„É≥„ÇíÂá¶ÁêÜ„Åó„ÄÅ„Åô„Å∏„Çô„Å¶„ÅÆ„Éò„Çô„ÇØ„Éà„É´DBÁÆ°ÁêÜ„Åä„Çà„Å≤„ÇôLLM„Ç§„É≥„Çø„É©„ÇØ„Ç∑„Éß„É≥„ÇíË°å„ÅÜNodeJS express„Çµ„Éº„Éè„Çô„Éº„ÄÇ
`collector`: UI„Åã„Çâ„Éà„Çô„Ç≠„É•„É°„É≥„Éà„ÇíÂá¶ÁêÜ„Åä„Çà„Å≤„ÇôËß£Êûê„Åô„ÇãNodeJS express„Çµ„Éº„Éè„Çô„Éº„ÄÇ
`docker`: Docker„ÅÆÊåáÁ§∫„Åä„Çà„Å≤„Çô„Éí„Çô„É´„Éà„Çô„Éï„Çö„É≠„Çª„Çπ + „ÇΩ„Éº„Çπ„Åã„Çâ„ÅÆ„Éí„Çô„É´„Éà„ÇôÊÉÖÂ†±„ÄÇ
`embed`: Âüã„ÇÅËæº„Åø„Ç¶„Ç£„Ç∑„Çô„Çß„ÉÉ„Éà„ÅÆÁîüÊàê„Å´ÁâπÂåñ„Åó„Åü„Ç≥„Éº„Éà„Çô„ÄÇ
üõ≥ „Çª„É´„Éï„Éõ„Çπ„ÉÜ„Ç£„É≥„ÇØ„Çô
Mintplex Labs„Åä„Çà„Å≤„Çô„Ç≥„Éü„É•„Éã„ÉÜ„Ç£„ÅØ„ÄÅAnythingLLM„Çí„É≠„Éº„Ç´„É´„Å¶„ÇôÂÆüË°å„Å¶„Çô„Åç„ÇãÂ§öÊï∞„ÅÆ„ÉÜ„Çô„Éï„Çö„É≠„Ç§„É°„É≥„ÉàÊñπÊ≥ï„ÄÅ„Çπ„ÇØ„É™„Éï„Çö„Éà„ÄÅ„ÉÜ„É≥„Éï„Çö„É¨„Éº„Éà„ÇíÁ∂≠ÊåÅ„Åó„Å¶„ÅÑ„Åæ„Åô„ÄÇ‰ª•‰∏ã„ÅÆË°®„ÇíÂèÇÁÖß„Åó„Å¶„ÄÅ„ÅäÂ•Ω„Åø„ÅÆÁí∞Â¢É„Å¶„Çô„ÅÆ„ÉÜ„Çô„Éï„Çö„É≠„Ç§ÊñπÊ≥ï„ÇíË™≠„ÇÄ„Åã„ÄÅËá™Âãï„ÉÜ„Çô„Éï„Çö„É≠„Ç§„ÇíË°å„Å£„Å¶„Åè„Åü„Çô„Åï„ÅÑ„ÄÇ
| Docker | AWS | GCP | Digital Ocean | Render.com |
|----------------------------------------|----|-----|---------------|------------|
| [![Docker‰∏ä„Å¶„Çô„ÉÜ„Çô„Éï„Çö„É≠„Ç§][docker-btn]][docker-deploy] | [![AWS‰∏ä„Å¶„Çô„ÉÜ„Çô„Éï„Çö„É≠„Ç§][aws-btn]][aws-deploy] | [![GCP‰∏ä„Å¶„Çô„ÉÜ„Çô„Éï„Çö„É≠„Ç§][gcp-btn]][gcp-deploy] | [![DigitalOcean‰∏ä„Å¶„Çô„ÉÜ„Çô„Éï„Çö„É≠„Ç§][do-btn]][do-deploy] | [![Render.com‰∏ä„Å¶„Çô„ÉÜ„Çô„Éï„Çö„É≠„Ç§][render-btn]][render-deploy] |
| Railway |
| --------------------------------------------------- |
| [![Railway‰∏ä„Å¶„Çô„ÉÜ„Çô„Éï„Çö„É≠„Ç§][railway-btn]][railway-deploy] |
Docker„Çí‰ΩøÁî®„Åõ„Åô„Çô„Å´Êú¨Áï™Áí∞Â¢É„ÅÆAnythingLLM„Ç§„É≥„Çπ„Çø„É≥„Çπ„ÇíË®≠ÂÆö„Åô„Çã ‚Üí
ÈñãÁô∫Áí∞Â¢É„ÅÆ„Çª„ÉÉ„Éà„Ç¢„ÉÉ„Éï„ÇöÊñπÊ≥ï
`yarn setup` ÂêÑ„Ç¢„Éï„Çö„É™„Ç±„Éº„Ç∑„Éß„É≥„Çª„ÇØ„Ç∑„Éß„É≥„Å´ÂøÖË¶Å„Å™`.env`„Éï„Ç°„Ç§„É´„ÇíÂÖ•Âäõ„Åó„Åæ„Åô(„É™„Éõ„Çö„Ç∑„Çô„Éà„É™„ÅÆ„É´„Éº„Éà„Åã„Çâ)„ÄÇ
 - Ê¨°„Å´ÈÄ≤„ÇÄÂâç„Å´„Åì„Çå„Çâ„ÇíÂÖ•Âäõ„Åó„Å¶„Åè„Åü„Çô„Åï„ÅÑ„ÄÇ`server/.env.development`„Åã„ÇôÂÖ•Âäõ„Åï„Çå„Å¶„ÅÑ„Å™„ÅÑ„Å®Ê≠£„Åó„ÅèÂãï‰Ωú„Åó„Åæ„Åõ„Çì„ÄÇ
`yarn dev:server` „É≠„Éº„Ç´„É´„Å¶„Çô„Çµ„Éº„Éè„Çô„Éº„ÇíËµ∑Âãï„Åó„Åæ„Åô(„É™„Éõ„Çö„Ç∑„Çô„Éà„É™„ÅÆ„É´„Éº„Éà„Åã„Çâ)„ÄÇ
`yarn dev:frontend` „É≠„Éº„Ç´„É´„Å¶„Çô„Éï„É≠„É≥„Éà„Ç®„É≥„Éà„Çô„ÇíËµ∑Âãï„Åó„Åæ„Åô(„É™„Éõ„Çö„Ç∑„Çô„Éà„É™„ÅÆ„É´„Éº„Éà„Åã„Çâ)„ÄÇ
`yarn dev:collector` „Éà„Çô„Ç≠„É•„É°„É≥„Éà„Ç≥„É¨„ÇØ„Çø„Éº„ÇíÂÆüË°å„Åó„Åæ„Åô(„É™„Éõ„Çö„Ç∑„Çô„Éà„É™„ÅÆ„É´„Éº„Éà„Åã„Çâ)„ÄÇ
„Éà„Çô„Ç≠„É•„É°„É≥„Éà„Å´„Å§„ÅÑ„Å¶Â≠¶„Åµ„Çô
„Éò„Çô„ÇØ„Éà„É´„Ç≠„É£„ÉÉ„Ç∑„É•„Å´„Å§„ÅÑ„Å¶Â≠¶„Åµ„Çô
Ë≤¢ÁåÆ„Åô„ÇãÊñπÊ≥ï
issue„Çí‰ΩúÊàê„Åô„Çã
`<issue number>-<short name>`„ÅÆÂΩ¢Âºè„ÅÆ„Éï„Çô„É©„É≥„ÉÅÂêç„Å¶„ÇôPR„Çí‰ΩúÊàê„Åô„Çã
„Éû„Éº„Ç∑„Çô„Åó„Åæ„Åó„Çá„ÅÜ
„ÉÜ„É¨„É°„Éà„É™„Éº„Å®„Éï„Çö„É©„Ç§„Éè„Çô„Ç∑„Éº
Mintplex Labs Inc.„Å´„Çà„Å£„Å¶ÈñãÁô∫„Åï„Çå„ÅüAnythingLLM„Å´„ÅØ„ÄÅÂåøÂêç„ÅÆ‰ΩøÁî®ÊÉÖÂ†±„ÇíÂèéÈõÜ„Åô„Çã„ÉÜ„É¨„É°„Éà„É™„ÉºÊ©üËÉΩ„Åã„ÇôÂê´„Åæ„Çå„Å¶„ÅÑ„Åæ„Åô„ÄÇ
<details>
<summary><kbd>AnythingLLM„ÅÆ„ÉÜ„É¨„É°„Éà„É™„Éº„Å®„Éï„Çö„É©„Ç§„Éè„Çô„Ç∑„Éº„Å´„Å§„ÅÑ„Å¶„ÅÆË©≥Á¥∞</kbd></summary>
„Å™„Åõ„Çô?
„Åì„ÅÆÊÉÖÂ†±„Çí‰ΩøÁî®„Åó„Å¶„ÄÅAnythingLLM„ÅÆ‰ΩøÁî®ÊñπÊ≥ï„ÇíÁêÜËß£„Åó„ÄÅÊñ∞Ê©üËÉΩ„Å®„Éè„Çô„ÇØ„Çô‰øÆÊ≠£„ÅÆÂÑ™ÂÖàÈ†Ü‰Ωç„ÇíÊ±∫ÂÆö„Åó„ÄÅAnythingLLM„ÅÆ„Éè„Çö„Éï„Ç©„Éº„Éû„É≥„Çπ„Å®ÂÆâÂÆöÊÄß„ÇíÂêë‰∏ä„Åï„Åõ„Çã„ÅÆ„Å´ÂΩπÁ´ã„Å¶„Åæ„Åô„ÄÇ
„Ç™„Éï„Çö„Éà„Ç¢„Ç¶„Éà
„Çµ„Éº„Éè„Çô„Éº„Åæ„Åü„ÅØdocker„ÅÆ.envË®≠ÂÆö„Å¶„Çô`DISABLE_TELEMETRY`„Çí„Äåtrue„Äç„Å´Ë®≠ÂÆö„Åó„Å¶„ÄÅ„ÉÜ„É¨„É°„Éà„É™„Éº„Åã„Çâ„Ç™„Éï„Çö„Éà„Ç¢„Ç¶„Éà„Åó„Åæ„Åô„ÄÇ„Ç¢„Éï„Çö„É™ÂÜÖ„Å¶„Çô„ÇÇ„ÄÅ„Çµ„Ç§„Éà„Çô„Éè„Çô„Éº > `„Éï„Çö„É©„Ç§„Éè„Çô„Ç∑„Éº`„Å´ÁßªÂãï„Åó„Å¶„ÉÜ„É¨„É°„Éà„É™„Éº„ÇíÁÑ°Âäπ„Å´„Åô„Çã„Åì„Å®„Åã„Çô„Å¶„Çô„Åç„Åæ„Åô„ÄÇ
ÊòéÁ§∫ÁöÑ„Å´ËøΩË∑°„Åô„Çã„ÇÇ„ÅÆ
Ë£ΩÂìÅ„Åä„Çà„Å≤„Çô„É≠„Éº„Éà„Çô„Éû„ÉÉ„Éï„Çö„ÅÆÊÑèÊÄùÊ±∫ÂÆö„Å´ÂΩπÁ´ã„Å§‰ΩøÁî®Ë©≥Á¥∞„ÅÆ„Åø„ÇíËøΩË∑°„Åó„Åæ„Åô„ÄÇÂÖ∑‰ΩìÁöÑ„Å´„ÅØ:
„Ç§„É≥„Çπ„Éà„Éº„É´„ÅÆ„Çø„Ç§„Éï„Çö(Docker„Åæ„Åü„ÅØ„ÉÜ„Çô„Çπ„ÇØ„Éà„ÉÉ„Éï„Çö)
„Éà„Çô„Ç≠„É•„É°„É≥„Éà„Åã„ÇôËøΩÂä†„Åæ„Åü„ÅØÂâäÈô§„Åï„Çå„Åü„Å®„Åç„ÄÇ„Éà„Çô„Ç≠„É•„É°„É≥„Éà„Å´„Å§„ÅÑ„Å¶„ÅÆÊÉÖÂ†±„ÅØ„ÅÇ„Çä„Åæ„Åõ„Çì„ÄÇ„Ç§„Éò„Çô„É≥„Éà„Åã„ÇôÁô∫Áîü„Åó„Åü„Åì„Å®„ÅÆ„Åø„ÇíÁü•„Çä„Åæ„Åô„ÄÇ„Åì„Çå„Å´„Çà„Çä„ÄÅ‰ΩøÁî®Áä∂Ê≥Å„ÇíÊääÊè°„Å¶„Çô„Åç„Åæ„Åô„ÄÇ
‰ΩøÁî®‰∏≠„ÅÆ„Éò„Çô„ÇØ„Éà„É´„ÉÜ„Çô„Éº„Çø„Éò„Çô„Éº„Çπ„ÅÆ„Çø„Ç§„Éï„Çö„ÄÇ„Å®„Çô„ÅÆ„Éò„Çô„ÇØ„Éà„É´„ÉÜ„Çô„Éº„Çø„Éò„Çô„Éº„Çπ„Éï„Çö„É≠„Éè„Çô„Ç§„Çø„Çô„Éº„Åã„ÇôÊúÄ„ÇÇ‰ΩøÁî®„Åï„Çå„Å¶„ÅÑ„Çã„Åã„ÇíÁü•„Çä„ÄÅÊõ¥Êñ∞„Åã„Çô„ÅÇ„Å£„Åü„Å®„Åç„Å´ÂÑ™ÂÖà„Åó„Å¶Â§âÊõ¥„ÇíË°å„ÅÑ„Åæ„Åô„ÄÇ
‰ΩøÁî®‰∏≠„ÅÆLLM„ÅÆ„Çø„Ç§„Éï„Çö„ÄÇÊúÄ„ÇÇ‰∫∫Ê∞ó„ÅÆ„ÅÇ„ÇãÈÅ∏ÊäûËÇ¢„ÇíÁü•„Çä„ÄÅÊõ¥Êñ∞„Åã„Çô„ÅÇ„Å£„Åü„Å®„Åç„Å´ÂÑ™ÂÖà„Åó„Å¶Â§âÊõ¥„ÇíË°å„ÅÑ„Åæ„Åô„ÄÇ
„ÉÅ„É£„ÉÉ„Éà„Åã„ÇôÈÄÅ‰ø°„Åï„Çå„Åü„ÄÇ„Åì„Çå„ÅØÊúÄ„ÇÇ‰∏ÄËà¨ÁöÑ„Å™„Äå„Ç§„Éò„Çô„É≥„Éà„Äç„Å¶„Çô„ÅÇ„Çä„ÄÅ„Åô„Å∏„Çô„Å¶„ÅÆ„Ç§„É≥„Çπ„Éà„Éº„É´„Å¶„Çô„ÅÆ„Åì„ÅÆ„Éï„Çö„É≠„Ç∑„Çô„Çß„ÇØ„Éà„ÅÆÊó•Â∏∏ÁöÑ„Å™„Äå„Ç¢„ÇØ„ÉÜ„Ç£„Éí„Çô„ÉÜ„Ç£„Äç„Å´„Å§„ÅÑ„Å¶„ÅÆ„Ç¢„Ç§„ÉÜ„Çô„Ç¢„ÇíÊèê‰æõ„Åó„Åæ„Åô„ÄÇÂÜç„Å≤„Çô„ÄÅ„Ç§„Éò„Çô„É≥„Éà„ÅÆ„Åø„Åã„ÇôÈÄÅ‰ø°„Åï„Çå„ÄÅ„ÉÅ„É£„ÉÉ„ÉàËá™‰Ωì„ÅÆÊÄßË≥™„ÇÑÂÜÖÂÆπ„Å´Èñ¢„Åô„ÇãÊÉÖÂ†±„ÅØ„ÅÇ„Çä„Åæ„Åõ„Çì„ÄÇ
„Åì„Çå„Çâ„ÅÆ‰∏ªÂºµ„ÇíÊ§úË®º„Åô„Çã„Å´„ÅØ„ÄÅ`Telemetry.sendTelemetry`„Åã„ÇôÂëº„Å≤„ÇôÂá∫„Åï„Çå„Çã„Åô„Å∏„Çô„Å¶„ÅÆÂ†¥ÊâÄ„ÇíË¶ã„Å§„Åë„Å¶„Åè„Åü„Çô„Åï„ÅÑ„ÄÇ„Åæ„Åü„ÄÅ„Åì„Çå„Çâ„ÅÆ„Ç§„Éò„Çô„É≥„Éà„ÅØÂá∫Âäõ„É≠„ÇØ„Çô„Å´Êõ∏„ÅçËæº„Åæ„Çå„Çã„Åü„ÇÅ„ÄÅÈÄÅ‰ø°„Åï„Çå„ÅüÂÖ∑‰ΩìÁöÑ„Å™„ÉÜ„Çô„Éº„Çø„ÇÇÁ¢∫Ë™ç„Å¶„Çô„Åç„Åæ„Åô„ÄÇIP„Ç¢„Éà„Çô„É¨„Çπ„ÇÑ„Åù„ÅÆ‰ªñ„ÅÆË≠òÂà•ÊÉÖÂ†±„ÅØÂèéÈõÜ„Åï„Çå„Åæ„Åõ„Çì„ÄÇ„ÉÜ„É¨„É°„Éà„É™„Éº„Éï„Çö„É≠„Éè„Çô„Ç§„Çø„Çô„Éº„ÅØPostHog„Å¶„Çô„Åô„ÄÇ
„ÇΩ„Éº„Çπ„Ç≥„Éº„Éà„ÇôÂÜÖ„ÅÆ„Åô„Å∏„Çô„Å¶„ÅÆ„ÉÜ„É¨„É°„Éà„É™„Éº„Ç§„Éò„Çô„É≥„Éà„ÇíË°®Á§∫
</details>
üîó „Åù„ÅÆ‰ªñ„ÅÆË£ΩÂìÅ
**[VectorAdmin][vector-admin]**:„Éò„Çô„ÇØ„Éà„É´„ÉÜ„Çô„Éº„Çø„Éò„Çô„Éº„Çπ„ÇíÁÆ°ÁêÜ„Åô„Çã„Åü„ÇÅ„ÅÆ„Ç™„Éº„É´„Ç§„É≥„ÉØ„É≥GUI„Åä„Çà„Å≤„Çô„ÉÑ„Éº„É´„Çπ„Ç§„Éº„Éà„ÄÇ
**[OpenAI Assistant Swarm][assistant-swarm]**:Âçò‰∏Ä„ÅÆ„Ç®„Éº„Ç∑„Çô„Çß„É≥„Éà„Åã„ÇâÊåáÊèÆ„Å¶„Çô„Åç„ÇãOpenAI„Ç¢„Ç∑„Çπ„Çø„É≥„Éà„ÅÆËªçÈöä„Å´„ÄÅ„É©„Ç§„Éï„Çô„É©„É™ÂÖ®‰Ωì„ÇíÂ§âÊèõ„Åó„Åæ„Åô„ÄÇ
<div align="right">
[
</div>
Copyright ¬© 2025 [Mintplex Labs][profile-link]„ÄÇ<br />
„Åì„ÅÆ„Éï„Çö„É≠„Ç∑„Çô„Çß„ÇØ„Éà„ÅØMIT„É©„Ç§„Çª„É≥„Çπ„ÅÆ‰∏ã„Å¶„Çô„É©„Ç§„Çª„É≥„Çπ„Åï„Çå„Å¶„ÅÑ„Åæ„Åô„ÄÇ
<!-- LINK GROUP -->
[back-to-top]: 
[profile-link]: 
[vector-admin]: 
[assistant-swarm]: 
[docker-btn]: ./images/deployBtns/docker.png
[docker-deploy]: ./docker/HOW_TO_USE_DOCKER.md
[aws-btn]: ./images/deployBtns/aws.png
[aws-deploy]: ./cloud-deployments/aws/cloudformation/DEPLOY.md
[gcp-btn]: 
[gcp-deploy]: ./cloud-deployments/gcp/deployment/DEPLOY.md
[do-btn]: 
[do-deploy]: ./cloud-deployments/digitalocean/terraform/DEPLOY.md
[render-btn]: 
[render-deploy]: 
[render-btn]: 
[render-deploy]: 
[railway-btn]: 
[railway-deploy]:


===== C:\Users\Breno\AppData\Local\Temp\repo_1u3zfsju\repo\locales\README.tr-TR.md =====
<a name="readme-top"></a>
<p align="center">
 <a href=" src=" alt="AnythingLLM logo"></a>
</p>
<div align='center'>
<a href=" target="_blank"><img src=" alt="Mintplex-Labs%2Fanything-llm | Trendshift" style="width: 250px; height: 55px;" width="250" height="55"/></a>
</div>
<p align="center">
<b>AnythingLLM:</b> Aradƒ±gÃÜƒ±nƒ±z hepsi bir arada yapay zeka uygulamasƒ±.<br />
Belgelerinizle sohbet edin, yapay zeka ajanlarƒ±nƒ± kullanƒ±n, son derece oÃàzellesÃßtirilebilir, cÃßok kullanƒ±cƒ±lƒ± ve zahmetsiz kurulum!
</p>
<p align="center">
 <a href=" target="_blank">
 <img src=" alt="Discord">
 </a> |
 <a href=" target="_blank">
 <img src=" alt="License">
 </a> |
 <a href=" target="_blank">
 Docs
 </a> |
 <a href=" target="_blank">
 Hosted Instance
 </a>
</p>
<p align="center">
 <b>English</b> ¬∑ <a href='./locales/README.zh-CN.md'>ÁÆÄ‰Ωì‰∏≠Êñá</a> ¬∑ <a href='./locales/README.ja-JP.md'>Êó•Êú¨Ë™û</a> ¬∑ <a href='./locales/README.tr-TR.md'>Turkish</a>
</p>
<p align="center"> 
üëâ MasauÃàstuÃà icÃßin AnythingLLM (Mac, Windows ve Linux)! <a href=" target="_blank"> SÃßimdi IÃándir</a> 
</p> 
Herhangi bir belgeyi, kaynagÃÜƒ± veya icÃßerigÃÜi sohbet sƒ±rasƒ±nda herhangi bir buÃàyuÃàk dil modelinin referans olarak kullanabilecegÃÜi bir bagÃÜlama doÃànuÃàsÃßtuÃàrmenizi sagÃÜlayan tam kapsamlƒ± bir uygulama. Bu uygulama, kullanmak istedigÃÜiniz LLM veya VektoÃàr Veritabanƒ±nƒ± secÃßmenize olanak tanƒ±rken, cÃßok kullanƒ±cƒ±lƒ± yoÃànetim ve yetkilendirme destegÃÜi de sunar.
<details>
<summary><kbd>Demoyu izle!</kbd></summary>
[](
</details>
UÃàruÃàn Genel Bakƒ±sÃßƒ± 
AnythingLLM, ticari hazƒ±r buÃàyuÃàk dil modellerini veya popuÃàler acÃßƒ±k kaynak LLM'leri ve vektoÃàr veritabanƒ± cÃßoÃàzuÃàmlerini kullanarak, hicÃßbir oÃàduÃàn vermeden oÃàzel bir ChatGPT olusÃßturmanƒ±za olanak tanƒ±yan tam kapsamlƒ± bir uygulamadƒ±r. Bu uygulamayƒ± yerel olarak cÃßalƒ±sÃßtƒ±rabilir veya uzaktan barƒ±ndƒ±rarak sagÃÜladƒ±gÃÜƒ±nƒ±z belgelerle akƒ±llƒ± sohbetler gercÃßeklesÃßtirebilirsiniz. 
AnythingLLM, belgelerinizi **"cÃßalƒ±sÃßma alanlarƒ±" (workspaces)** adƒ± verilen nesnelere ayƒ±rƒ±r. Bir cÃßalƒ±sÃßma alanƒ±, bir sohbet dizisi gibi cÃßalƒ±sÃßƒ±r ancak belgelerinizi kapsuÃàlleyen bir yapƒ± sunar. CÃßalƒ±sÃßma alanlarƒ± belgeleri paylasÃßabilir, ancak birbirleriyle iletisÃßim kurmaz, boÃàylece her cÃßalƒ±sÃßma alanƒ±nƒ±n bagÃÜlamƒ±nƒ± temiz tutabilirsiniz. 
AnythingLLM‚Äôin Harika OÃàzellikleri 
üÜï **OÃàzel Yapay Zeka Ajanlarƒ±** 
üÜï **Kod yazmadan AI Ajanƒ± olusÃßturma aracƒ±** 
üñºÔ∏è **CÃßoklu-mod destegÃÜi (hem kapalƒ± kaynak hem de acÃßƒ±k kaynak LLM'ler!)** 
üë§ CÃßok kullanƒ±cƒ±lƒ± destek ve yetkilendirme _(Yalnƒ±zca Docker suÃàruÃàmuÃànde)_ 
ü¶æ CÃßalƒ±sÃßma alanƒ± icÃßinde ajanlar (web'de gezinme vb.) 
üí¨ Web sitenize goÃàmuÃàlebilir oÃàzel sohbet aracƒ± _(Yalnƒ±zca Docker suÃàruÃàmuÃànde)_ 
üìñ CÃßoklu belge tuÃàruÃà destegÃÜi (PDF, TXT, DOCX vb.) 
Sade ve kullanƒ±sÃßlƒ± sohbet arayuÃàzuÃà, suÃàruÃàkle-bƒ±rak oÃàzelligÃÜi ve net kaynak goÃàsterimi. 
%100 bulut konusÃßlandƒ±rmaya hazƒ±r. 
TuÃàm popuÃàler kapalƒ± ve acÃßƒ±k kaynak LLM sagÃÜlayƒ±cƒ±larƒ±yla uyumlu. 
BuÃàyuÃàk belgeleri yoÃànetirken zaman ve maliyet tasarrufu sagÃÜlayan dahili optimizasyonlar. 
OÃàzel entegrasyonlar icÃßin tam kapsamlƒ± GelisÃßtirici API‚Äôsi. 
Ve cÃßok daha fazlasƒ±... Kurup kesÃßfedin!
Desteklenen LLM'ler, Embedding Modelleri, KonusÃßma Modelleri ve VektoÃàr Veritabanlarƒ± 
BuÃàyuÃàk Dil Modelleri (LLMs):**
Any open-source llama.cpp compatible model
OpenAI
OpenAI (Generic)
Azure OpenAI
AWS Bedrock
Anthropic
NVIDIA NIM (chat models)
Google Gemini Pro
Hugging Face (chat models)
Ollama (chat models)
LM Studio (all models)
LocalAi (all models)
Together AI (chat models)
Fireworks AI (chat models)
Perplexity (chat models)
OpenRouter (chat models)
DeepSeek (chat models)
Mistral
Groq
Cohere
KoboldCPP
LiteLLM
Text Generation Web UI
Apipie
xAI
Novita AI (chat models)
PPIO
Embedder modelleri:**
AnythingLLM Native Embedder (default)
OpenAI
Azure OpenAI
LocalAi (all)
Ollama (all)
LM Studio (all)
Cohere
Ses Transkripsiyon Modelleri:**
AnythingLLM Built-in (default)
OpenAI
TTS (text-to-speech) destegÃÜi:**
Native Browser Built-in (default)
PiperTTSLocal - runs in browser
OpenAI TTS
ElevenLabs
Any OpenAI Compatible TTS service.
STT (speech-to-text) destegÃÜi:**
Native Browser Built-in (default)
VektoÃàr Databases:**
LanceDB (default)
PGVector
Astra DB
Pinecone
Chroma
Weaviate
Qdrant
Milvus
Zilliz
Teknik Genel Bakƒ±sÃß 
Bu monorepo uÃàcÃß ana boÃàluÃàmden olusÃßmaktadƒ±r: 
**`frontend`**: ViteJS + React tabanlƒ± bir oÃàn yuÃàz, LLM'in kullanabilecegÃÜi tuÃàm icÃßerigÃÜi kolayca olusÃßturup yoÃànetmenizi sagÃÜlar. 
**`server`**: NodeJS ve Express tabanlƒ± bir sunucu, tuÃàm etkilesÃßimleri yoÃànetir ve vektoÃàr veritabanƒ± isÃßlemleri ile LLM entegrasyonlarƒ±nƒ± gercÃßeklesÃßtirir. 
**`collector`**: Kullanƒ±cƒ± arayuÃàzuÃànden gelen belgeleri isÃßleyen ve ayrƒ±sÃßtƒ±ran NodeJS Express tabanlƒ± bir sunucu. 
**`docker`**: Docker kurulum talimatlarƒ±, derleme suÃàreci ve kaynak koddan nasƒ±l derlenecegÃÜine dair bilgiler icÃßerir. 
**`embed`**: Web goÃàmme widget‚Äôƒ± olusÃßturma ve entegrasyonu icÃßin alt moduÃàl. 
**`browser-extension`**: Chrome tarayƒ±cƒ± eklentisi icÃßin alt moduÃàl.
üõ≥ Kendi Sunucunuzda Barƒ±ndƒ±rma 
Mintplex Labs ve topluluk, AnythingLLM'i yerel olarak cÃßalƒ±sÃßtƒ±rmak icÃßin cÃßesÃßitli dagÃÜƒ±tƒ±m yoÃàntemleri, betikler ve sÃßablonlar sunmaktadƒ±r. AsÃßagÃÜƒ±daki tabloya goÃàz atarak tercih ettigÃÜiniz ortamda nasƒ±l dagÃÜƒ±tƒ±m yapabilecegÃÜinizi oÃàgÃÜrenebilir veya otomatik dagÃÜƒ±tƒ±m secÃßeneklerini kesÃßfedebilirsiniz.
| Docker | AWS | GCP | Digital Ocean | Render.com |
|----------------------------------------|----|-----|---------------|------------|
| [![Deploy on Docker][docker-btn]][docker-deploy] | [![Deploy on AWS][aws-btn]][aws-deploy] | [![Deploy on GCP][gcp-btn]][gcp-deploy] | [![Deploy on DigitalOcean][do-btn]][do-deploy] | [![Deploy on Render.com][render-btn]][render-deploy] |
| Railway | RepoCloud | Elestio |
| --- | --- | --- |
| [![Deploy on Railway][railway-btn]][railway-deploy] | [![Deploy on RepoCloud][repocloud-btn]][repocloud-deploy] | [![Deploy on Elestio][elestio-btn]][elestio-deploy] |
veya Docker kullanmadan uÃàretim ortamƒ±nda AnythingLLM kurun ‚Üí 
GelisÃßtirme IÃácÃßin Kurulum 
`yarn setup` ‚Üí Uygulamanƒ±n her bilesÃßeni icÃßin gerekli `.env` dosyalarƒ±nƒ± olusÃßturur (repo‚Äônun koÃàk dizininden cÃßalƒ±sÃßtƒ±rƒ±lmalƒ±dƒ±r). 
 - Devam etmeden oÃànce bu dosyalarƒ± doldurun. **OÃàzellikle `server/.env.development` dosyasƒ±nƒ±n dolduruldugÃÜundan emin olun**, aksi takdirde sistem duÃàzguÃàn cÃßalƒ±sÃßmaz. 
`yarn dev:server` ‚Üí Sunucuyu yerel olarak basÃßlatƒ±r (repo‚Äônun koÃàk dizininden cÃßalƒ±sÃßtƒ±rƒ±lmalƒ±dƒ±r). 
`yarn dev:frontend` ‚Üí OÃàn yuÃàzuÃà yerel olarak cÃßalƒ±sÃßtƒ±rƒ±r (repo‚Äônun koÃàk dizininden cÃßalƒ±sÃßtƒ±rƒ±lmalƒ±dƒ±r). 
`yarn dev:collector` ‚Üí Belge toplayƒ±cƒ±yƒ± cÃßalƒ±sÃßtƒ±rƒ±r (repo‚Äônun koÃàk dizininden cÃßalƒ±sÃßtƒ±rƒ±lmalƒ±dƒ±r). 
Belgeler hakkƒ±nda bilgi edinin 
VektoÃàr oÃànbellekleme hakkƒ±nda bilgi edinin 
Harici Uygulamalar ve Entegrasyonlar 
_Bu uygulamalar Mintplex Labs tarafƒ±ndan yoÃànetilmemektedir, ancak AnythingLLM ile uyumludur. Burada listelenmeleri bir onay anlamƒ±na gelmez._ 
Midori AI Alt Sistem YoÃàneticisi - Docker konteyner teknolojisini kullanarak yapay zeka sistemlerini verimli bir sÃßekilde dagÃÜƒ±tmanƒ±n pratik bir yolu. 
Coolify - Tek tƒ±klamayla AnythingLLM dagÃÜƒ±tƒ±mƒ± yapmanƒ±za olanak tanƒ±r. 
GPTLocalhost for Microsoft Word - AnythingLLM‚Äôi Microsoft Word icÃßinde kullanmanƒ±za olanak tanƒ±yan yerel bir Word eklentisi.
Telemetri ve Gizlilik 
Mintplex Labs Inc. tarafƒ±ndan gelisÃßtirilen AnythingLLM, anonim kullanƒ±m bilgilerini toplayan bir telemetri oÃàzelligÃÜi icÃßermektedir. 
<details> 
<summary><kbd>AnythingLLM icÃßin Telemetri ve Gizlilik hakkƒ±nda daha fazla bilgi</kbd></summary> 
Neden? 
Bu bilgileri, AnythingLLM‚Äôin nasƒ±l kullanƒ±ldƒ±gÃÜƒ±nƒ± anlamak, yeni oÃàzellikler ve hata duÃàzeltmelerine oÃàncelik vermek ve uygulamanƒ±n performansƒ±nƒ± ve kararlƒ±lƒ±gÃÜƒ±nƒ± iyilesÃßtirmek icÃßin kullanƒ±yoruz. 
Telemetriden CÃßƒ±kƒ±sÃß Yapma (Opt-Out) 
Sunucu veya Docker `.env` ayarlarƒ±nda `DISABLE_TELEMETRY` degÃÜerini "true" olarak ayarlayarak telemetriyi devre dƒ±sÃßƒ± bƒ±rakabilirsiniz. Ayrƒ±ca, uygulama icÃßinde **Kenar CÃßubugÃÜu > Gizlilik** boÃàluÃàmuÃàne giderek de bu oÃàzelligÃÜi kapatabilirsiniz. 
Hangi Verileri AcÃßƒ±kcÃßa Takip Ediyoruz? 
Yalnƒ±zca uÃàruÃàn ve yol haritasƒ± kararlarƒ±nƒ± almamƒ±za yardƒ±mcƒ± olacak kullanƒ±m detaylarƒ±nƒ± takip ediyoruz: 
Kurulum tuÃàruÃà (Docker veya MasauÃàstuÃà) 
Bir belgenin eklenme veya kaldƒ±rƒ±lma olayƒ±. **Belgenin icÃßerigÃÜi hakkƒ±nda hicÃßbir bilgi toplanmaz**, yalnƒ±zca olayƒ±n gercÃßeklesÃßtigÃÜi kaydedilir. Bu, kullanƒ±m sƒ±klƒ±gÃÜƒ±nƒ± anlamamƒ±za yardƒ±mcƒ± olur. 
Kullanƒ±lan vektoÃàr veritabanƒ± tuÃàruÃà. Hangi sagÃÜlayƒ±cƒ±nƒ±n daha cÃßok tercih edildigÃÜini belirlemek icÃßin bu bilgiyi topluyoruz. 
Kullanƒ±lan LLM tuÃàruÃà. En popuÃàler modelleri belirleyerek bu sagÃÜlayƒ±cƒ±lara oÃàncelik verebilmemizi sagÃÜlar. 
Sohbet basÃßlatƒ±lmasƒ±. Bu en sƒ±k gercÃßeklesÃßen "olay" olup, projenin guÃànluÃàk etkinligÃÜi hakkƒ±nda genel bir fikir edinmemize yardƒ±mcƒ± olur. **Yalnƒ±zca olay kaydedilir, sohbetin icÃßerigÃÜi veya dogÃÜasƒ± hakkƒ±nda hicÃßbir bilgi toplanmaz.** 
Bu verileri dogÃÜrulamak icÃßin kod icÃßinde **`Telemetry.sendTelemetry` cÃßagÃÜrƒ±larƒ±nƒ±** inceleyebilirsiniz. Ayrƒ±ca, bu olaylar guÃànluÃàk kaydƒ±na yazƒ±ldƒ±gÃÜƒ± icÃßin hangi verilerin goÃànderildigÃÜini goÃàrebilirsiniz (egÃÜer etkinlesÃßtirilmisÃßse). **IP adresi veya digÃÜer tanƒ±mlayƒ±cƒ± bilgiler toplanmaz.** Telemetri sagÃÜlayƒ±cƒ±sƒ±, acÃßƒ±k kaynaklƒ± bir telemetri toplama hizmeti olan PostHog‚Äòdur. 
Kaynak kodda tuÃàm telemetri olaylarƒ±nƒ± goÃàruÃàntuÃàle
</details>
üëã Katkƒ±da Bulunma 
Bir **issue** olusÃßturun. 
`<issue numarasƒ±>-<kƒ±sa ad>` formatƒ±nda bir **PR (Pull Request)** olusÃßturun. 
CÃßekirdek ekipten **LGTM (Looks Good To Me)** onayƒ± alƒ±n. 
üåü Katkƒ±da Bulunanlar 
[]( 
[]( 
üîó DigÃÜer UÃàruÃànler 
**[VectorAdmin][vector-admin]:** VektoÃàr veritabanlarƒ±nƒ± yoÃànetmek icÃßin hepsi bir arada GUI ve aracÃß paketi. 
**[OpenAI Assistant Swarm][assistant-swarm]:** TuÃàm OpenAI asistanlarƒ±nƒ±zƒ± tek bir ajan tarafƒ±ndan yoÃànetilen bir yapay zeka ordusuna doÃànuÃàsÃßtuÃàruÃàn. 
<div align="right"> 
[ 
</div> 
Telif Hakkƒ± ¬© 2025 [Mintplex Labs][profile-link]. <br /> 
Bu proje MIT lisansƒ± ile lisanslanmƒ±sÃßtƒ±r.
<!-- LINK GROUP -->
[back-to-top]: 
[profile-link]: 
[vector-admin]: 
[assistant-swarm]: 
[docker-btn]: ./images/deployBtns/docker.png
[docker-deploy]: ./docker/HOW_TO_USE_DOCKER.md
[aws-btn]: ./images/deployBtns/aws.png
[aws-deploy]: ./cloud-deployments/aws/cloudformation/DEPLOY.md
[gcp-btn]: 
[gcp-deploy]: ./cloud-deployments/gcp/deployment/DEPLOY.md
[do-btn]: 
[do-deploy]: ./cloud-deployments/digitalocean/terraform/DEPLOY.md
[render-btn]: 
[render-deploy]: 
[render-btn]: 
[render-deploy]: 
[railway-btn]: 
[railway-deploy]: 
[repocloud-btn]: 
[repocloud-deploy]: 
[elestio-btn]: 
[elestio-deploy]:


===== C:\Users\Breno\AppData\Local\Temp\repo_1u3zfsju\repo\locales\README.zh-CN.md =====
<a name="readme-top"></a>
<p align="center">
 <a href=" src=" alt="AnythingLLM logo"></a>
</p>
<div align='center'>
<a href=" target="_blank"><img src=" alt="Mintplex-Labs%2Fanything-llm | Trendshift" style="width: 250px; height: 55px;" width="250" height="55"/></a>
</div>
<p align="center">
 <b>AnythingLLM:</b> ÊÇ®‰∏ÄÁõ¥Âú®ÂØªÊâæÁöÑÂÖ®Êñπ‰ΩçAIÂ∫îÁî®Á®ãÂ∫è„ÄÇ<br />
 ‰∏éÊÇ®ÁöÑÊñáÊ°£ËÅäÂ§©,‰ΩøÁî®AI‰ª£ÁêÜ,È´òÂ∫¶ÂèØÈÖçÁΩÆ,Â§öÁî®Êà∑,Êó†ÈúÄÁπÅÁêêÁöÑËÆæÁΩÆ„ÄÇ
</p>
<p align="center">
	<a href=" target="_blank">
 <img src=" alt="Discord">
 </a> |
 <a href=" target="_blank">
 <img src=" alt="ËÆ∏ÂèØËØÅ">
 </a> |
 <a href=" target="_blank">
 ÊñáÊ°£
 </a> |
 <a href=" target="_blank">
 ÊâòÁÆ°ÂÆû‰æã
 </a>
</p>
<p align="center">
 <a href='../README.md'>English</a> ¬∑ <b>ÁÆÄ‰Ωì‰∏≠Êñá</b> ¬∑ <a href='./README.ja-JP.md'>Êó•Êú¨Ë™û</a>
</p>
<p align="center">
üëâ ÈÄÇÁî®‰∫éÊ°åÈù¢(Mac„ÄÅWindowsÂíåLinux)ÁöÑAnythingLLM!<a href=" target="_blank">Á´ãÂç≥‰∏ãËΩΩ</a>
</p>
ËøôÊòØ‰∏Ä‰∏™ÂÖ®Ê†àÂ∫îÁî®Á®ãÂ∫è,ÂèØ‰ª•Â∞Ü‰ªª‰ΩïÊñáÊ°£„ÄÅËµÑÊ∫ê(Â¶ÇÁΩëÂùÄÈìæÊé•„ÄÅÈü≥È¢ë„ÄÅËßÜÈ¢ë)ÊàñÂÜÖÂÆπÁâáÊÆµËΩ¨Êç¢‰∏∫‰∏ä‰∏ãÊñá,‰ª•‰æø‰ªª‰ΩïÂ§ßËØ≠Ë®ÄÊ®°Âûã(LLM)Âú®ËÅäÂ§©ÊúüÈó¥‰Ωú‰∏∫ÂèÇËÄÉ‰ΩøÁî®„ÄÇÊ≠§Â∫îÁî®Á®ãÂ∫èÂÖÅËÆ∏ÊÇ®ÈÄâÊã©‰ΩøÁî®Âì™‰∏™LLMÊàñÂêëÈáèÊï∞ÊçÆÂ∫ì,ÂêåÊó∂ÊîØÊåÅÂ§öÁî®Êà∑ÁÆ°ÁêÜÂπ∂ËÆæÁΩÆ‰∏çÂêåÊùÉÈôê„ÄÇ
<details>
<summary><kbd>ËßÇÁúãÊºîÁ§∫ËßÜÈ¢ë!</kbd></summary>
[](
</details>
‰∫ßÂìÅÊ¶ÇËßà
AnythingLLMÊòØ‰∏Ä‰∏™ÂÖ®Ê†àÂ∫îÁî®Á®ãÂ∫è,ÊÇ®ÂèØ‰ª•‰ΩøÁî®Áé∞ÊàêÁöÑÂïÜ‰∏öÂ§ßËØ≠Ë®ÄÊ®°ÂûãÊàñÊµÅË°åÁöÑÂºÄÊ∫êÂ§ßËØ≠Ë®ÄÊ®°Âûã,ÂÜçÁªìÂêàÂêëÈáèÊï∞ÊçÆÂ∫ìËß£ÂÜ≥ÊñπÊ°àÊûÑÂª∫‰∏Ä‰∏™ÁßÅÊúâChatGPT,‰∏çÂÜçÂèóÂà∂‰∫é‰∫∫:ÊÇ®ÂèØ‰ª•Êú¨Âú∞ËøêË°å,‰πüÂèØ‰ª•ËøúÁ®ãÊâòÁÆ°,Âπ∂ËÉΩÂ§ü‰∏éÊÇ®Êèê‰æõÁöÑ‰ªª‰ΩïÊñáÊ°£Êô∫ËÉΩËÅäÂ§©„ÄÇ
AnythingLLMÂ∞ÜÊÇ®ÁöÑÊñáÊ°£ÂàíÂàÜ‰∏∫Áß∞‰∏∫`workspaces` (Â∑•‰ΩúÂå∫)ÁöÑÂØπË±°„ÄÇÂ∑•‰ΩúÂå∫ÁöÑÂäüËÉΩÁ±ª‰ºº‰∫éÁ∫øÁ®ã,ÂêåÊó∂Â¢ûÂä†‰∫ÜÊñáÊ°£ÁöÑÂÆπÂô®Âåñ„ÄÇÂ∑•‰ΩúÂå∫ÂèØ‰ª•ÂÖ±‰∫´ÊñáÊ°£,‰ΩÜÂ∑•‰ΩúÂå∫‰πãÈó¥ÁöÑÂÜÖÂÆπ‰∏ç‰ºö‰∫íÁõ∏Âπ≤Êâ∞ÊàñÊ±°Êüì,Âõ†Ê≠§ÊÇ®ÂèØ‰ª•‰øùÊåÅÊØè‰∏™Â∑•‰ΩúÂå∫ÁöÑ‰∏ä‰∏ãÊñáÊ∏ÖÊô∞„ÄÇ
AnythingLLMÁöÑ‰∏Ä‰∫õÈÖ∑ÁÇ´ÁâπÊÄß
üÜï **ÂÆåÂÖ®ÂÖºÂÆπ MCP**
üÜï **Êó†‰ª£Á†ÅAI‰ª£ÁêÜÊûÑÂª∫Âô®**
üñºÔ∏è **Â§öÁî®Êà∑ÂÆû‰æãÊîØÊåÅÂíåÊùÉÈôêÁÆ°ÁêÜ(ÊîØÊåÅÂ∞ÅÈó≠Ê∫êÂíåÂºÄÊ∫êLLM!)**
**Ëá™ÂÆö‰πâ‰∫∫Â∑•Êô∫ËÉΩ‰ª£ÁêÜ**
üë§ Â§öÁî®Êà∑ÂÆû‰æãÊîØÊåÅÂíåÊùÉÈôêÁÆ°ÁêÜ _‰ªÖÈôêDockerÁâàÊú¨_
ü¶æ Â∑•‰ΩúÂå∫ÂÜÖÁöÑÊô∫ËÉΩ‰Ωì(ÊµèËßàÁΩëÈ°µ„ÄÅËøêË°å‰ª£Á†ÅÁ≠â)
üí¨ ‰∏∫ÊÇ®ÁöÑÁΩëÁ´ôÂÆöÂà∂ÁöÑÂèØÂµåÂÖ•ËÅäÂ§©Á™óÂè£
üìñ ÊîØÊåÅÂ§öÁßçÊñáÊ°£Á±ªÂûã(PDF„ÄÅTXT„ÄÅDOCXÁ≠â)
Â∏¶ÊúâÊãñÊîæÂäüËÉΩÂíåÊ∏ÖÊô∞ÂºïÁî®ÁöÑÁÆÄÊ¥ÅËÅäÂ§©ÁïåÈù¢„ÄÇ
100%‰∫ëÈÉ®ÁΩ≤Â∞±Áª™„ÄÇ
ÂÖºÂÆπÊâÄÊúâ‰∏ªÊµÅÁöÑÈó≠Ê∫êÂíåÂºÄÊ∫êÂ§ßËØ≠Ë®ÄÊ®°ÂûãÊèê‰æõÂïÜ„ÄÇ
ÂÜÖÁΩÆËäÇÁúÅÊàêÊú¨ÂíåÊó∂Èó¥ÁöÑÊú∫Âà∂,Áî®‰∫éÂ§ÑÁêÜË∂ÖÂ§ßÊñáÊ°£,‰ºò‰∫é‰ªª‰ΩïÂÖ∂‰ªñËÅäÂ§©ÁïåÈù¢„ÄÇ
ÂÖ®Â•óÁöÑÂºÄÂèë‰∫∫ÂëòAPI,Áî®‰∫éËá™ÂÆö‰πâÈõÜÊàê!
ËÄå‰∏îËøòÊúâÊõ¥Â§öÁ≤æÂΩ©ÂäüËÉΩ......ÂÆâË£ÖÂêé‰∫≤Ëá™‰ΩìÈ™åÂêß!
ÊîØÊåÅÁöÑLLM„ÄÅÂµåÂÖ•Ê®°Âûã„ÄÅËΩ¨ÂΩïÊ®°ÂûãÂíåÂêëÈáèÊï∞ÊçÆÂ∫ì
ÊîØÊåÅÁöÑLLM:**
‰ªª‰Ωï‰∏éllama.cppÂÖºÂÆπÁöÑÂºÄÊ∫êÊ®°Âûã
OpenAI
OpenAI (ÈÄöÁî®)
Azure OpenAI
AWS Bedrock
Anthropic
NVIDIA NIM (ËÅäÂ§©Ê®°Âûã)
Google Gemini Pro
Hugging Face (ËÅäÂ§©Ê®°Âûã)
Ollama (ËÅäÂ§©Ê®°Âûã)
LM Studio (ÊâÄÊúâÊ®°Âûã)
LocalAI (ÊâÄÊúâÊ®°Âûã)
Together AI (ËÅäÂ§©Ê®°Âûã)
Fireworks AI (ËÅäÂ§©Ê®°Âûã)
Perplexity (ËÅäÂ§©Ê®°Âûã)
OpenRouter (ËÅäÂ§©Ê®°Âûã)
DeepSeek (ËÅäÂ§©Ê®°Âûã)
Mistral
Groq
Cohere
KoboldCPP
LiteLLM
Text Generation Web UI
Apipie
xAI
Novita AI (ËÅäÂ§©Ê®°Âûã)
PPIO (ËÅäÂ§©Ê®°Âûã)
CometAPI (ËÅäÂ§©Ê®°Âûã)
ÊîØÊåÅÁöÑÂµåÂÖ•Ê®°Âûã:**
AnythingLLMÂéüÁîüÂµåÂÖ•Âô®(ÈªòËÆ§)
OpenAI
Azure OpenAI
LocalAI (ÂÖ®ÈÉ®)
Ollama (ÂÖ®ÈÉ®)
LM Studio (ÂÖ®ÈÉ®)
Cohere
ÊîØÊåÅÁöÑËΩ¨ÂΩïÊ®°Âûã:**
AnythingLLMÂÜÖÁΩÆ (ÈªòËÆ§)
OpenAI
TTS (ÊñáÊú¨ËΩ¨ËØ≠Èü≥) ÊîØÊåÅ:**
ÊµèËßàÂô®ÂÜÖÁΩÆ(ÈªòËÆ§)
PiperTTSLocal - Âú®ÊµèËßàÂô®‰∏≠ËøêË°å
OpenAI TTS
ElevenLabs
‰ªª‰Ωï‰∏é OpenAI ÂÖºÂÆπÁöÑ TTS ÊúçÂä°
STT (ËØ≠Èü≥ËΩ¨ÊñáÊú¨) ÊîØÊåÅ:**
ÊµèËßàÂô®ÂÜÖÁΩÆ(ÈªòËÆ§)
ÊîØÊåÅÁöÑÂêëÈáèÊï∞ÊçÆÂ∫ì:**
LanceDB (ÈªòËÆ§)
PGVector
Astra DB
Pinecone
Chroma
Weaviate
QDrant
Milvus
Zilliz
ÊäÄÊúØÊ¶ÇËßà
Ëøô‰∏™ÂçïÂ∫ìÁî±ÂÖ≠‰∏™‰∏ªË¶ÅÈÉ®ÂàÜÁªÑÊàê:
`frontend`: ‰∏Ä‰∏™ viteJS + React ÂâçÁ´Ø,ÊÇ®ÂèØ‰ª•ËøêË°åÂÆÉÊù•ËΩªÊùæÂàõÂª∫ÂíåÁÆ°ÁêÜLLMÂèØ‰ª•‰ΩøÁî®ÁöÑÊâÄÊúâÂÜÖÂÆπ„ÄÇ
`server`: ‰∏Ä‰∏™ NodeJS express ÊúçÂä°Âô®,Áî®‰∫éÂ§ÑÁêÜÊâÄÊúâ‰∫§‰∫íÂπ∂ËøõË°åÊâÄÊúâÂêëÈáèÊï∞ÊçÆÂ∫ìÁÆ°ÁêÜÂíå LLM ‰∫§‰∫í„ÄÇ
`collector`: NodeJS express ÊúçÂä°Âô®,Áî®‰∫é‰ªéUIÂ§ÑÁêÜÂíåËß£ÊûêÊñáÊ°£„ÄÇ
`docker`: Docker Êåá‰ª§ÂíåÊûÑÂª∫ËøáÁ®ã + ‰ªéÊ∫ê‰ª£Á†ÅÊûÑÂª∫ÁöÑ‰ø°ÊÅØ„ÄÇ
`embed`: Áî®‰∫éÁîüÊàêÂíåÂàõÂª∫ÁΩëÈ°µÂµåÂÖ•ÁªÑ‰ª∂ÁöÑÂ≠êÊ®°Âùó.
`browser-extension`: Áî®‰∫éChrome ÊµèËßàÂô®Êâ©Â±ïÁöÑÂ≠êÊ®°Âùó.
üõ≥ Ëá™ÊâòÁÆ°
Mintplex LabsÂíåÁ§æÂå∫Áª¥Êä§‰∫ÜËÆ∏Â§öÈÉ®ÁΩ≤ÊñπÊ≥ï„ÄÅËÑöÊú¨ÂíåÊ®°Êùø,ÊÇ®ÂèØ‰ª•‰ΩøÁî®ÂÆÉ‰ª¨Âú®Êú¨Âú∞ËøêË°åAnythingLLM„ÄÇËØ∑ÂèÇÈòÖ‰∏ãÈù¢ÁöÑË°®Ê†º,‰∫ÜËß£Â¶Ç‰ΩïÂú®ÊÇ®ÂñúÊ¨¢ÁöÑÁéØÂ¢É‰∏äÈÉ®ÁΩ≤,ÊàñËá™Âä®ÈÉ®ÁΩ≤„ÄÇ
| Docker | AWS | GCP | Digital Ocean | Render.com |
|----------------------------------------|----|-----|---------------|------------|
| [![Âú® Docker ‰∏äÈÉ®ÁΩ≤][docker-btn]][docker-deploy] | [![Âú® AWS ‰∏äÈÉ®ÁΩ≤][aws-btn]][aws-deploy] | [![Âú® GCP ‰∏äÈÉ®ÁΩ≤][gcp-btn]][gcp-deploy] | [![Âú®DigitalOcean‰∏äÈÉ®ÁΩ≤][do-btn]][do-deploy] | [![Âú® Render.com ‰∏äÈÉ®ÁΩ≤][render-btn]][render-deploy] |
| Railway | RepoCloud | Elestio |
| --- | --- | --- |
| [![Âú® Railway ‰∏äÈÉ®ÁΩ≤][railway-btn]][railway-deploy] | [![Âú® RepoCloud ‰∏äÈÉ®ÁΩ≤][repocloud-btn]][repocloud-deploy] | [![Âú® Elestio ‰∏äÈÉ®ÁΩ≤][elestio-btn]][elestio-deploy] |
ÂÖ∂‰ªñÊñπÊ°à:‰∏ç‰ΩøÁî®DockerÈÖçÁΩÆAnythingLLMÂÆû‰æã ‚Üí
Â¶Ç‰ΩïËÆæÁΩÆÂºÄÂèëÁéØÂ¢É
`yarn setup` Â°´ÂÖÖÊØè‰∏™Â∫îÁî®Á®ãÂ∫èÈÉ®ÂàÜÊâÄÈúÄÁöÑ `.env` Êñá‰ª∂(‰ªé‰ªìÂ∫ìÁöÑÊ†πÁõÆÂΩï)„ÄÇ
 - Âú®ÂºÄÂßã‰∏ã‰∏ÄÊ≠•‰πãÂâç,ÂÖàÂ°´ÂÜôËøô‰∫õ‰ø°ÊÅØ`server/.env.development`,‰∏çÁÑ∂‰ª£Á†ÅÊó†Ê≥ïÊ≠£Â∏∏ÊâßË°å„ÄÇ
`yarn dev:server` Âú®Êú¨Âú∞ÂêØÂä®ÊúçÂä°Âô®(‰ªé‰ªìÂ∫ìÁöÑÊ†πÁõÆÂΩï)„ÄÇ
`yarn dev:frontend` Âú®Êú¨Âú∞ÂêØÂä®ÂâçÁ´Ø(‰ªé‰ªìÂ∫ìÁöÑÊ†πÁõÆÂΩï)„ÄÇ
`yarn dev:collector` ÁÑ∂ÂêéËøêË°åÊñáÊ°£Êî∂ÈõÜÂô®(‰ªé‰ªìÂ∫ìÁöÑÊ†πÁõÆÂΩï)„ÄÇ
‰∫ÜËß£ÊñáÊ°£
‰∫ÜËß£ÂêëÈáèÁºìÂ≠ò
Â§ñÈÉ®Â∫îÁî®‰∏éÈõÜÊàê
_‰ª•‰∏ãÊòØ‰∏Ä‰∫õ‰∏é AnythingLLM ÂÖºÂÆπÁöÑÂ∫îÁî®Á®ãÂ∫è,‰ΩÜÂπ∂ÈùûÁî± Mintplex Labs Áª¥Êä§„ÄÇÂàóÂú®Ê≠§Â§ÑÂπ∂‰∏ç‰ª£Ë°®ÂÆòÊñπËÉå‰π¶„ÄÇ_
Midori AI Â≠êÁ≥ªÁªüÁÆ°ÁêÜÂô® - ‰ΩøÁî® Docker ÂÆπÂô®ÊäÄÊúØÈ´òÊïàÈÉ®ÁΩ≤ AI Á≥ªÁªüÁöÑÁÆÄÂåñÊñπÂºè - ‰ΩøÁî® Docker ÂÆπÂô®ÊäÄÊúØÈ´òÊïàÈÉ®ÁΩ≤ AI Á≥ªÁªüÁöÑÁÆÄÂåñÊñπÂºè„ÄÇ
Coolify - ‰∏ÄÈîÆÈÉ®ÁΩ≤ AnythingLLM„ÄÇ
ÈÄÇÁî®‰∫é Microsoft Word ÁöÑ GPTLocalhost - ‰∏Ä‰∏™Êú¨Âú∞ Word Êèí‰ª∂,ËÆ©‰Ω†ÂèØ‰ª•Âú® Microsoft Word ‰∏≠‰ΩøÁî® AnythingLLM„ÄÇ
ËøúÁ®ã‰ø°ÊÅØÊî∂ÈõÜ‰∏éÈöêÁßÅ‰øùÊä§
Áî± Mintplex Labs Inc ÂºÄÂèëÁöÑ AnythingLLM ÂåÖÂê´‰∏Ä‰∏™Êî∂ÈõÜÂåøÂêç‰ΩøÁî®‰ø°ÊÅØÁöÑ Telemetry ÂäüËÉΩ„ÄÇ
<details>
<summary><kbd>ÊúâÂÖ≥ AnythingLLM ÁöÑËøúÁ®ã‰ø°ÊÅØÊî∂ÈõÜ‰∏éÈöêÁßÅ‰øùÊä§Êõ¥Â§ö‰ø°ÊÅØ</kbd></summary>
‰∏∫‰ªÄ‰πàÊî∂ÈõÜ‰ø°ÊÅØ?
Êàë‰ª¨‰ΩøÁî®Ëøô‰∫õ‰ø°ÊÅØÊù•Â∏ÆÂä©Êàë‰ª¨ÁêÜËß£ AnythingLLM ÁöÑ‰ΩøÁî®ÊÉÖÂÜµ,Â∏ÆÂä©Êàë‰ª¨Á°ÆÂÆöÊñ∞ÂäüËÉΩÂíåÈîôËØØ‰øÆÂ§çÁöÑ‰ºòÂÖàÁ∫ß,Âπ∂Â∏ÆÂä©Êàë‰ª¨ÊèêÈ´ò AnythingLLM ÁöÑÊÄßËÉΩÂíåÁ®≥ÂÆöÊÄß„ÄÇ
ÊÄéÊ†∑ÂÖ≥Èó≠
Âú®ÊúçÂä°Âô®Êàñ Docker ÁöÑ .env ËÆæÁΩÆ‰∏≠Â∞Ü `DISABLE_TELEMETRY` ËÆæÁΩÆ‰∏∫ "true",Âç≥ÂèØÈÄâÊã©‰∏çÂèÇ‰∏éÈÅ•ÊµãÊï∞ÊçÆÊî∂ÈõÜ„ÄÇ‰Ω†‰πüÂèØ‰ª•Âú®Â∫îÁî®ÂÜÖÈÄöËøá‰ª•‰∏ãË∑ØÂæÑÊìç‰Ωú:‰æßËæπÊ†è > `Privacy` (ÈöêÁßÅ) > ÂÖ≥Èó≠ÈÅ•ÊµãÂäüËÉΩ„ÄÇ
‰Ω†‰ª¨Ë∑üË∏™Êî∂ÈõÜÂì™‰∫õ‰ø°ÊÅØ?
Êàë‰ª¨Âè™‰ºöË∑üË∏™ÊúâÂä©‰∫éÊàë‰ª¨ÂÅöÂá∫‰∫ßÂìÅÂíåË∑ØÁ∫øÂõæÂÜ≥Á≠ñÁöÑ‰ΩøÁî®ÁªÜËäÇ,ÂÖ∑‰ΩìÂåÖÊã¨:
ÊÇ®ÁöÑÂÆâË£ÖÊñπÂºè(DockerÊàñÊ°åÈù¢Áâà)
ÊñáÊ°£Ë¢´Ê∑ªÂä†ÊàñÁßªÈô§ÁöÑÊó∂Èó¥„ÄÇ‰ΩÜ‰∏çÂåÖÊã¨ÊñáÊ°£ÂÜÖÁöÑÂÖ∑‰ΩìÂÜÖÂÆπ„ÄÇÊàë‰ª¨Âè™ÂÖ≥Ê≥®Ê∑ªÂä†ÊàñÁßªÈô§ÊñáÊ°£Ëøô‰∏™Ë°å‰∏∫„ÄÇËøô‰∫õ‰ø°ÊÅØËÉΩËÆ©Êàë‰ª¨‰∫ÜËß£Âà∞ÊñáÊ°£ÂäüËÉΩÁöÑ‰ΩøÁî®ÊÉÖÂÜµ„ÄÇ
‰ΩøÁî®‰∏≠ÁöÑÂêëÈáèÊï∞ÊçÆÂ∫ìÁ±ªÂûã„ÄÇËÆ©Êàë‰ª¨Áü•ÈÅìÂì™‰∏™ÂêëÈáèÊï∞ÊçÆÂ∫ìÊúÄÂèóÊ¨¢Ëøé,Âπ∂Âú®ÂêéÁª≠Êõ¥Êñ∞‰∏≠‰ºòÂÖàËÄÉËôëÁõ∏Â∫îÁöÑÊï∞ÊçÆÂ∫ì„ÄÇ
‰ΩøÁî®‰∏≠ÁöÑLLMÁ±ªÂûã„ÄÇËÆ©Êàë‰ª¨Áü•ÈÅìË∞ÅÊâçÊòØÊúÄÂèóÊ¨¢ËøéÁöÑLLMÊ®°Âûã,Âπ∂Âú®ÂêéÁª≠Êõ¥Êñ∞‰∏≠‰ºòÂÖàËÄÉËôëÁõ∏Â∫îÊ®°Âûã„ÄÇ
‰ø°ÊÅØË¢´`ÂèëÈÄÅ`Âá∫Âéª„ÄÇËøôÊòØÊúÄÂ∏∏ËßÑÁöÑ‚Äú‰∫ã‰ª∂/Ë°å‰∏∫/event‚Äù,Âπ∂ËÆ©Êàë‰ª¨‰∫ÜËß£Âà∞ÊâÄÊúâÂÆâË£Ö‰∫ÜËøô‰∏™È°πÁõÆÁöÑÊØèÊó•Ê¥ªÂä®ÊÉÖÂÜµ„ÄÇÂêåÊ†∑,Âè™Êî∂ÈõÜ`ÂèëÈÄÅ`Ëøô‰∏™Ë°å‰∏∫ÁöÑ‰ø°ÊÅØ,Êàë‰ª¨‰∏ç‰ºöÊî∂ÈõÜÂÖ≥‰∫éËÅäÂ§©Êú¨Ë∫´ÁöÑÊÄßË¥®ÊàñÂÜÖÂÆπÁöÑ‰ªª‰Ωï‰ø°ÊÅØ„ÄÇ
ÊÇ®ÂèØ‰ª•ÈÄöËøáÊü•ÊâæÊâÄÊúâË∞ÉÁî®`Telemetry.sendTelemetry`ÁöÑ‰ΩçÁΩÆÊù•È™åËØÅËøô‰∫õÂ£∞Êòé„ÄÇÊ≠§Â§ñ,Â¶ÇÊûúÂêØÁî®,Ëøô‰∫õ‰∫ã‰ª∂‰πü‰ºöË¢´ÂÜôÂÖ•ËæìÂá∫Êó•Âøó,Âõ†Ê≠§ÊÇ®‰πüÂèØ‰ª•ÁúãÂà∞ÂèëÈÄÅ‰∫ÜÂì™‰∫õÂÖ∑‰ΩìÊï∞ÊçÆ„ÄÇ**IPÊàñÂÖ∂‰ªñËØÜÂà´‰ø°ÊÅØ‰∏ç‰ºöË¢´Êî∂ÈõÜ**„ÄÇTelemetryËøúÁ®ã‰ø°ÊÅØÊî∂ÈõÜÁöÑÊñπÊ°àÊù•Ëá™PostHog - ‰∏Ä‰∏™ÂºÄÊ∫êÁöÑËøúÁ®ã‰ø°ÊÅØÊî∂ÈõÜÊúçÂä°„ÄÇ
Êàë‰ª¨ÈùûÂ∏∏ÈáçËßÜÈöêÁßÅ,‰∏î‰∏çÁî®ÁÉ¶‰∫∫ÁöÑÂºπÁ™óÈóÆÂç∑Êù•Ëé∑ÂèñÂèçÈ¶à,Â∏åÊúõ‰Ω†ËÉΩÁêÜËß£‰∏∫‰ªÄ‰πàÊàë‰ª¨ÊÉ≥Ë¶ÅÁü•ÈÅìËØ•Â∑•ÂÖ∑ÁöÑ‰ΩøÁî®ÊÉÖÂÜµ,ËøôÊ†∑Êàë‰ª¨ÊâçËÉΩÊâìÈÄ†ÁúüÊ≠£ÂÄºÂæó‰ΩøÁî®ÁöÑ‰∫ßÂìÅ„ÄÇÊâÄÊúâÂåøÂêçÊï∞ÊçÆ _Áªù‰∏ç‰ºö_ ‰∏é‰ªª‰ΩïÁ¨¨‰∏âÊñπÂÖ±‰∫´„ÄÇ
Âú®Ê∫ê‰ª£Á†Å‰∏≠Êü•ÁúãÊâÄÊúâ‰ø°ÊÅØÊî∂ÈõÜÊ¥ªÂä®
</details>
üëã Â¶Ç‰ΩïË¥°ÁåÆ
ÂàõÂª∫ issue
ÂàõÂª∫ PR,ÂàÜÊîØÂêçÁß∞Ê†ºÂºè‰∏∫ `<issue number>-<short name>`
ÂêàÂπ∂
üíñ ËµûÂä©ÂïÜ
È´òÁ∫ßËµûÂä©ÂïÜ
<!-- premium-sponsors (reserved for $100/mth sponsors who request to be called out here and/or are non-private sponsors) -->
<a href=" target="_blank">
 <img src=" height="100px" alt="User avatar: DCS DIGITAL" />
</a>
<!-- premium-sponsors -->
ÊâÄÊúâËµûÂä©ÂïÜ
<!-- all-sponsors --><a href=" src=" width="60px" alt="User avatar: Jascha" /></a><a href=" src=" width="60px" alt="User avatar: KickAss" /></a><a href=" src=" width="60px" alt="User avatar: ShadowArcanist" /></a><a href=" src=" width="60px" alt="User avatar: Atlas" /></a><a href=" src=" width="60px" alt="User avatar: Predrag StojadinovicÃÅ" /></a><a href=" src=" width="60px" alt="User avatar: Diego Spinola" /></a><a href=" src=" width="60px" alt="User avatar: Kyle" /></a><a href=" src=" width="60px" alt="User avatar: Giulio De Pasquale" /></a><a href=" src=" width="60px" alt="User avatar: " /></a><a href=" src=" width="60px" alt="User avatar: MacStadium" /></a><a href=" src=" width="60px" alt="User avatar: " /></a><a href=" src=" width="60px" alt="User avatar: " /></a><a href=" src=" width="60px" alt="User avatar: " /></a><a href=" src=" width="60px" alt="User avatar: " /></a><a href=" src=" width="60px" alt="User avatar: Dennis" /></a><a href=" src=" width="60px" alt="User avatar: Michael Hamilton, Ph.D." /></a><a href=" src=" width="60px" alt="User avatar: " /></a><a href=" src=" width="60px" alt="User avatar: TernaryLabs" /></a><a href=" src=" width="60px" alt="User avatar: Daniel Cela" /></a><a href=" src=" width="60px" alt="User avatar: Alesso" /></a><a href=" src=" width="60px" alt="User avatar: Rune Mathisen" /></a><a href=" src=" width="60px" alt="User avatar: " /></a><a href=" src=" width="60px" alt="User avatar: " /></a><a href=" src=" width="60px" alt="User avatar: Alan" /></a><a href=" src=" width="60px" alt="User avatar: Damien Peters" /></a><a href=" src=" width="60px" alt="User avatar: DCS Digital" /></a><a href=" src=" width="60px" alt="User avatar: Paul Mcilreavy" /></a><a href=" src=" width="60px" alt="User avatar: Til Wolf" /></a><a href=" src=" width="60px" alt="User avatar: Leopoldo Crhistian Riverin Gomez" /></a><a href=" src=" width="60px" alt="User avatar: AJEsau" /></a><a href=" src=" width="60px" alt="User avatar: Steven VanOmmeren" /></a><a href=" src=" width="60px" alt="User avatar: Casey Boettcher" /></a><a href=" src=" width="60px" alt="User avatar: " /></a><a href=" src=" width="60px" alt="User avatar: Avineet" /></a><!-- all-sponsors -->
üåü Ë¥°ÁåÆËÄÖ‰ª¨
[](
[](
üîó Êõ¥Â§ö‰∫ßÂìÅ
**[VectorAdmin][vector-admin]**:‰∏Ä‰∏™Áî®‰∫éÁÆ°ÁêÜÂêëÈáèÊï∞ÊçÆÂ∫ìÁöÑÂÖ®Êñπ‰ΩçÂõæÂΩ¢Áî®Êà∑ÁïåÈù¢ÂíåÂ∑•ÂÖ∑Â•ó‰ª∂„ÄÇ
**[OpenAI Assistant Swarm][assistant-swarm]**:‰∏Ä‰∏™Êô∫ËÉΩ‰ΩìÂ∞±ÂèØ‰ª•ÁÆ°ÁêÜÊÇ®ÊâÄÊúâÁöÑOpenAIÂä©Êâã„ÄÇ
<div align="right">
[
</div>
ÁâàÊùÉÊâÄÊúâ ¬© 2025 [Mintplex Labs][profile-link]„ÄÇ<br />
Êú¨È°πÁõÆÈááÁî®MITËÆ∏ÂèØËØÅ„ÄÇ
<!-- LINK GROUP -->
[back-to-top]: 
[profile-link]: 
[vector-admin]: 
[assistant-swarm]: 
[docker-btn]: ../images/deployBtns/docker.png
[docker-deploy]: ../docker/HOW_TO_USE_DOCKER.md
[aws-btn]: ../images/deployBtns/aws.png
[aws-deploy]: ../cloud-deployments/aws/cloudformation/DEPLOY.md
[gcp-btn]: 
[gcp-deploy]: ../cloud-deployments/gcp/deployment/DEPLOY.md
[do-btn]: 
[do-deploy]: ../cloud-deployments/digitalocean/terraform/DEPLOY.md
[render-btn]: 
[render-deploy]: 
[render-btn]: 
[render-deploy]: 
[railway-btn]: 
[railway-deploy]: 
[repocloud-btn]: 
[repocloud-deploy]: 
[elestio-btn]: 
[elestio-deploy]:


===== C:\Users\Breno\AppData\Local\Temp\repo_1u3zfsju\repo\server\storage\README.md =====
AnythingLLM Storage
This folder is for the local or disk storage of ready-to-embed documents, vector-cached embeddings, and the disk-storage of LanceDB and the local SQLite database.
This folder should contain the following folders.
`documents`
`lancedb` (if using lancedb)
`vector-cache`
and a file named exactly `anythingllm.db`
Common issues
SQLITE_FILE_CANNOT_BE_OPENED** in the server log = The DB file does not exist probably because the node instance does not have the correct permissions to write a file to the disk. To solve this..
Local dev
 - Create a `anythingllm.db` empty file in this directory. Thats all. No need to reboot the server or anything. If your permissions are correct this should not ever occur since the server will create the file if it does not exist automatically.
Docker Instance
 - Get your AnythingLLM docker container id with `docker ps -a`. The container must be running to execute the next commands.
 - Run `docker container exec -u 0 -t <ANYTHINGLLM DOCKER CONTAINER ID> mkdir -p /app/server/storage /app/server/storage/documents /app/server/storage/vector-cache /app/server/storage/lancedb`
 - Run `docker container exec -u 0 -t <ANYTHINGLLM DOCKER CONTAINER ID> touch /app/server/storage/anythingllm.db`
 - Run `docker container exec -u 0 -t <ANYTHINGLLM DOCKER CONTAINER ID> chown -R anythingllm:anythingllm /app/collector /app/server`
 - The above commands will create the appropriate folders inside of the docker container and will persist as long as you do not destroy the container and volume. This will also fix any ownership issues of folder files in the collector and the server.


===== C:\Users\Breno\AppData\Local\Temp\repo_1u3zfsju\repo\server\storage\documents\DOCUMENTS.md =====
What is this folder of documents?
This is a temporary cache of the resulting files you have collected from `collector/`. You really should not be adding files manually to this folder. However the general format of this is you should partion data by how it was collected - it will be added to the appropriate namespace when you undergo vectorizing.
You can manage these files from the frontend application.
All files should be JSON files and in general there is only one main required key: `pageContent` all other keys will be inserted as metadata for each document inserted into the vector DB.
There is also a special reserved key called `published` that should be reserved for timestamps.


===== C:\Users\Breno\AppData\Local\Temp\repo_1u3zfsju\repo\server\storage\models\README.md =====
Native models used by AnythingLLM
This folder is specifically created as a local cache and storage folder that is used for native models that can run on a CPU.
Currently, AnythingLLM uses this folder for the following parts of the application.
Embedding
When your embedding engine preference is `native` we will use the ONNX **all-MiniLM-L6-v2** model built by Xenova on HuggingFace.co. This model is a quantized and WASM version of the popular all-MiniLM-L6-v2 which produces a 384-dimension vector.
If you are using the `native` embedding engine your vector database should be configured to accept 384-dimension models if that parameter is directly editable (Pinecone only).
Audio/Video transcription
AnythingLLM allows you to upload various audio and video formats as source documents. In all cases the audio tracks will be transcribed by a locally running ONNX model **whisper-small** built by Xenova on HuggingFace.co. The model is a smaller version of the OpenAI Whisper model. Given the model runs locally on CPU, larger files will result in longer transcription times.
Once transcribed you can embed these transcriptions into your workspace like you would any other file! 
Other external model/transcription providers are also live.**
OpenAI Whisper via API key.
Text generation (LLM selection)
[!IMPORTANT]
Use of a locally running LLM model is **experimental** and may behave unexpectedly, crash, or not function at all.
We suggest for production-use of a local LLM model to use a purpose-built inference server like LocalAI or LMStudio.
[!TIP]
We recommend at _least_ using a 4-bit or 5-bit quantized model for your LLM. Lower quantization models tend to
just output unreadable garbage.
If you would like to use a local Llama compatible LLM model for chatting you can select any model from this HuggingFace search filter
Requirements**
Model must be in the latest `GGUF` format
Model should be compatible with latest `llama.cpp`
You should have the proper RAM to run such a model. Requirement depends on model size.
Where do I put my GGUF model?
[!IMPORTANT]
If running in Docker you should be running the container to a mounted storage location on the host machine so you
can update the storage files directly without having to re-download or re-build your docker container. See suggested Docker config
[!NOTE]
`/server/storage/models/downloaded` is the default location that your model files should be at. 
Your storage directory may differ if you changed the STORAGE_DIR environment variable.
All local models you want to have available for LLM selection should be placed in the `server/storage/models/downloaded` folder. Only `.gguf` files will be allowed to be selected from the UI.


===== C:\Users\Breno\AppData\Local\Temp\repo_1u3zfsju\repo\server\utils\AiProviders\ollama\README.md =====
Common Issues with Ollama
If you encounter an error stating `llama:streaming - could not stream chat. Error: connect ECONNREFUSED 172.17.0.1:11434` when using AnythingLLM in a Docker container, this indicates that the IP of the Host inside of the virtual docker network does not bind to port 11434 of the host system by default, due to Ollama's restriction to localhost and 127.0.0.1. To resolve this issue and ensure proper communication between the Dockerized AnythingLLM and the Ollama service, you must configure Ollama to bind to 0.0.0.0 or a specific IP address.
Setting Environment Variables on Mac
If Ollama is run as a macOS application, environment variables should be set using `launchctl`:
1. For each environment variable, call `launchctl setenv`.
2. Restart the Ollama application.
Setting Environment Variables on Linux
If Ollama is run as a systemd service, environment variables should be set using `systemctl`:
1. Edit the systemd service by calling `systemctl edit ollama.service`. This will open an editor.
2. For each environment variable, add a line `Environment` under the section `[Service]`:
3. Save and exit.
4. Reload `systemd` and restart Ollama:
Setting Environment Variables on Windows
On Windows, Ollama inherits your user and system environment variables.
1. First, quit Ollama by clicking on it in the taskbar.
2. Edit system environment variables from the Control Panel.
3. Edit or create new variable(s) for your user account for `OLLAMA_HOST`, `OLLAMA_MODELS`, etc.
4. Click OK/Apply to save.
5. Run `ollama` from a new terminal window.


===== C:\Users\Breno\AppData\Local\Temp\repo_1u3zfsju\repo\server\utils\prisma\PRISMA.md =====
Prisma Setup and Usage Guide
This guide will help you set up and use Prisma for the project. Prisma is a powerful ORM for Node.js and TypeScript, helping developers build faster and make fewer errors. Follow the guide to understand how to use Prisma and the scripts available in the project to manage the Prisma setup.
Setting Up Prisma
To get started with setting up Prisma, you should run the setup script from the project root directory:
This script will install the necessary node modules in both the server and frontend directories, set up the environment files, and set up Prisma (generate client, run migrations, and seed the database).
Prisma Scripts
In the project root's `package.json`, there are several scripts set up to help you manage Prisma:
**prisma:generate**: Generates the Prisma client.
**prisma:migrate**: Runs the migrations to ensure the database is in sync with the schema.
**prisma:seed**: Seeds the database with initial data.
**prisma:setup**: A convenience script that runs `prisma:generate`, `prisma:migrate`, and `prisma:seed` in sequence.
**sqlite:migrate**: (To be run from the `server` directory) This script is for users transitioning from the old SQLite custom ORM setup to Prisma and will migrate all existing data over to Prisma. If you're a new user, your setup will already use Prisma.
To run any of these scripts, use `yarn` followed by the script name from the project root directory. For example:
Manual Prisma Commands
While the scripts should cover most of your needs, you may sometimes want to run Prisma commands manually. Here are some commands you might find useful, along with their descriptions:
`npx prisma introspect`: Introspects the database to update the Prisma schema by reading the schema of the existing database.
`npx prisma generate`: Generates the Prisma client.
`npx prisma migrate dev --name init`: Ensures the database is in sync with the schema, naming the migration 'init'.
`npx prisma migrate reset`: Resets the database, deleting all data and recreating the schema.
These commands should be run from the `server` directory, where the Prisma schema is located.
Notes
Always make sure to run scripts from the root level to avoid path issues.
Before running migrations, ensure that the Prisma schema is correctly defined to prevent data loss or corruption.
If you are adding a new feature or making changes that require a change in the database schema, create a new migration rather than editing existing migrations.
For users transitioning from the old SQLite ORM, navigate to the `server` directory and run the `sqlite:migrate` script to smoothly transition to Prisma. If you're setting up the project fresh, this step is unnecessary as the setup will already be using Prisma.


===== C:\Users\Breno\AppData\Local\Temp\repo_1u3zfsju\repo\server\utils\vectorDbProviders\astra\ASTRA_SETUP.md =====
How to setup Astra Vector Database for AnythingLLM
Official Astra DB Docs for reference.
How to get started
Requirements**
Astra Vector Database with active status.
Instructions**
Create an Astra account or sign in to an existing Astra account
Create an Astra Serverless(Vector) Database.
Make sure DB is in active state.
Get `API ENDPOINT`and `Application Token` from Overview screen


===== C:\Users\Breno\AppData\Local\Temp\repo_1u3zfsju\repo\server\utils\vectorDbProviders\chroma\CHROMA_SETUP.md =====
How to setup a local (or remote) Chroma Vector Database
Official Chroma Docs for reference.
How to get started
Requirements**
Docker
`git` available in your CLI/terminal
Instructions**
`git clone git@github.com:chroma-core/chroma.git` to somewhere on computer.
`cd chroma`
`docker-compose up -d --build`
set the `CHROMA_ENDPOINT=` .env variable in `server` and also set `VECTOR_DB=` to `chroma`.
If you have an API Gateway or auth middleway be sure to set the `CHROMA_API_HEADER` and `CHROMA_API_KEY` keys.
eg: `server/.env.development`


===== C:\Users\Breno\AppData\Local\Temp\repo_1u3zfsju\repo\server\utils\vectorDbProviders\milvus\MILVUS_SETUP.md =====
How to setup a local (or remote) Milvus Vector Database
Official Milvus Docs for reference.
How to get started
Requirements**
Choose one of the following
Cloud
 - Cloud account
Local
 - Docker
 - `git` available in your CLI/terminal
Instructions**
Cloud
 - Create a Cluster on your cloud account
 - Get connect Public Endpoint and Token
 - Set .env.development variable in server
Local
 - Download yaml file `wget -O docker-compose.yml`
 - Start Milvus `sudo docker compose up -d`
 - Check the containers are up and running `sudo docker compose ps`
 - Get port number and set .env.development variable in server
eg: `server/.env.development`


===== C:\Users\Breno\AppData\Local\Temp\repo_1u3zfsju\repo\server\utils\vectorDbProviders\pgvector\SETUP.md =====
Setting up `PGVector` for AnythingLLM
Setting up PGVector for anythingllm to use as your vector database is quite easy. At a minimum, you will need the following:
PostgreSQL v12+
`pgvector` extension installed on DB
User with DB table creation perms and READ access
Setup on Mac (example)
Install pgvector extension on PostgreSQL DB
Set PG as your vector db
_this can be done via the UI or by directly editing the `.env` file_
First, obtain a valid connection string for the user, credentials, and db you want to target.
eg: `postgresql://dbuser:dbuserpass@localhost:5432/yourdb`
[!IMPORTANT]
If you have an existing table that you want to use as a vector database, AnythingLLM **requires** that the table be
at least minimally conform to the expected schema - this can be seen in the index.js file.
_optional_ - set a table name you wish to have AnythingLLM store vectors to. By default this is `anythingllm_vectors`
Common Questions
I cannot connect to the DB (Running AnythingLLM in Docker)
If you are running AnythingLLM in Docker, you will need to ensure that the DB is accessible from the container.
If you are running your DB in another Docker container **or** on the host machine, you will need to ensure that the container can access the DB.
`localhost` will not work in this case as it will attempt to connect to the DB _inside the AnythingLLM container_ instead of the host machine or another container.
You will need to use the `host.docker.internal` (or `172.17.0.1` on Linux/Ubuntu) address.
Can I use an existing table as a vector database?
Yes, you can use an existing table as a vector database. However, AnythingLLM **requires** that the table be at least minimally conform to the expected schema - this can be seen in the index.js file.
It is **absolutely critical** that the `embedding` column's `VECTOR(XXXX)` dimensions match the dimension of the embedder in AnythingLLM. The default embedding model is 384 dimensions. However, if you are using a custom embedder, you will need to ensure that the dimension value is set correctly.
Validate the connection to the database
When setting the connection string in or table name via the AnythingLLM UI, the following validations will be attempted:
Validate the connection string
Validate the table name
Run test connection to ensure the table exists and is accessible by the connection string used
Check if the table name already exists and if so, validate that it is an embedding table with the correct schema
My embedding table is not present in the DB
The embedding storage table is created by AnythingLLM **on the first upsert** of a vector. If you have not yet embedding any documents, the table will not be present in the DB.
How do I reset my vector database?
_at the workspace level in Settings > Vector Database_
You can use the "Reset Vector Database" button in the AnythingLLM UI to reset your vector database. This will drop all vectors within that workspace, but the table will remain in the DB.
_reset the vector database at the db level_
For this, you will need to `DROP TABLE` from the command line or however you manage your DB. Once the table is dropped, it will be recreated by AnythingLLM on the next upsert.
Troubleshooting
Cannot connect to DB
Ensure the connection string is valid
Ensure the user has access to the database
Ensure the pgvector extension is installed
Cannot create table
Ensure the user has `CREATE TABLE` permissions
Cannot insert vector
Ensure the user has `INSERT` permissions in the database
Ensure the table has a dimension value set and this matches the dimension of the embedder in AnythingLLM
Ensure the table has a vector column set
Cannot query vector
Ensure the user has `SELECT` permissions in the database
Ensure the table has a vector column set
Ensure the table has a dimension value set and this matches the dimension of the embedder in AnythingLLM
"type 'vector' does not exist" issues with PGVector
If you are using the PGVector as your vector database, you may encounter an error similar to the following when embedding documents:
This is due to the fact that the `vector` type is not installed on the PG database.
First, follow the instructions in the PGVector README to install the `vector` type on your database.
Then, you will need to create the extension on the database. This can be done by running the following command:


===== C:\Users\Breno\AppData\Local\Temp\repo_1u3zfsju\repo\server\utils\vectorDbProviders\pinecone\PINECONE_SETUP.md =====
How to setup Pinecone Vector Database for AnythingLLM
Official Pinecone Docs for reference.
How to get started
Requirements**
Pinecone account with index that allows namespaces.
Note:** Namespaces are not supported in `gcp-starter` environments and are required to work with AnythingLLM.
Instructions**
Create an index on your Pinecone account. Name can be anything eg: `my-primary-index`
Metric `cosine`
Dimensions `1536` since we use OpenAI for embeddings
1 pod, all other default settings are fine.


===== C:\Users\Breno\AppData\Local\Temp\repo_1u3zfsju\repo\server\utils\vectorDbProviders\qdrant\QDRANT_SETUP.md =====
How to setup a local (or cloud) QDrant Vector Database
Get a QDrant Cloud instance.
Set up QDrant locally on Docker.
Fill out the variables in the "Vector Database" tab of settings. Select Qdrant as your provider and fill out the appropriate fields
with the information from either of the above steps.
How to get started _Development mode only_
After setting up either the Qdrant cloud or local dockerized instance you just need to set these variable in `.env.development` or defined them at runtime via the UI.


===== C:\Users\Breno\AppData\Local\Temp\repo_1u3zfsju\repo\server\utils\vectorDbProviders\weaviate\WEAVIATE_SETUP.md =====
How to setup a local (or cloud) Weaviate Vector Database
Get a Weaviate Cloud instance.
Set up Weaviate locally on Docker.
Fill out the variables in the "Vector Database" tab of settings. Select Weaviate as your provider and fill out the appropriate fields
with the information from either of the above steps.
How to get started _Development mode only_
After setting up either the Weaviate cloud or local dockerized instance you just need to set these variable in `.env.development` or defined them at runtime via the UI.
