# -*- coding: utf-8 -*-
"""teste5.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1qf5vhCWKoAjsWHgDDlGquLYJ9nCuryM7
"""

import os
import time
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np
import torch

# Configurações do projeto



BASE_DIR = os.path.dirname(os.path.abspath(__file__))

ARQUIVO_SAIDA_TXT = os.path.join(BASE_DIR, "..", "..", "resultados", "allminilm_l6_v2.txt")
entradas_dir = os.path.abspath(os.path.join(BASE_DIR, "..", "..", "entradas"))
os.makedirs(entradas_dir, exist_ok=True)
ARQUIVO_ENTRADA_TXT = os.path.join(entradas_dir, "entrada1.txt")

INPUT_FILES = [ARQUIVO_ENTRADA_TXT]
OUTPUT_FILE = os.path.join(BASE_DIR, "..", "..", "resultados", "all_minilm_l6_v2.txt")


# Padrões arquiteturais EXPANDIDOS e MELHORADOS
ARCHITECTURAL_PATTERNS = [
    # Padrões principais
    "MVC - Model-View-Controller separation pattern with frontend and backend separation",
    "Microservices - Independently deployable services architecture with multiple components",
    "Client-Server - Centralized server with multiple clients and API endpoints",
    "Layered Architecture - Hierarchical layer separation with presentation and data layers",
    "Event-Driven - Asynchronous event-based communication and messaging",
    "Service-Oriented - Reusable services with standardized interfaces and APIs",
    "Component-Based - Reusable component architecture with modular design",
    "Repository Pattern - Data access abstraction layer with database operations",
    "Factory Pattern - Object creation pattern for multiple providers",
    "Singleton Pattern - Single instance global access for shared resources",

    # Padrões específicos para projetos web/LLM
    "API Gateway - Single entry point for multiple backend services",
    "Plugin Architecture - Extensible system with pluggable components",
    "Provider Pattern - Multiple implementations for same interface",
    "Modular Monolith - Structured monolith with clear module boundaries",
    "Frontend-Backend Separation - Clear separation between UI and business logic",
    "Multi-tenant Architecture - Support for multiple workspaces and users",
    "Data Pipeline - Processing pipeline for document ingestion and transformation",
    "Vector Database Architecture - Semantic search with embedding storage",
    "LLM Integration Pattern - Multiple AI provider integrations",
    "Real-time Communication - WebSocket or streaming responses"
]

# Carregar modelo de embeddings
device = "cuda" if torch.cuda.is_available() else "cpu"
print("Usando dispositivo:", device)
model = SentenceTransformer(
    'sentence-transformers/all-MiniLM-L6-v2',
    device=device
)

def load_input_files():
    """Carrega os arquivos de entrada especificados"""
    files_data = []

    for file_path in INPUT_FILES:
        if os.path.exists(file_path):
            try:
                with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                    content = f.read()

                files_data.append({
                    'path': file_path,
                    'name': os.path.basename(file_path),
                    'content': content,
                    'type': 'input_file'
                })
                print(f"✓ Carregado: {file_path} ({len(content)} caracteres)")
            except Exception as e:
                print(f"✗ Erro ao carregar {file_path}: {e}")
        else:
            print(f"✗ Arquivo não encontrado: {file_path}")

    return files_data

def save_arquivos_analisados(files_data):
    """Salva a lista de arquivos que serão analisados em um txt"""
    with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:
        f.write("# Arquivos Selecionados para Análise\n\n")

        f.write("## Arquivos de Entrada Analisados\n")
        f.write(f"Total: {len(files_data)} arquivos\n\n")
        for i, file_info in enumerate(files_data, 1):
            f.write(f"{i}. {file_info['name']}\n")
            f.write(f"   Tamanho: {len(file_info['content'])} caracteres\n")
            f.write(f"   Caminho: {file_info['path']}\n\n")

def analyze_content(content, pattern_embeddings):
    """Analisa o conteúdo em busca de padrões arquiteturais"""
    if not content.strip():
        return []

    # Gerar embedding do conteúdo
    content_embedding = model.encode(content)

    # Calcular similaridade com os padrões arquiteturais
    similarities = cosine_similarity([content_embedding], pattern_embeddings)[0]

    # Identificar padrões com probabilidade acima de 15% (BAIXADO)
    detected_patterns = []
    for i, similarity in enumerate(similarities):
        if similarity > 0.15:  # LIMITE BAIXADO
            detected_patterns.append({
                'pattern': ARCHITECTURAL_PATTERNS[i],
                'probability': float(similarity)
            })

    # Ordenar por probabilidade decrescente
    detected_patterns.sort(key=lambda x: x['probability'], reverse=True)
    return detected_patterns

def enhance_content_analysis(content, file_name):
    """Melhora a análise baseada no tipo de conteúdo"""
    enhanced_content = content

    # Adiciona contexto baseado no tipo de arquivo
    if "entrada1" in file_name:
        # Estrutura de pastas - adiciona contexto arquitetural
        enhanced_content += " frontend backend server collector microservices components api endpoints middleware database models utils providers plugins extensions"

    elif "entrada2" in file_name:
        # package.json - adiciona contexto de build e dependências
        enhanced_content += " build scripts dependencies dev server frontend collector separate services microservices architecture"

    elif "entrada3" in file_name:
        # Commits e arquivos - adiciona contexto de funcionalidades
        enhanced_content += " api endpoints database models vector storage embeddings llm providers microservices components plugins realtime streaming"

    return enhanced_content
def main():
    start_time = time.perf_counter()

    print("Iniciando análise de padrões arquiteturais...")
    print(f"Arquivos de entrada: {INPUT_FILES}")

    # Preparar embeddings dos padrões arquiteturais
    pattern_embeddings = model.encode(ARCHITECTURAL_PATTERNS)

    # Carregar arquivos
    files_data = load_input_files()

    if not files_data:
        print("Nenhum arquivo de entrada encontrado.")
        return

    # Salvar lista de arquivos analisados
    save_arquivos_analisados(files_data)

    print(f"Total de arquivos carregados: {len(files_data)}\n")

    # Dicionário: padrão → ocorrências
    grouped_patterns = {}

    for file_info in files_data:
        file_name = file_info['name']
        content = file_info['content']

        # IMPRIMIR CONTEÚDO DA ENTRADA
        print("\n===== CONTEÚDO DO ARQUIVO DE ENTRADA =====")
        print(f"Arquivo: {file_name}")
        print("------------------------------------------")
        print(content[:3000])  # imprime até 3000 caracteres para segurança
        print("------------------------------------------\n")

        # aplicar melhoria de contexto
        enhanced_content = enhance_content_analysis(content, file_name)


        # encontrar padrões
        patterns = analyze_content(enhanced_content, pattern_embeddings)

        if not patterns:
            print(f"✗ Nenhum padrão encontrado em {file_name}.")
            continue

        print(f"✓ Padrões encontrados em {file_name}: {len(patterns)}")

        # Guardar resultados
        for p in patterns:
            key = p["pattern"]
            if key not in grouped_patterns:
                grouped_patterns[key] = []
            grouped_patterns[key].append({
                "source": file_name,
                "probability": p["probability"]
            })

    # -------------------------------
    # GERAR ARQUIVO DE SAÍDA ÚNICO
    # -------------------------------

    with open(OUTPUT_FILE, "w", encoding="utf-8") as f:
        f.write("# Padrões Arquiteturais Identificados\n\n")

        if not grouped_patterns:
            f.write("Nenhum padrão arquitetural encontrado.")
        else:
            # ordenar padrões por média
            ordered = sorted(
                grouped_patterns.items(),
                key=lambda x: np.mean([i["probability"] for i in x[1]]),
                reverse=True
            )

            for pattern, occurrences in ordered:
                short = pattern.split(" - ")[0]
                avg = np.mean([x["probability"] for x in occurrences])
                fontes = ", ".join({x["source"] for x in occurrences})

                f.write(f"## {short}\n")
                f.write(f"Probabilidade média: {avg:.1%}\n")
                f.write(f"Fontes: {fontes}\n\n")

    # -------------------------------
    # RESUMO NO TERMINAL
    # -------------------------------
    print("\n=== RESUMO ===\n")

    if not grouped_patterns:
        print("Nenhum padrão encontrado.")
    else:
        ordered = sorted(
            grouped_patterns.items(),
            key=lambda x: np.mean([i["probability"] for i in x[1]]),
            reverse=True
        )

        for pattern, occurrences in ordered:
            short = pattern.split(" - ")[0]
            avg = np.mean([x["probability"] for x in occurrences])
            print(f"- {short}: {avg:.1%}")

    total = time.perf_counter() - start_time
    print(f"\nTempo total: {total:.2f} segundos")
    print(f"Relatório salvo em: {OUTPUT_FILE}")
    print(f"Arquivos analisados em: {OUTPUT_FILE}")

# Executar a análise
if __name__ == "__main__":
    main()