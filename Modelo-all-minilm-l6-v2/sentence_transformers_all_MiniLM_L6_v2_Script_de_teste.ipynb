{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Script principal"
      ],
      "metadata": {
        "id": "k_GDhhViAL6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "import subprocess\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "\n",
        "# Configurações do projeto\n",
        "REPO_PATH = \"anything-llm\"\n",
        "REPO_URL = \"https://github.com/Mintplex-Labs/anything-llm.git\"  # URL CORRETO\n",
        "OUTPUT_FILE = \"padroes_arquiteturais.txt\"\n",
        "ARQUIVOS_ANALISADOS_FILE = \"arquivos_analisados.txt\"\n",
        "\n",
        "# Padrões arquiteturais para busca\n",
        "ARCHITECTURAL_PATTERNS = [\n",
        "    \"MVC - Model-View-Controller separation pattern\",\n",
        "    \"Microservices - Independently deployable services architecture\",\n",
        "    \"Client-Server - Centralized server with multiple clients\",\n",
        "    \"Layered Architecture - Hierarchical layer separation\",\n",
        "    \"Event-Driven - Asynchronous event-based communication\",\n",
        "    \"Service-Oriented - Reusable services with standardized interfaces\",\n",
        "    \"Component-Based - Reusable component architecture\",\n",
        "    \"Repository Pattern - Data access abstraction layer\",\n",
        "    \"Factory Pattern - Object creation pattern\",\n",
        "    \"Singleton Pattern - Single instance global access\"\n",
        "]\n",
        "\n",
        "# Carregar modelo de embeddings\n",
        "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
        "\n",
        "def get_recent_commits(num_commits=100):\n",
        "    \"\"\"Obtém os últimos commits do repositório\"\"\"\n",
        "    try:\n",
        "        result = subprocess.run([\n",
        "            'git', 'log', f'-{num_commits}', '--oneline', '--format=%H %s'\n",
        "        ], capture_output=True, text=True, cwd=REPO_PATH)\n",
        "\n",
        "        if result.returncode != 0:\n",
        "            print(\"Erro ao obter commits\")\n",
        "            return []\n",
        "\n",
        "        commits = []\n",
        "        lines = result.stdout.strip().split('\\n')\n",
        "        for line in lines:\n",
        "            if line.strip():\n",
        "                parts = line.split(' ', 1)\n",
        "                if len(parts) == 2:\n",
        "                    commit_hash, message = parts\n",
        "                    commits.append({\n",
        "                        'hash': commit_hash,\n",
        "                        'message': message,\n",
        "                        'type': 'commit'\n",
        "                    })\n",
        "\n",
        "        return commits\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Erro ao obter commits: {e}\")\n",
        "        return []\n",
        "\n",
        "def get_commit_details(commit_hash):\n",
        "    \"\"\"Obtém detalhes de um commit específico\"\"\"\n",
        "    try:\n",
        "        result = subprocess.run([\n",
        "            'git', 'show', '--stat', '--oneline', commit_hash\n",
        "        ], capture_output=True, text=True, cwd=REPO_PATH)\n",
        "\n",
        "        if result.returncode == 0:\n",
        "            return result.stdout\n",
        "        return \"\"\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Erro ao obter detalhes do commit {commit_hash}: {e}\")\n",
        "        return \"\"\n",
        "\n",
        "def find_text_files():\n",
        "    \"\"\"Encontra arquivos de texto, README e documentação no repositório\"\"\"\n",
        "    text_files = []\n",
        "\n",
        "    for root, dirs, files in os.walk(REPO_PATH):\n",
        "        if 'node_modules' in dirs:\n",
        "            dirs.remove('node_modules')\n",
        "        if '.git' in dirs:\n",
        "            dirs.remove('.git')\n",
        "\n",
        "        for file in files:\n",
        "            if file.lower().endswith(('.md', '.txt', '.rst')) or file.lower() in ['readme', 'documentation']:\n",
        "                full_path = os.path.join(root, file)\n",
        "                text_files.append({\n",
        "                    'path': full_path,\n",
        "                    'type': 'file'\n",
        "                })\n",
        "\n",
        "    return text_files[:40]\n",
        "\n",
        "def save_arquivos_analisados(commits, text_files):  # FUNÇÃO MODIFICADA\n",
        "    \"\"\"Salva a lista de arquivos que serão analisados em um txt\"\"\"\n",
        "    with open(ARQUIVOS_ANALISADOS_FILE, 'w', encoding='utf-8') as f:\n",
        "        f.write(\"# Arquivos Selecionados para Análise\\n\\n\")\n",
        "\n",
        "        f.write(\"## Commits Analisados\\n\")\n",
        "        f.write(f\"Total: {len(commits)} commits\\n\\n\")\n",
        "        for i, commit in enumerate(commits, 1):  # MODIFICADO: lista TODOS os commits\n",
        "            f.write(f\"{i}. {commit['hash'][:8]} - {commit['message']}\\n\")\n",
        "\n",
        "        f.write(\"\\n## Arquivos de Texto Analisados\\n\")\n",
        "        f.write(f\"Total: {len(text_files)} arquivos\\n\\n\")\n",
        "        for i, file_info in enumerate(text_files, 1):\n",
        "            relative_path = file_info['path'].replace(REPO_PATH + '/', '')\n",
        "            f.write(f\"{i}. {relative_path}\\n\")\n",
        "\n",
        "def analyze_content(content, pattern_embeddings):\n",
        "    \"\"\"Analisa o conteúdo em busca de padrões arquiteturais\"\"\"\n",
        "    if not content.strip():\n",
        "        return []\n",
        "\n",
        "    # Gerar embedding do conteúdo\n",
        "    content_embedding = model.encode(content)\n",
        "\n",
        "    # Calcular similaridade com os padrões arquiteturais\n",
        "    similarities = cosine_similarity([content_embedding], pattern_embeddings)[0]\n",
        "\n",
        "    # Identificar padrões com probabilidade acima de 25%\n",
        "    detected_patterns = []\n",
        "    for i, similarity in enumerate(similarities):\n",
        "        if similarity > 0.25:\n",
        "            detected_patterns.append({\n",
        "                'pattern': ARCHITECTURAL_PATTERNS[i],\n",
        "                'probability': float(similarity)\n",
        "            })\n",
        "\n",
        "    # Ordenar por probabilidade decrescente\n",
        "    detected_patterns.sort(key=lambda x: x['probability'], reverse=True)\n",
        "    return detected_patterns\n",
        "\n",
        "def main():\n",
        "    # Iniciar contagem de tempo\n",
        "    start_time = time.perf_counter()\n",
        "\n",
        "    print(\"Iniciando analise de padroes arquiteturais...\")\n",
        "    print(\"Prompt: Considere que o seu objetivo é encontrar padrões arquiteturais no repositório especificado. APENAS utilizando os arquivos e pastas especificados, faça uma busca no repositório, cite quais são os possíveis modelos encontrados, a probabilidade de ser cada um e retorne com os resultados em um arquivo .txt\")\n",
        "    print(\"Analisando ultimos 100 commits e arquivos de texto...\")\n",
        "\n",
        "    # Preparar embeddings dos padrões arquiteturais\n",
        "    pattern_embeddings = model.encode(ARCHITECTURAL_PATTERNS)\n",
        "\n",
        "    # Obter dados para análise\n",
        "    commits = get_recent_commits(100)\n",
        "    text_files = find_text_files()\n",
        "\n",
        "    print(f\"Encontrados {len(commits)} commits e {len(text_files)} arquivos de texto\")\n",
        "\n",
        "    # NOVO: Salvar lista de arquivos que serão analisados\n",
        "    save_arquivos_analisados(commits, text_files)\n",
        "    print(f\"Lista de arquivos salva em: {ARQUIVOS_ANALISADOS_FILE}\")\n",
        "\n",
        "    # Analisar commits e arquivos\n",
        "    results = {}\n",
        "\n",
        "    # Processar commits\n",
        "    for commit in commits:\n",
        "        commit_hash = commit['hash']\n",
        "        commit_message = commit['message']\n",
        "\n",
        "        commit_details = get_commit_details(commit_hash)\n",
        "        commit_content = f\"{commit_message} {commit_details}\"[:1500]\n",
        "\n",
        "        patterns = analyze_content(commit_content, pattern_embeddings)\n",
        "        if patterns:\n",
        "            source_key = f\"COMMIT: {commit_hash[:8]} - {commit_message}\"\n",
        "            results[source_key] = patterns\n",
        "\n",
        "    # Processar arquivos de texto\n",
        "    for file_info in text_files:\n",
        "        file_path = file_info['path']\n",
        "        file_name = os.path.basename(file_path)\n",
        "\n",
        "        try:\n",
        "            with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
        "                content = f.read(2000)\n",
        "\n",
        "            patterns = analyze_content(content, pattern_embeddings)\n",
        "            if patterns:\n",
        "                source_key = f\"FILE: {file_path.replace(REPO_PATH + '/', '')}\"\n",
        "                results[source_key] = patterns\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Erro ao analisar arquivo {file_name}: {e}\")\n",
        "\n",
        "    # Gerar relatório em arquivo txt\n",
        "    with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:\n",
        "        f.write(\"# Padroes Arquiteturais de Software Identificados\\n\\n\")\n",
        "\n",
        "        # Agrupar resultados por padrão arquitetural\n",
        "        grouped_patterns = {}\n",
        "        for source, patterns in results.items():\n",
        "            for pattern_info in patterns:\n",
        "                pattern_name = pattern_info['pattern']\n",
        "                if pattern_name not in grouped_patterns:\n",
        "                    grouped_patterns[pattern_name] = []\n",
        "                grouped_patterns[pattern_name].append({\n",
        "                    'source': source,\n",
        "                    'probability': pattern_info['probability']\n",
        "                })\n",
        "\n",
        "        # Escrever padrões identificados\n",
        "        if grouped_patterns:\n",
        "            f.write(\"## Padroes Identificados\\n\\n\")\n",
        "            for pattern, occurrences in grouped_patterns.items():\n",
        "                avg_probability = np.mean([occ['probability'] for occ in occurrences])\n",
        "                pattern_short = pattern.split(' - ')[0]\n",
        "                f.write(f\"### {pattern_short}\\n\")\n",
        "                f.write(f\"Probabilidade Media: {avg_probability:.1%}\\n\")\n",
        "                f.write(\"Fontes relacionadas:\\n\")\n",
        "\n",
        "                for occ in sorted(occurrences, key=lambda x: x['probability'], reverse=True)[:3]:\n",
        "                    source_type = \"COMMIT\" if occ['source'].startswith('COMMIT') else \"FILE\"\n",
        "                    f.write(f\"- [{source_type}] {occ['source']} ({occ['probability']:.1%})\\n\")\n",
        "                f.write(\"\\n\")\n",
        "        else:\n",
        "            f.write(\"Nenhum padrao arquitetural significativo foi identificado.\\n\\n\")\n",
        "\n",
        "        f.write(\"# Resumo\\n\\n\")\n",
        "        f.write(f\"Repositorio analisado: {REPO_PATH}\\n\")\n",
        "        f.write(f\"Commits analisados: {len(commits)}\\n\")\n",
        "        f.write(f\"Arquivos de texto analisados: {len(text_files)}\\n\")\n",
        "        f.write(f\"Padroes identificados: {len(grouped_patterns)}\\n\")\n",
        "        f.write(\"Metodologia: Analise semantica de commits e documentacao utilizando embeddings de texto\\n\")\n",
        "        f.write(\"Modelo utilizado: sentence-transformers/all-MiniLM-L6-v2\\n\")\n",
        "\n",
        "    # Calcular tempo de execução\n",
        "    execution_time = time.perf_counter() - start_time\n",
        "\n",
        "    print(f\"Analise concluida!\")\n",
        "    print(f\"Relatorio salvo em: {OUTPUT_FILE}\")\n",
        "    print(f\"Lista de entrada salva em: {ARQUIVOS_ANALISADOS_FILE}\")\n",
        "    print(f\"Tempo de execucao: {execution_time:.2f} segundos\")\n",
        "\n",
        "# Executar a análise\n",
        "if __name__ == \"__main__\":\n",
        "    # Clonar o repositório antes de executar\n",
        "    if not os.path.exists(REPO_PATH):\n",
        "        print(f\"Clonando repositório: {REPO_URL}\")\n",
        "        subprocess.run(['git', 'clone', REPO_URL])\n",
        "\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jnUWGm5u-HMi",
        "outputId": "75db2484-42c9-479a-8d06-a22f55a452bf"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iniciando analise de padroes arquiteturais...\n",
            "Prompt: Considere que o seu objetivo é encontrar padrões arquiteturais no repositório especificado. APENAS utilizando os arquivos e pastas especificados, faça uma busca no repositório, cite quais são os possíveis modelos encontrados, a probabilidade de ser cada um e retorne com os resultados em um arquivo .txt\n",
            "Analisando ultimos 100 commits e arquivos de texto...\n",
            "Encontrados 100 commits e 33 arquivos de texto\n",
            "Lista de arquivos salva em: arquivos_analisados.txt\n",
            "Analise concluida!\n",
            "Relatorio salvo em: padroes_arquiteturais.txt\n",
            "Lista de entrada salva em: arquivos_analisados.txt\n",
            "Tempo de execucao: 13.29 segundos\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Script para mostrar os resultados\n"
      ],
      "metadata": {
        "id": "NVDVpZwAATED"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Célula para visualizar os arquivos gerados\n",
        "from google.colab import files\n",
        "import os\n",
        "\n",
        "# Verificar se os arquivos foram criados\n",
        "arquivos_gerados = [OUTPUT_FILE, ARQUIVOS_ANALISADOS_FILE]\n",
        "\n",
        "for arquivo in arquivos_gerados:\n",
        "    if os.path.exists(arquivo):\n",
        "        print(f\"=== CONTEÚDO DE {arquivo} ===\")\n",
        "        print(f\"Tamanho: {os.path.getsize(arquivo)} bytes\")\n",
        "        print(\"\\n\" + \"=\"*50)\n",
        "\n",
        "        with open(arquivo, 'r', encoding='utf-8') as f:\n",
        "            content = f.read()\n",
        "            print(content)\n",
        "\n",
        "        print(\"\\n\" + \"=\"*50)\n",
        "        print()\n",
        "    else:\n",
        "        print(f\"Arquivo {arquivo} não encontrado.\")\n",
        "        print()\n",
        "\n",
        "print(\"Para baixar os arquivos, execute a próxima célula ou:\")\n",
        "print(\"1. Clique no ícone de pasta no menu lateral\")\n",
        "print(\"2. Encontre os arquivos .txt\")\n",
        "print(\"3. Clique com botão direito → Download\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-_2eAjkrAY0s",
        "outputId": "d1b2c39f-99a3-4769-8db3-cfb054709858"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== CONTEÚDO DE padroes_arquiteturais.txt ===\n",
            "Tamanho: 1448 bytes\n",
            "\n",
            "==================================================\n",
            "# Padroes Arquiteturais de Software Identificados\n",
            "\n",
            "## Padroes Identificados\n",
            "\n",
            "### Microservices\n",
            "Probabilidade Media: 29.9%\n",
            "Fontes relacionadas:\n",
            "- [FILE] FILE: server/utils/vectorDbProviders/milvus/MILVUS_SETUP.md (38.3%)\n",
            "- [COMMIT] COMMIT: 599a3fd8 - Microsoft Foundry Local LLM provider & agent provider (#4435) (34.9%)\n",
            "- [FILE] FILE: cloud-deployments/aws/cloudformation/DEPLOY.md (30.1%)\n",
            "\n",
            "### Component-Based\n",
            "Probabilidade Media: 26.2%\n",
            "Fontes relacionadas:\n",
            "- [FILE] FILE: server/storage/models/README.md (26.9%)\n",
            "- [FILE] FILE: cloud-deployments/helm/charts/anythingllm/README.md (25.9%)\n",
            "- [COMMIT] COMMIT: c2e7ccc0 - Reimplement Cohere models for basic chat (#4489) (25.8%)\n",
            "\n",
            "### Event-Driven\n",
            "Probabilidade Media: 29.3%\n",
            "Fontes relacionadas:\n",
            "- [COMMIT] COMMIT: d6f0d305 - Enable real-time agent tool call streaming for all providers (#4279) (34.6%)\n",
            "- [COMMIT] COMMIT: 5922349b - feat: Implement CometAPI integration for chat completions and model m… (#4379) (32.1%)\n",
            "- [COMMIT] COMMIT: 8cdadd8c - Sync models from remote for FireworksAI (#4475) (25.3%)\n",
            "\n",
            "### Repository Pattern\n",
            "Probabilidade Media: 30.0%\n",
            "Fontes relacionadas:\n",
            "- [FILE] FILE: pull_request_template.md (30.0%)\n",
            "\n",
            "# Resumo\n",
            "\n",
            "Repositorio analisado: anything-llm\n",
            "Commits analisados: 100\n",
            "Arquivos de texto analisados: 33\n",
            "Padroes identificados: 4\n",
            "Metodologia: Analise semantica de commits e documentacao utilizando embeddings de texto\n",
            "Modelo utilizado: sentence-transformers/all-MiniLM-L6-v2\n",
            "\n",
            "\n",
            "==================================================\n",
            "\n",
            "=== CONTEÚDO DE arquivos_analisados.txt ===\n",
            "Tamanho: 8549 bytes\n",
            "\n",
            "==================================================\n",
            "# Arquivos Selecionados para Análise\n",
            "\n",
            "## Commits Analisados\n",
            "Total: 100 commits\n",
            "\n",
            "1. 62b4d813 - Update Sponsors README\n",
            "2. c2d99b67 - Update Sponsors README\n",
            "3. e9db2745 - feat: support northflank deployment (#4570)\n",
            "4. c1539754 - Update Sponsors README\n",
            "5. 151f97da - Patch agent thoughts UI bug (#4549)\n",
            "6. d3619689 - Refactor `loadYouTubeTranscript()` to include YouTube Video Metadata in Content When `parseOnly` is `true` (#4552)\n",
            "7. 8c97240f - Refactor `DefaultChatContainer` To Display A Simple Welcome Message  (#4542)\n",
            "8. f5f8fb1e - Agent workspace system prompt with variable expansion (#4526)\n",
            "9. 985527c3 - fix(server): correct Qdrant batching logic for large uploads (#4545)\n",
            "10. 5edc1bea - Add ability to auto-handle YT video URLs in uploader & chat (#4547)\n",
            "11. be82f91f - Enable keyboard nav of slash commands with arrow keys on mount (#4543)\n",
            "12. 797920a2 - Enable Microsoft Foundry Local for workspace agent provider (#4539)\n",
            "13. 6270a0a1 - Fix KoboldCPP agent provider max tokens (#4519)\n",
            "14. 71cd46ce - 1.9.0 tag\n",
            "15. d48c7691 - Fix: File pulling fails with uppercase URL characters (#4516)\n",
            "16. 4e6f0b33 - fix label for chunk length setting (#4515)\n",
            "17. 8bc6aa71 - missed lint\n",
            "18. 0c07e040 - refactor: change naming - contextwarpper to authprovider #4510 (#4511)\n",
            "19. 89a01492 - Update .gitignore (#4507)\n",
            "20. cd1a8d07 - Merge branch 'master' of github.com:Mintplex-Labs/anything-llm\n",
            "21. 5173c751 - rescope validatedLink to local var\n",
            "22. 0ee0a965 - Migrate gemini agents away from `Untooled` (#4505)\n",
            "23. cf3fbcbf - Improve URL handler for collector processes (#4504)\n",
            "24. 3259ede9 - Merge branch 'master' of github.com:Mintplex-Labs/anything-llm\n",
            "25. 0a1a5a21 - patch ollama context window error when unreachable\n",
            "26. 36138884 - Tooltips for workspace and threads (#4500)\n",
            "27. f6f56679 - Update changes to layout rendering for Experimental features\n",
            "28. c2e7ccc0 - Reimplement Cohere models for basic chat (#4489)\n",
            "29. 988a14e6 - Adding AnythingLLM Helm Chart (#4484)\n",
            "30. 87c66646 - Render html optional (#4478)\n",
            "31. 8cdadd8c - Sync models from remote for FireworksAI (#4475)\n",
            "32. 0b18ac65 - Model context limit auto-detection for LM Studio and Ollama LLM Providers (#4468)\n",
            "33. bdfa0328 - update comment about parseOnly\n",
            "34. 599a3fd8 - Microsoft Foundry Local LLM provider & agent provider (#4435)\n",
            "35. 8f0f9df4 - Migrate OpenAI Agent to use ResponsesAPI (#4467)\n",
            "36. f7b90571 - Fetch, Parse, and Create Documents for Statically Hosted Files (#4398)\n",
            "37. 00432726 - Add stream options to Gemini LLM for usage tracking (#4466)\n",
            "38. d6f0d305 - Enable real-time agent tool call streaming for all providers (#4279)\n",
            "39. d24f9c34 - fix(uiux): correct typo in System Prompt description text (#4461)\n",
            "40. cd340631 - Patch OpenAI metrics (#4458)\n",
            "41. be7e2b6b - Apply renderer from chat widget history to workspace chats (#4456)\n",
            "42. d800f8a0 - Merge branch 'master' of github.com:Mintplex-Labs/anything-llm\n",
            "43. d1e39e17 - Fix issue where filenames with spaces could be marked as orphaned and pruned from threads they are attached to - resulting in the model not seeing a file during chat after the backend boots\n",
            "44. 96bf1276 - New Default System Prompt Variables (User ID, Workspace ID, & Workspace Name) (#4414)\n",
            "45. 7ca2753c - Sanitize Metadata Before PG Vector Database Insertion (#4434)\n",
            "46. eb778761 - Add HTTP request/response logging middleware for development mode (#4425)\n",
            "47. 8fc1f24d - fix: youtube transcript collector not work well with non en or non asr caption (#4442)\n",
            "48. c8f13d5f - Enable custom HTTP response timeout for ollama (#4448)\n",
            "49. ac444c8f - Change incorrect notation of Weaviate to PG Vector in env.example (#4439)\n",
            "50. 6855bbf6 - Refactor Class Name Logging (#4426)\n",
            "51. 473ff906 - fix: resolve Firefox search icon overlapping placeholder text (#4390)\n",
            "52. 2226f29a - Add PostgreSQL vector extension in createTableIfNotExists function (#4430)\n",
            "53. 3cb54fdb - [BUGFIX] Update Dell Pro AI Studio Default URL (#4433)\n",
            "54. 9466f671 - Update the timeout value on all stream-timeout providers: (#4412)\n",
            "55. 1209606d - Migrate OpenAI LLM provider to use Responses API (#4404)\n",
            "56. f0cdea4e - Ignore hasOwnProperty linting errors (#4406)\n",
            "57. 01a3cc92 - Enhanced Chat Embed History View (#4281)\n",
            "58. 226802d3 - API request delay for Generic OpenAI embedding engine (#4317)\n",
            "59. 95557ee1 - Allow user to specify args for chromium process so they dont need SYS_ADMIN on container. (#4397)\n",
            "60. 7864e1a9 - Report sources in API responses on finalized chunk (#4396)\n",
            "61. 50d4a198 - Add User-Agent header on the requests sent by Generic OpenAI providers. (#4393)\n",
            "62. e81bd471 - patch folder name GET request response (#4395)\n",
            "63. cd063af4 - Add support for `SIMPLE_SSO_NO_LOGIN_REDIRECT` config setting (#4394)\n",
            "64. b8d4cc34 - Added metadata parameter to document/upload, document/upload/{folderName}, and document/upload-link (#4342)\n",
            "65. 9841deb5 - update save file agent text (#4389)\n",
            "66. a97b5149 - Resize chat textarea on paste (#4369)\n",
            "67. 5922349b - feat: Implement CometAPI integration for chat completions and model m… (#4379)\n",
            "68. 631dd2e7 - Update Sponsors README\n",
            "69. eb3cc98a - update bare metal docs\n",
            "70. 4f86a513 - feat(i18n): add missing Portuguese (Brazil) translations (#4328)\n",
            "71. 2500c94b - Fix: missing edit icon for prompts (#4344)\n",
            "72. e31465a6 - Export image support for JSON and JSONL (#4359)\n",
            "73. bb7d65f0 - patch missing options resolves #4316\n",
            "74. 6358d087 - Fix /openai/models compat endpoint to return correct response schema resolves #4295\n",
            "75. b44cf21c - Allow default users to reorder workspaces (#4292)\n",
            "76. a4a84f9b - forgot 1.8.5 tag :)\n",
            "77. e6a33ec0 - patch paths\n",
            "78. 0200e647 - add back normalization + docs link\n",
            "79. 43e5d040 - Update Security UI to match all other Settings (#4274)\n",
            "80. c6e1b9c3 - Chroma Cloud vector db provider (#4273)\n",
            "81. a432f82b - Update common.js (#4278)\n",
            "82. a230a44f - feat: Add Exa as a Search Provider (#4258)\n",
            "83. 0fb33736 - Workspace Chat with documents overhaul (#4261)\n",
            "84. 9451cd59 - feat: add Romanian translation (#4247)\n",
            "85. 585308c2 - docs(i18n): Complete missing Spanish translations (#4263)\n",
            "86. 20468d70 - Update common.js Hebrew language update (#4241)\n",
            "87. fe263120 - Fix password hint text (#4235)\n",
            "88. a9d9f9cd - Add custom JWT TTL (#4234)\n",
            "89. c218a0df - Mobile sync support (#4173)\n",
            "90. 755ef4bb - STT append spoken text (#4216)\n",
            "91. 4b7932f9 - Fix agent iam_role implied agentic tool calling resolves #4193 contrib: @bechir2000\n",
            "92. ea5f6697 -  fix: API export-chats endpoint function import error (#4220)\n",
            "93. a0af2462 - Update check for `validFuncCall` to only check required args and for undefined options (#4214)\n",
            "94. f1486c03 - patch microphone tooltip id mismatch\n",
            "95. 24f176c0 - [Chore]: `sendCommand` non positional call signature (#4218)\n",
            "96. 70a07b74 - Update `writeToServerDocuments` to take config object (#4213)\n",
            "97. 76927759 - minor change to XLSX parse and upload output folder\n",
            "98. 8c7923a2 - Fix app version metrics endpoint (#4209)\n",
            "99. 78cbb06c - Modify the PostgreSQL SQL connector to support querying tables from schemas other than the default public schema. (#4202)\n",
            "100. 89724169 - fix: correct some typos and grammatical errors in translation strings and index.jsx (#4195)\n",
            "\n",
            "## Arquivos de Texto Analisados\n",
            "Total: 33 arquivos\n",
            "\n",
            "1. CONTRIBUTING.md\n",
            "2. BARE_METAL.md\n",
            "3. README.md\n",
            "4. SECURITY.md\n",
            "5. pull_request_template.md\n",
            "6. server/utils/AiProviders/ollama/README.md\n",
            "7. server/utils/AiProviders/perplexity/scripts/chat_models.txt\n",
            "8. server/utils/vectorDbProviders/pgvector/SETUP.md\n",
            "9. server/utils/vectorDbProviders/astra/ASTRA_SETUP.md\n",
            "10. server/utils/vectorDbProviders/qdrant/QDRANT_SETUP.md\n",
            "11. server/utils/vectorDbProviders/chroma/CHROMA_SETUP.md\n",
            "12. server/utils/vectorDbProviders/milvus/MILVUS_SETUP.md\n",
            "13. server/utils/vectorDbProviders/weaviate/WEAVIATE_SETUP.md\n",
            "14. server/utils/vectorDbProviders/pinecone/PINECONE_SETUP.md\n",
            "15. server/utils/prisma/PRISMA.md\n",
            "16. server/storage/README.md\n",
            "17. server/storage/models/README.md\n",
            "18. server/storage/documents/DOCUMENTS.md\n",
            "19. collector/hotdir/__HOTDIR__.md\n",
            "20. locales/README.zh-CN.md\n",
            "21. locales/README.tr-TR.md\n",
            "22. locales/README.fa-IR.md\n",
            "23. locales/README.ja-JP.md\n",
            "24. extras/support/announcements/list.txt\n",
            "25. frontend/public/robots.txt\n",
            "26. .devcontainer/README.md\n",
            "27. cloud-deployments/gcp/deployment/DEPLOY.md\n",
            "28. cloud-deployments/helm/charts/anythingllm/README.md\n",
            "29. cloud-deployments/helm/charts/anythingllm/templates/NOTES.txt\n",
            "30. cloud-deployments/aws/cloudformation/aws_https_instructions.md\n",
            "31. cloud-deployments/aws/cloudformation/DEPLOY.md\n",
            "32. cloud-deployments/digitalocean/terraform/DEPLOY.md\n",
            "33. docker/HOW_TO_USE_DOCKER.md\n",
            "\n",
            "\n",
            "==================================================\n",
            "\n",
            "Para baixar os arquivos, execute a próxima célula ou:\n",
            "1. Clique no ícone de pasta no menu lateral\n",
            "2. Encontre os arquivos .txt\n",
            "3. Clique com botão direito → Download\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Script para baixar os arquivos txt de entrada e saída"
      ],
      "metadata": {
        "id": "1zbmttmHAkw7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Célula para baixar os arquivos\n",
        "from google.colab import files\n",
        "\n",
        "arquivos_para_download = [OUTPUT_FILE, ARQUIVOS_ANALISADOS_FILE]\n",
        "\n",
        "for arquivo in arquivos_para_download:\n",
        "    if os.path.exists(arquivo):\n",
        "        files.download(arquivo)\n",
        "        print(f\"✅ {arquivo} - Download iniciado!\")\n",
        "    else:\n",
        "        print(f\"❌ {arquivo} - Arquivo não encontrado\")\n",
        "\n",
        "print(\"Todos os downloads foram iniciados!\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "id": "tbDIYdm3ApH7",
        "outputId": "f4cbdcca-5367-49b9-adcb-58f10e2e8cab"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_928c8ff2-5906-4ec7-93a3-8d7471f36b38\", \"padroes_arquiteturais.txt\", 1296)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ padroes_arquiteturais.txt - Download iniciado!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_2329871c-f5f4-466c-9873-03ebf59342ed\", \"arquivos_analisados.txt\", 7820)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ arquivos_analisados.txt - Download iniciado!\n",
            "Todos os downloads foram iniciados!\n"
          ]
        }
      ]
    }
  ]
}