{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "import subprocess\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "\n",
        "# Configurações do projeto\n",
        "REPO_PATH = \"anything-llm\"\n",
        "OUTPUT_FILE = \"padroes_arquiteturais.txt\"\n",
        "ARQUIVOS_ANALISADOS_FILE = \"arquivos_analisados.txt\"  # NOVO: arquivo da entrada\n",
        "\n",
        "# Padrões arquiteturais para busca\n",
        "ARCHITECTURAL_PATTERNS = [\n",
        "    \"MVC - Model-View-Controller separation pattern\",\n",
        "    \"Microservices - Independently deployable services architecture\",\n",
        "    \"Client-Server - Centralized server with multiple clients\",\n",
        "    \"Layered Architecture - Hierarchical layer separation\",\n",
        "    \"Event-Driven - Asynchronous event-based communication\",\n",
        "    \"Service-Oriented - Reusable services with standardized interfaces\",\n",
        "    \"Component-Based - Reusable component architecture\",\n",
        "    \"Repository Pattern - Data access abstraction layer\",\n",
        "    \"Factory Pattern - Object creation pattern\",\n",
        "    \"Singleton Pattern - Single instance global access\"\n",
        "]\n",
        "\n",
        "# Carregar modelo de embeddings\n",
        "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
        "\n",
        "def get_recent_commits(num_commits=100):\n",
        "    \"\"\"Obtém os últimos commits do repositório\"\"\"\n",
        "    try:\n",
        "        result = subprocess.run([\n",
        "            'git', 'log', f'-{num_commits}', '--oneline', '--format=%H %s'\n",
        "        ], capture_output=True, text=True, cwd=REPO_PATH)\n",
        "\n",
        "        if result.returncode != 0:\n",
        "            print(\"Erro ao obter commits\")\n",
        "            return []\n",
        "\n",
        "        commits = []\n",
        "        lines = result.stdout.strip().split('\\n')\n",
        "        for line in lines:\n",
        "            if line.strip():\n",
        "                parts = line.split(' ', 1)\n",
        "                if len(parts) == 2:\n",
        "                    commit_hash, message = parts\n",
        "                    commits.append({\n",
        "                        'hash': commit_hash,\n",
        "                        'message': message,\n",
        "                        'type': 'commit'\n",
        "                    })\n",
        "\n",
        "        return commits\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Erro ao obter commits: {e}\")\n",
        "        return []\n",
        "\n",
        "def get_commit_details(commit_hash):\n",
        "    \"\"\"Obtém detalhes de um commit específico\"\"\"\n",
        "    try:\n",
        "        result = subprocess.run([\n",
        "            'git', 'show', '--stat', '--oneline', commit_hash\n",
        "        ], capture_output=True, text=True, cwd=REPO_PATH)\n",
        "\n",
        "        if result.returncode == 0:\n",
        "            return result.stdout\n",
        "        return \"\"\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Erro ao obter detalhes do commit {commit_hash}: {e}\")\n",
        "        return \"\"\n",
        "\n",
        "def find_text_files():\n",
        "    \"\"\"Encontra arquivos de texto, README e documentação no repositório\"\"\"\n",
        "    text_files = []\n",
        "\n",
        "    for root, dirs, files in os.walk(REPO_PATH):\n",
        "        if 'node_modules' in dirs:\n",
        "            dirs.remove('node_modules')\n",
        "        if '.git' in dirs:\n",
        "            dirs.remove('.git')\n",
        "\n",
        "        for file in files:\n",
        "            if file.lower().endswith(('.md', '.txt', '.rst')) or file.lower() in ['readme', 'documentation']:\n",
        "                full_path = os.path.join(root, file)\n",
        "                text_files.append({\n",
        "                    'path': full_path,\n",
        "                    'type': 'file'\n",
        "                })\n",
        "\n",
        "    return text_files[:15]\n",
        "\n",
        "def save_arquivos_analisados(commits, text_files):  # NOVA FUNÇÃO\n",
        "    \"\"\"Salva a lista de arquivos que serão analisados em um txt\"\"\"\n",
        "    with open(ARQUIVOS_ANALISADOS_FILE, 'w', encoding='utf-8') as f:\n",
        "        f.write(\"# Arquivos Selecionados para Análise\\n\\n\")\n",
        "\n",
        "        f.write(\"## Commits Analisados\\n\")\n",
        "        f.write(f\"Total: {len(commits)} commits\\n\\n\")\n",
        "        for i, commit in enumerate(commits[:10], 1):  # Mostra primeiros 10\n",
        "            f.write(f\"{i}. {commit['hash'][:8]} - {commit['message']}\\n\")\n",
        "        if len(commits) > 10:\n",
        "            f.write(f\"... e mais {len(commits) - 10} commits\\n\")\n",
        "\n",
        "        f.write(\"\\n## Arquivos de Texto Analisados\\n\")\n",
        "        f.write(f\"Total: {len(text_files)} arquivos\\n\\n\")\n",
        "        for i, file_info in enumerate(text_files, 1):\n",
        "            relative_path = file_info['path'].replace(REPO_PATH + '/', '')\n",
        "            f.write(f\"{i}. {relative_path}\\n\")\n",
        "\n",
        "def analyze_content(content, pattern_embeddings):\n",
        "    \"\"\"Analisa o conteúdo em busca de padrões arquiteturais\"\"\"\n",
        "    if not content.strip():\n",
        "        return []\n",
        "\n",
        "    # Gerar embedding do conteúdo\n",
        "    content_embedding = model.encode(content)\n",
        "\n",
        "    # Calcular similaridade com os padrões arquiteturais\n",
        "    similarities = cosine_similarity([content_embedding], pattern_embeddings)[0]\n",
        "\n",
        "    # Identificar padrões com probabilidade acima de 25%\n",
        "    detected_patterns = []\n",
        "    for i, similarity in enumerate(similarities):\n",
        "        if similarity > 0.25:\n",
        "            detected_patterns.append({\n",
        "                'pattern': ARCHITECTURAL_PATTERNS[i],\n",
        "                'probability': float(similarity)\n",
        "            })\n",
        "\n",
        "    # Ordenar por probabilidade decrescente\n",
        "    detected_patterns.sort(key=lambda x: x['probability'], reverse=True)\n",
        "    return detected_patterns\n",
        "\n",
        "def main():\n",
        "    # Iniciar contagem de tempo\n",
        "    start_time = time.perf_counter()\n",
        "\n",
        "    print(\"Iniciando analise de padroes arquiteturais...\")\n",
        "    print(\"Prompt: Considere que o seu objetivo é encontrar padrões arquiteturais no repositório especificado. APENAS utilizando os arquivos e pastas especificados, faça uma busca no repositório, cite quais são os possíveis modelos encontrados, a probabilidade de ser cada um e retorne com os resultados em um arquivo .txt\")\n",
        "    print(\"Analisando ultimos 100 commits e arquivos de texto...\")\n",
        "\n",
        "    # Preparar embeddings dos padrões arquiteturais\n",
        "    pattern_embeddings = model.encode(ARCHITECTURAL_PATTERNS)\n",
        "\n",
        "    # Obter dados para análise\n",
        "    commits = get_recent_commits(100)\n",
        "    text_files = find_text_files()\n",
        "\n",
        "    print(f\"Encontrados {len(commits)} commits e {len(text_files)} arquivos de texto\")\n",
        "\n",
        "    # NOVO: Salvar lista de arquivos que serão analisados\n",
        "    save_arquivos_analisados(commits, text_files)\n",
        "    print(f\"Lista de arquivos salva em: {ARQUIVOS_ANALISADOS_FILE}\")\n",
        "\n",
        "    # Analisar commits e arquivos\n",
        "    results = {}\n",
        "\n",
        "    # Processar commits\n",
        "    for commit in commits:\n",
        "        commit_hash = commit['hash']\n",
        "        commit_message = commit['message']\n",
        "\n",
        "        commit_details = get_commit_details(commit_hash)\n",
        "        commit_content = f\"{commit_message} {commit_details}\"[:1500]\n",
        "\n",
        "        patterns = analyze_content(commit_content, pattern_embeddings)\n",
        "        if patterns:\n",
        "            source_key = f\"COMMIT: {commit_hash[:8]} - {commit_message}\"\n",
        "            results[source_key] = patterns\n",
        "\n",
        "    # Processar arquivos de texto\n",
        "    for file_info in text_files:\n",
        "        file_path = file_info['path']\n",
        "        file_name = os.path.basename(file_path)\n",
        "\n",
        "        try:\n",
        "            with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
        "                content = f.read(2000)\n",
        "\n",
        "            patterns = analyze_content(content, pattern_embeddings)\n",
        "            if patterns:\n",
        "                source_key = f\"FILE: {file_path.replace(REPO_PATH + '/', '')}\"\n",
        "                results[source_key] = patterns\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Erro ao analisar arquivo {file_name}: {e}\")\n",
        "\n",
        "    # Gerar relatório em arquivo txt\n",
        "    with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:\n",
        "        f.write(\"# Padroes Arquiteturais de Software Identificados\\n\\n\")\n",
        "\n",
        "        # Agrupar resultados por padrão arquitetural\n",
        "        grouped_patterns = {}\n",
        "        for source, patterns in results.items():\n",
        "            for pattern_info in patterns:\n",
        "                pattern_name = pattern_info['pattern']\n",
        "                if pattern_name not in grouped_patterns:\n",
        "                    grouped_patterns[pattern_name] = []\n",
        "                grouped_patterns[pattern_name].append({\n",
        "                    'source': source,\n",
        "                    'probability': pattern_info['probability']\n",
        "                })\n",
        "\n",
        "        # Escrever padrões identificados\n",
        "        if grouped_patterns:\n",
        "            f.write(\"## Padroes Identificados\\n\\n\")\n",
        "            for pattern, occurrences in grouped_patterns.items():\n",
        "                avg_probability = np.mean([occ['probability'] for occ in occurrences])\n",
        "                pattern_short = pattern.split(' - ')[0]\n",
        "                f.write(f\"### {pattern_short}\\n\")\n",
        "                f.write(f\"Probabilidade Media: {avg_probability:.1%}\\n\")\n",
        "                f.write(\"Fontes relacionadas:\\n\")\n",
        "\n",
        "                for occ in sorted(occurrences, key=lambda x: x['probability'], reverse=True)[:3]:\n",
        "                    source_type = \"COMMIT\" if occ['source'].startswith('COMMIT') else \"FILE\"\n",
        "                    f.write(f\"- [{source_type}] {occ['source']} ({occ['probability']:.1%})\\n\")\n",
        "                f.write(\"\\n\")\n",
        "        else:\n",
        "            f.write(\"Nenhum padrao arquitetural significativo foi identificado.\\n\\n\")\n",
        "\n",
        "        f.write(\"# Resumo\\n\\n\")\n",
        "        f.write(f\"Repositorio analisado: {REPO_PATH}\\n\")\n",
        "        f.write(f\"Commits analisados: {len(commits)}\\n\")\n",
        "        f.write(f\"Arquivos de texto analisados: {len(text_files)}\\n\")\n",
        "        f.write(f\"Padroes identificados: {len(grouped_patterns)}\\n\")\n",
        "        f.write(\"Metodologia: Analise semantica de commits e documentacao utilizando embeddings de texto\\n\")\n",
        "        f.write(\"Modelo utilizado: sentence-transformers/all-MiniLM-L6-v2\\n\")\n",
        "\n",
        "    # Calcular tempo de execução\n",
        "    execution_time = time.perf_counter() - start_time\n",
        "\n",
        "    print(f\"Analise concluida!\")\n",
        "    print(f\"Relatorio salvo em: {OUTPUT_FILE}\")\n",
        "    print(f\"Lista de entrada salva em: {ARQUIVOS_ANALISADOS_FILE}\")\n",
        "    print(f\"Tempo de execucao: {execution_time:.2f} segundos\")\n",
        "\n",
        "# Executar a análise\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HzFRfNMn0AEG",
        "outputId": "a4d93d7a-f9b5-4469-dca0-c936129ad3f3"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iniciando analise de padroes arquiteturais...\n",
            "Prompt: Considere que o seu objetivo é encontrar padrões arquiteturais no repositório especificado. APENAS utilizando os arquivos e pastas especificados, faça uma busca no repositório, cite quais são os possíveis modelos encontrados, a probabilidade de ser cada um e retorne com os resultados em um arquivo .txt\n",
            "Analisando ultimos 100 commits e arquivos de texto...\n",
            "Encontrados 100 commits e 15 arquivos de texto\n",
            "Lista de arquivos salva em: arquivos_analisados.txt\n",
            "Analise concluida!\n",
            "Relatorio salvo em: padroes_arquiteturais.txt\n",
            "Lista de entrada salva em: arquivos_analisados.txt\n",
            "Tempo de execucao: 12.34 segundos\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Célula para visualizar os arquivos gerados\n",
        "from google.colab import files\n",
        "import os\n",
        "\n",
        "# Verificar se os arquivos foram criados\n",
        "arquivos_gerados = [OUTPUT_FILE, ARQUIVOS_ANALISADOS_FILE]\n",
        "\n",
        "for arquivo in arquivos_gerados:\n",
        "    if os.path.exists(arquivo):\n",
        "        print(f\"=== CONTEÚDO DE {arquivo} ===\")\n",
        "        print(f\"Tamanho: {os.path.getsize(arquivo)} bytes\")\n",
        "        print(\"\\n\" + \"=\"*50)\n",
        "\n",
        "        with open(arquivo, 'r', encoding='utf-8') as f:\n",
        "            content = f.read()\n",
        "            print(content)\n",
        "\n",
        "        print(\"\\n\" + \"=\"*50)\n",
        "        print()\n",
        "    else:\n",
        "        print(f\"Arquivo {arquivo} não encontrado.\")\n",
        "        print()\n",
        "\n",
        "print(\"Para baixar os arquivos, execute a próxima célula ou:\")\n",
        "print(\"1. Clique no ícone de pasta no menu lateral\")\n",
        "print(\"2. Encontre os arquivos .txt\")\n",
        "print(\"3. Clique com botão direito → Download\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fMsToAmG23O4",
        "outputId": "4f7503da-c9d5-4e2c-b523-e43af5a58e68"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== CONTEÚDO DE padroes_arquiteturais.txt ===\n",
            "Tamanho: 1296 bytes\n",
            "\n",
            "==================================================\n",
            "# Padroes Arquiteturais de Software Identificados\n",
            "\n",
            "## Padroes Identificados\n",
            "\n",
            "### Microservices\n",
            "Probabilidade Media: 31.9%\n",
            "Fontes relacionadas:\n",
            "- [FILE] FILE: server/utils/vectorDbProviders/milvus/MILVUS_SETUP.md (38.3%)\n",
            "- [COMMIT] COMMIT: 599a3fd8 - Microsoft Foundry Local LLM provider & agent provider (#4435) (34.9%)\n",
            "- [COMMIT] COMMIT: 71cd46ce - 1.9.0 tag (27.7%)\n",
            "\n",
            "### Component-Based\n",
            "Probabilidade Media: 25.8%\n",
            "Fontes relacionadas:\n",
            "- [COMMIT] COMMIT: c2e7ccc0 - Reimplement Cohere models for basic chat (#4489) (25.8%)\n",
            "\n",
            "### Event-Driven\n",
            "Probabilidade Media: 29.3%\n",
            "Fontes relacionadas:\n",
            "- [COMMIT] COMMIT: d6f0d305 - Enable real-time agent tool call streaming for all providers (#4279) (34.6%)\n",
            "- [COMMIT] COMMIT: 5922349b - feat: Implement CometAPI integration for chat completions and model m… (#4379) (32.1%)\n",
            "- [COMMIT] COMMIT: 8cdadd8c - Sync models from remote for FireworksAI (#4475) (25.3%)\n",
            "\n",
            "### Repository Pattern\n",
            "Probabilidade Media: 30.0%\n",
            "Fontes relacionadas:\n",
            "- [FILE] FILE: pull_request_template.md (30.0%)\n",
            "\n",
            "# Resumo\n",
            "\n",
            "Repositorio analisado: anything-llm\n",
            "Commits analisados: 100\n",
            "Arquivos de texto analisados: 15\n",
            "Padroes identificados: 4\n",
            "Metodologia: Analise semantica de commits e documentacao utilizando embeddings de texto\n",
            "Modelo utilizado: sentence-transformers/all-MiniLM-L6-v2\n",
            "\n",
            "\n",
            "==================================================\n",
            "\n",
            "=== CONTEÚDO DE arquivos_analisados.txt ===\n",
            "Tamanho: 1476 bytes\n",
            "\n",
            "==================================================\n",
            "# Arquivos Selecionados para Análise\n",
            "\n",
            "## Commits Analisados\n",
            "Total: 100 commits\n",
            "\n",
            "1. 62b4d813 - Update Sponsors README\n",
            "2. c2d99b67 - Update Sponsors README\n",
            "3. e9db2745 - feat: support northflank deployment (#4570)\n",
            "4. c1539754 - Update Sponsors README\n",
            "5. 151f97da - Patch agent thoughts UI bug (#4549)\n",
            "6. d3619689 - Refactor `loadYouTubeTranscript()` to include YouTube Video Metadata in Content When `parseOnly` is `true` (#4552)\n",
            "7. 8c97240f - Refactor `DefaultChatContainer` To Display A Simple Welcome Message  (#4542)\n",
            "8. f5f8fb1e - Agent workspace system prompt with variable expansion (#4526)\n",
            "9. 985527c3 - fix(server): correct Qdrant batching logic for large uploads (#4545)\n",
            "10. 5edc1bea - Add ability to auto-handle YT video URLs in uploader & chat (#4547)\n",
            "... e mais 90 commits\n",
            "\n",
            "## Arquivos de Texto Analisados\n",
            "Total: 15 arquivos\n",
            "\n",
            "1. CONTRIBUTING.md\n",
            "2. BARE_METAL.md\n",
            "3. README.md\n",
            "4. SECURITY.md\n",
            "5. pull_request_template.md\n",
            "6. server/utils/AiProviders/ollama/README.md\n",
            "7. server/utils/AiProviders/perplexity/scripts/chat_models.txt\n",
            "8. server/utils/vectorDbProviders/pgvector/SETUP.md\n",
            "9. server/utils/vectorDbProviders/astra/ASTRA_SETUP.md\n",
            "10. server/utils/vectorDbProviders/qdrant/QDRANT_SETUP.md\n",
            "11. server/utils/vectorDbProviders/chroma/CHROMA_SETUP.md\n",
            "12. server/utils/vectorDbProviders/milvus/MILVUS_SETUP.md\n",
            "13. server/utils/vectorDbProviders/weaviate/WEAVIATE_SETUP.md\n",
            "14. server/utils/vectorDbProviders/pinecone/PINECONE_SETUP.md\n",
            "15. server/utils/prisma/PRISMA.md\n",
            "\n",
            "\n",
            "==================================================\n",
            "\n",
            "Para baixar os arquivos, execute a próxima célula ou:\n",
            "1. Clique no ícone de pasta no menu lateral\n",
            "2. Encontre os arquivos .txt\n",
            "3. Clique com botão direito → Download\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Célula para baixar os arquivos\n",
        "from google.colab import files\n",
        "\n",
        "arquivos_para_download = [OUTPUT_FILE, ARQUIVOS_ANALISADOS_FILE]\n",
        "\n",
        "for arquivo in arquivos_para_download:\n",
        "    if os.path.exists(arquivo):\n",
        "        files.download(arquivo)\n",
        "        print(f\"✅ {arquivo} - Download iniciado!\")\n",
        "    else:\n",
        "        print(f\"❌ {arquivo} - Arquivo não encontrado\")\n",
        "\n",
        "print(\"Todos os downloads foram iniciados!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "id": "takYHcMb4BNh",
        "outputId": "7978a91b-8114-4e80-b0de-2ad8b6bd5881"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_2a0aa163-52da-4734-971f-fe0f9af04ab0\", \"padroes_arquiteturais.txt\", 1296)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ padroes_arquiteturais.txt - Download iniciado!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_5c93f062-6e4d-4955-9f07-ac14c31025eb\", \"arquivos_analisados.txt\", 1476)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ arquivos_analisados.txt - Download iniciado!\n",
            "Todos os downloads foram iniciados!\n"
          ]
        }
      ]
    }
  ]
}